{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95109b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import functools\n",
    "\n",
    "# Kaggle-specific: Ensure we can download data if not present\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea54631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "DATASET = 'omniglot'\n",
    "N_WAY = 32\n",
    "K_SHOT = 1\n",
    "# 32-way 1-shot with 5 query means 32*(1+5) = 192 images per episode.\n",
    "N_QUERY = 5 \n",
    "NUM_EPOCHS = 500 \n",
    "EPISODES_PER_EPOCH = 50 \n",
    "TEST_EPISODES = 50 \n",
    "OUTPUT_DIM = 1024 # Fixed to 1024 to support segment_size up to 1024\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Distance Function (The Experiment)\n",
    "# -------------------------------\n",
    "def segmented_cosine_similarity(a, b, segment_size=2):\n",
    "    \"\"\"\n",
    "    Segmented Cosine Similarity with Sliding Window (Circular).\n",
    "    Segments are formed by sliding a window of size 'segment_size' over the vector with wrap-around.\n",
    "    Number of segments equals the vector dimension.\n",
    "    \n",
    "    Formula:\n",
    "    Sim = Sum_{i=1}^{N} ( (|q_i|*|s_i|) / (segment_size * |q|*|s|) * Cos(q_i, s_i) )\n",
    "    \n",
    "    Memory Efficient Version: Calculates in chunks to avoid OOM.\n",
    "    \"\"\"\n",
    "    batch_q, dim = a.size()\n",
    "    batch_s, _ = b.size()\n",
    "    \n",
    "    # 1. Circular Padding\n",
    "    if segment_size > 1:\n",
    "        pad_size = segment_size - 1\n",
    "        a_padded = torch.cat([a, a[:, :pad_size]], dim=1)\n",
    "        b_padded = torch.cat([b, b[:, :pad_size]], dim=1)\n",
    "    else:\n",
    "        a_padded = a\n",
    "        b_padded = b\n",
    "        \n",
    "    # 2. Unfold to get sliding windows\n",
    "    # Shape: [Batch, Num_Segments, Segment_Size]\n",
    "    a_seg = a_padded.unfold(dimension=1, size=segment_size, step=1)\n",
    "    b_seg = b_padded.unfold(dimension=1, size=segment_size, step=1)\n",
    "    \n",
    "    if a_seg.shape[1] != dim:\n",
    "         pass\n",
    "\n",
    "    # Global norms: [Batch, 1]\n",
    "    norm_a = torch.norm(a, p=2, dim=1).view(batch_q, 1) \n",
    "    norm_b = torch.norm(b, p=2, dim=1).view(1, batch_s) \n",
    "    \n",
    "    # Final Similarity Matrix: [Q, S]\n",
    "    # Initialize on device\n",
    "    final_sim = torch.zeros(batch_q, batch_s, device=a.device)\n",
    "    \n",
    "    # --- Chunking Strategy ---\n",
    "    # a_seg: [Q, N, D]\n",
    "    # b_seg: [S, N, D]\n",
    "    # Instead of broadcasting to [Q, S, N, D] which is Q*S*N*D floats.\n",
    "    # N=1024, Q=160, S=32. 160*32*1024*D. \n",
    "    # If D is large? No, the issue is Q*S*N term in broadcast.\n",
    "    # 160*32*1024 = 5,242,880 elements. \n",
    "    # But PyTorch broadcast creates temporary tensors. \n",
    "    # The traceback says 20GB. Likely due to large internal expansion.\n",
    "    \n",
    "    # Let's iterate over Query Batch to reduce memory usage.\n",
    "    # Or iterate over Segments (N) if we sum them up?\n",
    "    # Sim = Sum_i ( ... )\n",
    "    # It allows us to compute similarity segment by segment (or chunks of segments) and accumulate.\n",
    "    \n",
    "    # Chunk over Num_Segments (dim)\n",
    "    # This avoids storing the huge [Q, S, N] tensor of norms/dots.\n",
    "    \n",
    "    chunk_size = 64 # Process 64 segments at a time\n",
    "    num_segments = dim\n",
    "    \n",
    "    for i in range(0, num_segments, chunk_size):\n",
    "        end = min(i + chunk_size, num_segments)\n",
    "        \n",
    "        # Slices: [Q, Chunk, D]\n",
    "        sub_a = a_seg[:, i:end, :] \n",
    "        # Slices: [S, Chunk, D]\n",
    "        sub_b = b_seg[:, i:end, :]\n",
    "        \n",
    "        # Expand for pairwise: [Q, S, Chunk, D]\n",
    "        # Still might be big if Chunk is large.\n",
    "        # Let's calculate parts directly.\n",
    "\n",
    "        # a: [Q, 1, C, D], b: [1, S, C, D]\n",
    "        sub_a_exp = sub_a.unsqueeze(1)\n",
    "        sub_b_exp = sub_b.unsqueeze(0)\n",
    "        \n",
    "        # Dot: [Q, S, C]\n",
    "        dot_prod = (sub_a_exp * sub_b_exp).sum(dim=-1)\n",
    "        \n",
    "        # Norms: [Q, S, C]\n",
    "        norm_a_seg = torch.norm(sub_a_exp, p=2, dim=-1)\n",
    "        norm_b_seg = torch.norm(sub_b_exp, p=2, dim=-1)\n",
    "        \n",
    "        # Cosine: [Q, S, C]\n",
    "        cos_seg = dot_prod / (norm_a_seg * norm_b_seg + 1e-8)\n",
    "        \n",
    "        # Weights: [Q, S, C]\n",
    "        # (|q_i| |s_i|) / (D * |q| |s|)\n",
    "        # numerator = norm_a_seg * norm_b_seg\n",
    "        # Using global norms which are [Q, 1] and [1, S]\n",
    "        # Need to align dimensions to [Q, S, C]\n",
    "        \n",
    "        global_norm_denom = (segment_size * norm_a.unsqueeze(2) * norm_b.unsqueeze(2)) + 1e-8\n",
    "        \n",
    "        weights = (norm_a_seg * norm_b_seg) / global_norm_denom\n",
    "        \n",
    "        # Weighted sum contribution\n",
    "        # sim_chunk: [Q, S] (summed over C)\n",
    "        sim_chunk = (weights * cos_seg).sum(dim=2)\n",
    "        \n",
    "        final_sim += sim_chunk\n",
    "        \n",
    "        # Clear intermediates\n",
    "        del sub_a_exp, sub_b_exp, dot_prod, norm_a_seg, norm_b_seg, cos_seg, weights\n",
    "    \n",
    "    return final_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Helper Functions\n",
    "# -------------------------------\n",
    "def quantize_to_int8(tensor, scale=None):\n",
    "    if scale is None:\n",
    "        abs_max = torch.max(torch.abs(tensor)).item()\n",
    "        scale = 127.0 / (abs_max + 1e-8)\n",
    "    scaled = tensor * scale\n",
    "    quantized = scaled.detach().round().clamp(-128, 127) - scaled.detach() + scaled\n",
    "    return quantized, scale\n",
    "\n",
    "def dequantize_from_int8(quantized, scale):\n",
    "    return quantized.float() / scale\n",
    "\n",
    "def organize_by_class(dataset):\n",
    "    data_by_class = {}\n",
    "    for img, label in dataset:\n",
    "        if label not in data_by_class:\n",
    "            data_by_class[label] = []\n",
    "        data_by_class[label].append(img)\n",
    "    return data_by_class\n",
    "\n",
    "def sample_episode(data_by_class, n_way, k_shot, n_query):\n",
    "    selected_classes = random.sample(list(data_by_class.keys()), n_way)\n",
    "    support_images, support_labels, query_images, query_labels = [], [], [], []\n",
    "    for idx, cls in enumerate(selected_classes):\n",
    "        images = data_by_class[cls]\n",
    "        if len(images) < (k_shot + n_query):\n",
    "            images = images * ((k_shot + n_query) // len(images) + 1)\n",
    "        selected_imgs = random.sample(images, k_shot + n_query)\n",
    "        support_images += selected_imgs[:k_shot]\n",
    "        support_labels += [idx] * k_shot\n",
    "        query_images += selected_imgs[k_shot:]\n",
    "        query_labels += [idx] * n_query\n",
    "    return (\n",
    "        torch.stack(support_images),\n",
    "        torch.tensor(support_labels),\n",
    "        torch.stack(query_images),\n",
    "        torch.tensor(query_labels),\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# Model (Matched to mix/code.py)\n",
    "# -------------------------------\n",
    "class MANN(nn.Module):\n",
    "    def __init__(self, dataset, out_dim=1024, quantize=False):\n",
    "        super().__init__()\n",
    "        in_channels = 1 if dataset in ['omniglot', 'mnist'] else 3\n",
    "        planes = [128, 128, 128, 128] if dataset in ['omniglot', 'mnist'] else [128, 256, 512, 1024]\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, planes[0], kernel_size=5, stride=2, padding=2, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(planes[0], planes[1], kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(planes[1], planes[2], kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(planes[2], planes[3], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(planes[3] * 7 * 7, out_dim)  # Adjusted for input size\n",
    "        self.quantize = quantize\n",
    "        self.scale = None  # 用於存儲縮放因子\n",
    "\n",
    "    def forward(self, x, apply_quantize=None):\n",
    "        if apply_quantize is None:\n",
    "            apply_quantize = self.quantize\n",
    "            \n",
    "        emb = self.fc(self.model(x).view(x.size(0), -1))\n",
    "        \n",
    "        if apply_quantize:\n",
    "            # 量化 embedding\n",
    "            quantized_emb, self.scale = quantize_to_int8(emb, self.scale)\n",
    "            # 立即反量化回浮點，以便後續處理\n",
    "            emb = dequantize_from_int8(quantized_emb, self.scale)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Experiment logic\n",
    "# -------------------------------\n",
    "\n",
    "print(f\"Setting up {DATASET} experiment\")\n",
    "print(f\"N_WAY={N_WAY}, K_SHOT={K_SHOT}, OUTPUT_DIM={OUTPUT_DIM}\")\n",
    "\n",
    "# Data Setup\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: 1.0 - x)\n",
    "])\n",
    "\n",
    "# Use a local directory for data. In Kaggle, this will be /kaggle/working/data\n",
    "data_root = './data'\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to download/load dataset to {data_root}...\")\n",
    "    train_dataset = torchvision.datasets.Omniglot(root=data_root, background=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.Omniglot(root=data_root, background=False, download=True, transform=transform)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Download might have failed: {e}\")\n",
    "    # Fallback/Handling logic...\n",
    "\n",
    "train_data = organize_by_class(train_dataset)\n",
    "test_data = organize_by_class(test_dataset)\n",
    "\n",
    "# Parameters\n",
    "segment_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "NUM_RUNS = 10  # Train 10 models to get statistics\n",
    "results_per_size = {d: [] for d in segment_sizes}\n",
    "\n",
    "print(f\"\\n=========================================\")\n",
    "print(f\"Starting Experiment: {NUM_RUNS} runs\")\n",
    "print(f\"Each run trains ONE model with Standard Cosine (D=1), then tests on ALL segment sizes.\")\n",
    "print(f\"=========================================\")\n",
    "\n",
    "for run in range(NUM_RUNS):\n",
    "    print(f\"\\n--- Run {run+1}/{NUM_RUNS} ---\")\n",
    "    \n",
    "    # 1. Initialize Model\n",
    "    model = MANN(dataset=DATASET, out_dim=OUTPUT_DIM).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 2. Train with Standard Cosine Similarity (equivalent to D=1)\n",
    "    # Using D=1 during training as per request for \"general cosine similarity\"\n",
    "    train_dist_fn = functools.partial(segmented_cosine_similarity, segment_size=1)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"  Training model...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        for ep in range(EPISODES_PER_EPOCH):\n",
    "            support_imgs, support_labels, query_imgs, query_labels = sample_episode(\n",
    "                train_data, N_WAY, K_SHOT, N_QUERY)\n",
    "            \n",
    "            support_imgs = support_imgs.to(DEVICE)\n",
    "            support_labels = support_labels.to(DEVICE)\n",
    "            query_imgs = query_imgs.to(DEVICE)\n",
    "            query_labels = query_labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            support_emb = model(support_imgs)\n",
    "            query_emb = model(query_imgs)\n",
    "            \n",
    "            logits = train_dist_fn(query_emb, support_emb)\n",
    "            loss = criterion(logits, query_labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        if (epoch + 1) % 50 == 0 or (epoch + 1) == NUM_EPOCHS:\n",
    "             print(f\"    Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {epoch_loss/EPISODES_PER_EPOCH:.4f}\")\n",
    "\n",
    "    # 3. Test on ALL segment sizes using the SAME trained model\n",
    "    print(\"  Testing on all segment sizes...\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Pre-load test episodes for consistent testing across D? \n",
    "    # Or sample fresh for each D? Standard is sample fresh or fixed set.\n",
    "    # Given the loop structure, sampling fresh per D is fine, but sampling SAME episodes for comparing D is better.\n",
    "    # Let's sample a fixed set of episodes for this run's evaluation to be fair across D.\n",
    "    \n",
    "    test_episodes_data = []\n",
    "    for _ in range(TEST_EPISODES):\n",
    "        test_episodes_data.append(sample_episode(test_data, N_WAY, K_SHOT, N_QUERY))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for seg_size in segment_sizes:\n",
    "            test_dist_fn = functools.partial(segmented_cosine_similarity, segment_size=seg_size)\n",
    "            \n",
    "            total_correct = 0\n",
    "            total_total = 0\n",
    "            \n",
    "            for episode in test_episodes_data:\n",
    "                support_imgs, support_labels, query_imgs, query_labels = episode\n",
    "                \n",
    "                support_imgs = support_imgs.to(DEVICE)\n",
    "                support_labels = support_labels.to(DEVICE)\n",
    "                query_imgs = query_imgs.to(DEVICE)\n",
    "                query_labels = query_labels.to(DEVICE)\n",
    "                \n",
    "                support_emb = model(support_imgs)\n",
    "                query_emb = model(query_imgs)\n",
    "                \n",
    "                logits = test_dist_fn(query_emb, support_emb)\n",
    "                \n",
    "                preds = logits.argmax(dim=1)\n",
    "                pred_labels = support_labels[preds]\n",
    "                total_correct += (pred_labels == query_labels).sum().item()\n",
    "                total_total += query_labels.size(0)\n",
    "            \n",
    "            accuracy = 100.0 * total_correct / total_total\n",
    "            results_per_size[seg_size].append(accuracy)\n",
    "            # print(f\"    D={seg_size}: {accuracy:.2f}%\")\n",
    "            \n",
    "    # Clean up\n",
    "    del model, optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Aggregate Results\n",
    "final_results = []\n",
    "print(\"\\n=== Final Aggregated Results ===\")\n",
    "for seg_size in segment_sizes:\n",
    "    accs = results_per_size[seg_size]\n",
    "    mean_acc = np.mean(accs)\n",
    "    std_acc = np.std(accs)\n",
    "    print(f\"D={seg_size}: {mean_acc:.2f}% ± {std_acc:.2f}%\")\n",
    "    \n",
    "    final_results.append({\n",
    "        \"Segment_Size_D\": seg_size,\n",
    "        \"Mean_Accuracy\": mean_acc,\n",
    "        \"Std_Dev\": std_acc,\n",
    "        \"All_Accuracies\": str(accs)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "df = pd.DataFrame(final_results)\n",
    "df.to_csv(\"segcos_results.csv\", index=False)\n",
    "print(\"\\nResults saved to segcos_results.csv\")\n",
    "\n",
    "# Plot with Error Bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(df[\"Segment_Size_D\"], df[\"Mean_Accuracy\"], yerr=df[\"Std_Dev\"], \n",
    "             marker='o', capsize=5, linestyle='-', linewidth=2, markersize=8)\n",
    "\n",
    "plt.xscale('log', base=2)\n",
    "plt.xlabel('Segment Size (D)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title(f'{DATASET} {N_WAY}-way {K_SHOT}-shot Accuracy vs Segment Size\\n(Trained with Cosine, Tested with SegCos)')\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.savefig(\"segcos_plot.png\")\n",
    "print(\"Plot saved to segcos_plot.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
