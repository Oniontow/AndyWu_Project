{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95109b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import functools\n",
    "\n",
    "# Kaggle-specific: Ensure we can download data if not present\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea54631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "DATASET = 'omniglot'\n",
    "N_WAY = 32\n",
    "K_SHOT = 1\n",
    "# 32-way 1-shot with 5 query means 32*(1+5) = 192 images per episode.\n",
    "N_QUERY = 5 \n",
    "NUM_EPOCHS = 500 \n",
    "EPISODES_PER_EPOCH = 50 \n",
    "TEST_EPISODES = 50 \n",
    "OUTPUT_DIM = 1024 # Fixed to 1024 to support segment_size up to 1024\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Distance Function (The Experiment)\n",
    "# -------------------------------\n",
    "def segmented_cosine_similarity(a, b, segment_size=2):\n",
    "    \"\"\"\n",
    "    Segmented Cosine Similarity with Sliding Window (Circular).\n",
    "    Segments are formed by sliding a window of size 'segment_size' over the vector with wrap-around.\n",
    "    Number of segments equals the vector dimension.\n",
    "    \n",
    "    Formula:\n",
    "    Sim = Sum_{i=1}^{N} ( (|q_i|*|s_i|) / (segment_size * |q|*|s|) * Cos(q_i, s_i) )\n",
    "    \"\"\"\n",
    "    batch_q, dim = a.size()\n",
    "    batch_s, _ = b.size()\n",
    "    \n",
    "    # 1. Circular Padding to handle sliding window wrap-around\n",
    "    # If D=1, no padding needed, or pad 0 is fine.\n",
    "    # If D=2, we need index [N-1, 0] so we pad index 0 at the end. (Pad 1)\n",
    "    # If D=K, we need up to [N-1, 0, ..., K-2], so we pad K-1 elements.\n",
    "    if segment_size > 1:\n",
    "        pad_size = segment_size - 1\n",
    "        # Pad the last 'pad_size' elements with the first 'pad_size' elements\n",
    "        a_padded = torch.cat([a, a[:, :pad_size]], dim=1)\n",
    "        b_padded = torch.cat([b, b[:, :pad_size]], dim=1)\n",
    "    else:\n",
    "        a_padded = a\n",
    "        b_padded = b\n",
    "        \n",
    "    # 2. Unfold to get sliding windows\n",
    "    # Shape: [Batch, Num_Segments, Segment_Size]\n",
    "    # Num_Segments will be 'dim' because step=1\n",
    "    a_seg = a_padded.unfold(dimension=1, size=segment_size, step=1)\n",
    "    b_seg = b_padded.unfold(dimension=1, size=segment_size, step=1)\n",
    "    \n",
    "    # Check if we got 'dim' segments\n",
    "    if a_seg.shape[1] != dim:\n",
    "         # Fallback or correction if logic slightly off\n",
    "         # Unfold: (Len - Size) / Step + 1. \n",
    "         # (Dim + D - 1 - D) / 1 + 1 = Dim / 1. Correct.\n",
    "         pass\n",
    "\n",
    "    # 3. Pairwise handling\n",
    "    # a: [Q, 1, N, D], b: [1, S, N, D]\n",
    "    a_exp = a_seg.unsqueeze(1)\n",
    "    b_exp = b_seg.unsqueeze(0)\n",
    "    \n",
    "    # 4. Dot product per segment: [Q, S, N]\n",
    "    # Sum over D dimension\n",
    "    dot_prod = (a_exp * b_exp).sum(dim=-1)\n",
    "    \n",
    "    # 5. Norm per segment: [Q, S, N] (Broadcasted)\n",
    "    norm_a_seg = torch.norm(a_exp, p=2, dim=-1) # [Q, 1, N]\n",
    "    norm_b_seg = torch.norm(b_exp, p=2, dim=-1) # [1, S, N]\n",
    "    \n",
    "    # 6. Cosine per segment\n",
    "    # Avoid division by zero\n",
    "    cos_seg = dot_prod / (norm_a_seg * norm_b_seg + 1e-8)\n",
    "    \n",
    "    # 7. Weights: (|q_i| |s_i|) / (D * |q| |s|)\n",
    "    # Global norms\n",
    "    norm_a = torch.norm(a, p=2, dim=1).view(batch_q, 1) # [Q, 1]\n",
    "    norm_b = torch.norm(b, p=2, dim=1).view(1, batch_s) # [1, S]\n",
    "    \n",
    "    numerator = norm_a_seg * norm_b_seg # [Q, S, N]\n",
    "    denominator = segment_size * norm_a.unsqueeze(2) * norm_b.unsqueeze(2) + 1e-8\n",
    "    \n",
    "    weights = numerator / denominator\n",
    "    \n",
    "    # 8. Weighted sum over all N segments\n",
    "    sim = (weights * cos_seg).sum(dim=2) # [Q, S]\n",
    "    \n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Helper Functions\n",
    "# -------------------------------\n",
    "def quantize_to_int8(tensor, scale=None):\n",
    "    if scale is None:\n",
    "        abs_max = torch.max(torch.abs(tensor)).item()\n",
    "        scale = 127.0 / (abs_max + 1e-8)\n",
    "    scaled = tensor * scale\n",
    "    quantized = scaled.detach().round().clamp(-128, 127) - scaled.detach() + scaled\n",
    "    return quantized, scale\n",
    "\n",
    "def dequantize_from_int8(quantized, scale):\n",
    "    return quantized.float() / scale\n",
    "\n",
    "def organize_by_class(dataset):\n",
    "    data_by_class = {}\n",
    "    for img, label in dataset:\n",
    "        if label not in data_by_class:\n",
    "            data_by_class[label] = []\n",
    "        data_by_class[label].append(img)\n",
    "    return data_by_class\n",
    "\n",
    "def sample_episode(data_by_class, n_way, k_shot, n_query):\n",
    "    selected_classes = random.sample(list(data_by_class.keys()), n_way)\n",
    "    support_images, support_labels, query_images, query_labels = [], [], [], []\n",
    "    for idx, cls in enumerate(selected_classes):\n",
    "        images = data_by_class[cls]\n",
    "        if len(images) < (k_shot + n_query):\n",
    "            images = images * ((k_shot + n_query) // len(images) + 1)\n",
    "        selected_imgs = random.sample(images, k_shot + n_query)\n",
    "        support_images += selected_imgs[:k_shot]\n",
    "        support_labels += [idx] * k_shot\n",
    "        query_images += selected_imgs[k_shot:]\n",
    "        query_labels += [idx] * n_query\n",
    "    return (\n",
    "        torch.stack(support_images),\n",
    "        torch.tensor(support_labels),\n",
    "        torch.stack(query_images),\n",
    "        torch.tensor(query_labels),\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# Model (Matched to mix/code.py)\n",
    "# -------------------------------\n",
    "class MANN(nn.Module):\n",
    "    def __init__(self, dataset, out_dim=1024, quantize=False):\n",
    "        super().__init__()\n",
    "        in_channels = 1 if dataset in ['omniglot', 'mnist'] else 3\n",
    "        planes = [128, 128, 128, 128] if dataset in ['omniglot', 'mnist'] else [128, 256, 512, 1024]\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, planes[0], kernel_size=5, stride=2, padding=2, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(planes[0], planes[1], kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(planes[1], planes[2], kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(planes[2], planes[3], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(planes[3] * 7 * 7, out_dim)  # Adjusted for input size\n",
    "        self.quantize = quantize\n",
    "        self.scale = None  # 用於存儲縮放因子\n",
    "\n",
    "    def forward(self, x, apply_quantize=None):\n",
    "        if apply_quantize is None:\n",
    "            apply_quantize = self.quantize\n",
    "            \n",
    "        emb = self.fc(self.model(x).view(x.size(0), -1))\n",
    "        \n",
    "        if apply_quantize:\n",
    "            # 量化 embedding\n",
    "            quantized_emb, self.scale = quantize_to_int8(emb, self.scale)\n",
    "            # 立即反量化回浮點，以便後續處理\n",
    "            emb = dequantize_from_int8(quantized_emb, self.scale)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Experiment logic\n",
    "# -------------------------------\n",
    "\n",
    "print(f\"Setting up {DATASET} experiment\")\n",
    "print(f\"N_WAY={N_WAY}, K_SHOT={K_SHOT}, OUTPUT_DIM={OUTPUT_DIM}\")\n",
    "\n",
    "# Data Setup\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: 1.0 - x)\n",
    "])\n",
    "\n",
    "# Use a local directory for data. In Kaggle, this will be /kaggle/working/data\n",
    "# You can also look for input data in /kaggle/input if you have added the dataset\n",
    "data_root = './data'\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to download/load dataset to {data_root}...\")\n",
    "    train_dataset = torchvision.datasets.Omniglot(root=data_root, background=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.Omniglot(root=data_root, background=False, download=True, transform=transform)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Download might have failed: {e}\")\n",
    "    print(\"If running on Kaggle without internet, please add the 'Omniglot' dataset to your notebook input.\")\n",
    "    # Fallback to checking /kaggle/input if possible, but torchvision structure is specific.\n",
    "    # We will assume download works or user handles data.\n",
    "\n",
    "train_data = organize_by_class(train_dataset)\n",
    "test_data = organize_by_class(test_dataset)\n",
    "\n",
    "# Sweep Parameters\n",
    "segment_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "\n",
    "results = []\n",
    "\n",
    "for seg_size in segment_sizes:\n",
    "    print(f\"\\n=========================================\")\n",
    "    print(f\"Running Experiment with Segment Size D={seg_size}\")\n",
    "    print(f\"=========================================\")\n",
    "    \n",
    "    # 1. Initialize Model\n",
    "    model = MANN(dataset=DATASET, out_dim=OUTPUT_DIM).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 2. Define Distance Function for this run\n",
    "    dist_fn = functools.partial(segmented_cosine_similarity, segment_size=seg_size)\n",
    "    \n",
    "    # 3. Training Loop\n",
    "    model.train()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        for ep in range(EPISODES_PER_EPOCH):\n",
    "            support_imgs, support_labels, query_imgs, query_labels = sample_episode(\n",
    "                train_data, N_WAY, K_SHOT, N_QUERY)\n",
    "            \n",
    "            support_imgs = support_imgs.to(DEVICE)\n",
    "            support_labels = support_labels.to(DEVICE)\n",
    "            query_imgs = query_imgs.to(DEVICE)\n",
    "            query_labels = query_labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            support_emb = model(support_imgs)\n",
    "            query_emb = model(query_imgs)\n",
    "            \n",
    "            # Similarity\n",
    "            logits = dist_fn(query_emb, support_emb)\n",
    "            \n",
    "            loss = criterion(logits, query_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {epoch_loss/EPISODES_PER_EPOCH:.4f}\")\n",
    "\n",
    "    # 4. Testing Loop\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_total = 0\n",
    "    with torch.no_grad():\n",
    "        for ep in range(TEST_EPISODES):\n",
    "            support_imgs, support_labels, query_imgs, query_labels = sample_episode(\n",
    "                test_data, N_WAY, K_SHOT, N_QUERY)\n",
    "                \n",
    "            support_imgs = support_imgs.to(DEVICE)\n",
    "            support_labels = support_labels.to(DEVICE)\n",
    "            query_imgs = query_imgs.to(DEVICE)\n",
    "            query_labels = query_labels.to(DEVICE)\n",
    "            \n",
    "            support_emb = model(support_imgs)\n",
    "            query_emb = model(query_imgs)\n",
    "            \n",
    "            logits = dist_fn(query_emb, support_emb)\n",
    "            \n",
    "            preds = logits.argmax(dim=1)\n",
    "            pred_labels = support_labels[preds]\n",
    "            total_correct += (pred_labels == query_labels).sum().item()\n",
    "            total_total += query_labels.size(0)\n",
    "    \n",
    "    accuracy = 100.0 * total_correct / total_total\n",
    "    print(f\"  Accuracy for D={seg_size}: {accuracy:.2f}%\")\n",
    "    \n",
    "    results.append({\n",
    "        \"Segment_Size_D\": seg_size,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "    \n",
    "    del model, optimizer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"segcos_results.csv\", index=False)\n",
    "print(\"\\nResults saved to segcos_results.csv\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"Segment_Size_D\"], df[\"Accuracy\"], marker='o')\n",
    "plt.xscale('log', base=2)\n",
    "plt.xlabel('Segment Size (D)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title(f'{DATASET} {N_WAY}-way {K_SHOT}-shot Accuracy vs Segment Size')\n",
    "plt.grid(True)\n",
    "plt.savefig(\"segcos_plot.png\")\n",
    "print(\"Plot saved to segcos_plot.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
