{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05eba1cf",
   "metadata": {},
   "source": [
    "## 2. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a51583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import umap\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "# 设置设备\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 参数设置\n",
    "SIFT_DIM = 128\n",
    "REDUCED_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d4f1a",
   "metadata": {},
   "source": [
    "## 3. 下载和解压 SIFT1M 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sift1m():\n",
    "    \"\"\"\n",
    "    下载SIFT1M数据集\n",
    "    \"\"\"\n",
    "    # 创建数据目录\n",
    "    os.makedirs('sift1m', exist_ok=True)\n",
    "    \n",
    "    files = [\n",
    "        \"sift_base.fvecs\",\n",
    "        \"sift_query.fvecs\",\n",
    "        \"sift_groundtruth.ivecs\"\n",
    "    ]\n",
    "    \n",
    "    # 检查文件是否已存在\n",
    "    all_exist = True\n",
    "    for filename in files:\n",
    "        if not os.path.exists(os.path.join('sift1m', filename)):\n",
    "            all_exist = False\n",
    "            break\n",
    "            \n",
    "    if all_exist:\n",
    "        print(\"所有文件已存在，跳过下载\")\n",
    "        return True\n",
    "        \n",
    "    print(\"正在下载 SIFT1M 数据集...\")\n",
    "    \n",
    "    # 尝试下载 tar.gz 文件\n",
    "    tar_url = \"ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz\"\n",
    "    tar_path = \"sift1m/sift.tar.gz\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"尝试从 {tar_url} 下载...\")\n",
    "        # 增加 timeout\n",
    "        import socket\n",
    "        socket.setdefaulttimeout(30)\n",
    "        urllib.request.urlretrieve(tar_url, tar_path)\n",
    "        print(\"下载完成，正在解压...\")\n",
    "        \n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=\"sift1m\")\n",
    "            \n",
    "        # 移动文件到 sift1m 根目录 (解压后会在 sift/ 目录下)\n",
    "        extracted_dir = os.path.join(\"sift1m\", \"sift\")\n",
    "        if os.path.exists(extracted_dir):\n",
    "            for filename in files:\n",
    "                src = os.path.join(extracted_dir, filename)\n",
    "                dst = os.path.join(\"sift1m\", filename)\n",
    "                if os.path.exists(src):\n",
    "                    if os.path.exists(dst):\n",
    "                        os.remove(dst)\n",
    "                    os.rename(src, dst)\n",
    "            # 清理\n",
    "            try:\n",
    "                import shutil\n",
    "                shutil.rmtree(extracted_dir)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # 删除 tar 文件\n",
    "        if os.path.exists(tar_path):\n",
    "            os.remove(tar_path)\n",
    "        print(\"数据集准备完成\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"FTP下载失败: {e}\")\n",
    "        print(\"尝试使用 wget 下载...\")\n",
    "        \n",
    "        try:\n",
    "            # 尝试使用 wget\n",
    "            res = os.system(f\"wget {tar_url} -O {tar_path}\")\n",
    "            if res == 0 and os.path.exists(tar_path):\n",
    "                print(\"wget 下载成功，正在解压...\")\n",
    "                with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "                    tar.extractall(path=\"sift1m\")\n",
    "                \n",
    "                # 移动文件\n",
    "                extracted_dir = os.path.join(\"sift1m\", \"sift\")\n",
    "                if os.path.exists(extracted_dir):\n",
    "                    for filename in files:\n",
    "                        src = os.path.join(extracted_dir, filename)\n",
    "                        dst = os.path.join(\"sift1m\", filename)\n",
    "                        if os.path.exists(src):\n",
    "                            if os.path.exists(dst):\n",
    "                                os.remove(dst)\n",
    "                            os.rename(src, dst)\n",
    "                    try:\n",
    "                        import shutil\n",
    "                        shutil.rmtree(extracted_dir)\n",
    "                    except:\n",
    "                        pass\n",
    "                if os.path.exists(tar_path):\n",
    "                    os.remove(tar_path)\n",
    "                return True\n",
    "        except Exception as e2:\n",
    "            print(f\"wget 失败: {e2}\")\n",
    "            \n",
    "        print(\"无法自动下载数据集。\")\n",
    "        print(\"请手动下载 sift.tar.gz 从 http://corpus-texmex.irisa.fr/\")\n",
    "        print(\"并解压到 sift1m/ 目录下。\")\n",
    "        return False\n",
    "\n",
    "# 下载数据集\n",
    "print(\"开始下载SIFT1M数据集...\")\n",
    "download_sift1m()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692adc9a",
   "metadata": {},
   "source": [
    "## 4. INT3 量化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_to_int3(data):\n",
    "    \"\"\"\n",
    "    將數據量化到INT3 (-4 到 3，共8個離散值)\n",
    "    改進：使用百分位數 (1% - 99%) 進行截斷，減少極端值對量化的影響\n",
    "    支援 PyTorch GPU 運算\n",
    "    \"\"\"\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        # PyTorch implementation\n",
    "        # Ensure data is on the correct device\n",
    "        if data.device != DEVICE:\n",
    "            data = data.to(DEVICE)\n",
    "            \n",
    "        arr = data.float() # Ensure float for quantile\n",
    "        \n",
    "        # Calculate percentiles on GPU\n",
    "        # Note: quantile requires float\n",
    "        # Optimization: For large tensors, estimate quantiles from a subset\n",
    "        # torch.quantile can fail on very large tensors (RuntimeError: quantile() input tensor is too large)\n",
    "        if arr.numel() > 100000:\n",
    "            # Sample ~100k elements uniformly\n",
    "            step = max(1, arr.numel() // 100000)\n",
    "            # Use view(-1) to flatten and slice\n",
    "            sample = arr.view(-1)[::step]\n",
    "            min_val = torch.quantile(sample, 0.01)\n",
    "            max_val = torch.quantile(sample, 0.99)\n",
    "        else:\n",
    "            min_val = torch.quantile(arr, 0.01)\n",
    "            max_val = torch.quantile(arr, 0.99)\n",
    "        \n",
    "        # Scale to [-4, 3]\n",
    "        scaled = (arr - min_val) / (max_val - min_val + 1e-8) * 7 - 4\n",
    "        \n",
    "        # Round and clip\n",
    "        quantized = torch.clamp(torch.round(scaled), -4, 3).to(torch.int8)\n",
    "        return quantized\n",
    "    else:\n",
    "        # Numpy implementation\n",
    "        arr = data\n",
    "        # Similar optimization for numpy to speed up\n",
    "        if arr.size > 100000:\n",
    "             step = max(1, arr.size // 100000)\n",
    "             sample = arr.ravel()[::step]\n",
    "             min_val = np.percentile(sample, 1)\n",
    "             max_val = np.percentile(sample, 99)\n",
    "        else:\n",
    "             min_val = np.percentile(arr, 1)\n",
    "             max_val = np.percentile(arr, 99)\n",
    "             \n",
    "        scaled = (arr - min_val) / (max_val - min_val + 1e-8) * 7 - 4\n",
    "        quantized = np.clip(np.round(scaled), -4, 3).astype(np.int8)\n",
    "        return quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea22ff0",
   "metadata": {},
   "source": [
    "## 5. AutoEncoder 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=128, latent_dim=64):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder (加深並加入 BatchNorm)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 96),\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 96),\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "def train_autoencoder(data, input_dim=128, latent_dim=64, epochs=100, batch_size=256, patience=5, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    訓練自編碼器 (加入 Early Stopping)\n",
    "    \"\"\"\n",
    "    model = AutoEncoder(input_dim, latent_dim).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(data).to(DEVICE)\n",
    "    \n",
    "    # 創建 DataLoader 以便於 batch 處理\n",
    "    dataset = torch.utils.data.TensorDataset(data_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    pbar = tqdm(range(epochs), desc=\"训练AutoEncoder\")\n",
    "    for epoch in pbar:\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch_data = batch[0]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            _, reconstructed = model(batch_data)\n",
    "            loss = criterion(reconstructed, batch_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        pbar.set_postfix({'loss': f'{avg_loss:.6f}'})\n",
    "        \n",
    "        # Early Stopping Check\n",
    "        if avg_loss < best_loss - min_delta:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1} (Best Loss: {best_loss:.6f})\")\n",
    "            break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97cf329",
   "metadata": {},
   "source": [
    "## 6. 五种数据处理方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6cb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method1_direct_int3(data):\n",
    "    \"\"\"方法1: 直接將128維向量量化為INT3\"\"\"\n",
    "    # Convert to Tensor if numpy\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = torch.from_numpy(data).to(DEVICE)\n",
    "    return quantize_to_int3(data)\n",
    "\n",
    "def method2_average_pooling_int3(data):\n",
    "    \"\"\"方法2: 將相鄰兩維做平均，從128維降到64維，然後量化為INT3\"\"\"\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = torch.from_numpy(data).to(DEVICE)\n",
    "        \n",
    "    # Reshape (N, 64, 2)\n",
    "    reshaped = data.reshape(data.shape[0], 64, 2)\n",
    "    # PyTorch mean\n",
    "    averaged = torch.mean(reshaped.float(), dim=2)\n",
    "    return quantize_to_int3(averaged)\n",
    "\n",
    "def method3_pca_int3(data, query_data=None):\n",
    "    \"\"\"方法3: 使用PCA降維到64維，然後量化為INT3\"\"\"\n",
    "    print(\"  - 训练PCA (CPU)...\")\n",
    "    # PCA fitting is still better on CPU with sklearn for stability/ease\n",
    "    # But we can transform on GPU\n",
    "    \n",
    "    # Ensure data is numpy for sklearn fitting\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data_np = data.cpu().numpy()\n",
    "    else:\n",
    "        data_np = data\n",
    "        \n",
    "    # 使用 whiten=True 來標準化分量\n",
    "    pca = PCA(n_components=REDUCED_DIM, whiten=True)\n",
    "    pca.fit(data_np)\n",
    "    \n",
    "    # Prepare for GPU transform\n",
    "    mean = torch.from_numpy(pca.mean_).float().to(DEVICE)\n",
    "    components = torch.from_numpy(pca.components_).float().to(DEVICE)\n",
    "    explained_variance = torch.from_numpy(pca.explained_variance_).float().to(DEVICE)\n",
    "    \n",
    "    def transform_gpu(X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = torch.from_numpy(X).to(DEVICE)\n",
    "        X = X.float()\n",
    "        \n",
    "        # Center\n",
    "        X_centered = X - mean\n",
    "        # Project\n",
    "        X_transformed = torch.matmul(X_centered, components.T)\n",
    "        \n",
    "        # Whiten\n",
    "        if pca.whiten:\n",
    "            scale = torch.sqrt(explained_variance)\n",
    "            X_transformed = X_transformed / scale\n",
    "            \n",
    "        return X_transformed\n",
    "\n",
    "    data_reduced = transform_gpu(data_np)\n",
    "    data_quantized = quantize_to_int3(data_reduced)\n",
    "    \n",
    "    if query_data is not None:\n",
    "        query_reduced = transform_gpu(query_data)\n",
    "        query_quantized = quantize_to_int3(query_reduced)\n",
    "        return pca, data_quantized, query_quantized\n",
    "    \n",
    "    return pca, data_quantized\n",
    "\n",
    "def method4_max_pooling_int3(data, query_data=None):\n",
    "    \"\"\"方法4: 將相鄰兩維做最大絕對值池化 (Max Magnitude Pooling)，從128維降到64維，然後量化為INT3\"\"\"\n",
    "    \n",
    "    def max_magnitude_pool_torch(arr):\n",
    "        if isinstance(arr, np.ndarray):\n",
    "            arr = torch.from_numpy(arr).to(DEVICE)\n",
    "            \n",
    "        # Reshape to (N, 64, 2)\n",
    "        reshaped = arr.reshape(arr.shape[0], 64, 2)\n",
    "        a = reshaped[:, :, 0]\n",
    "        b = reshaped[:, :, 1]\n",
    "        # 比較絕對值大小\n",
    "        mask = torch.abs(a) >= torch.abs(b)\n",
    "        # 選擇絕對值較大的那個原始值\n",
    "        return torch.where(mask, a, b)\n",
    "\n",
    "    max_pooled = max_magnitude_pool_torch(data)\n",
    "    data_quantized = quantize_to_int3(max_pooled)\n",
    "    \n",
    "    if query_data is not None:\n",
    "        q_max_pooled = max_magnitude_pool_torch(query_data)\n",
    "        query_quantized = quantize_to_int3(q_max_pooled)\n",
    "        return None, data_quantized, query_quantized\n",
    "    \n",
    "    return None, data_quantized\n",
    "\n",
    "def method5_autoencoder_int3(data, query_data=None, epochs=100):\n",
    "    \"\"\"方法5: 使用AutoEncoder降維到64維，然後量化為INT3\"\"\"\n",
    "    print(\"  - 训练AutoEncoder...\")\n",
    "    # Ensure numpy for training (dataloader handles conversion)\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data_np = data.cpu().numpy()\n",
    "    else:\n",
    "        data_np = data\n",
    "        \n",
    "    # 這裡 epochs 預設為 100，配合 Early Stopping\n",
    "    ae_model = train_autoencoder(data_np, SIFT_DIM, REDUCED_DIM, epochs=epochs)\n",
    "    ae_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode on GPU\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data_tensor = torch.FloatTensor(data).to(DEVICE)\n",
    "        else:\n",
    "            data_tensor = data.float().to(DEVICE)\n",
    "            \n",
    "        data_reduced = ae_model.encode(data_tensor) # Returns tensor on GPU\n",
    "        data_quantized = quantize_to_int3(data_reduced)\n",
    "        \n",
    "        if query_data is not None:\n",
    "            if isinstance(query_data, np.ndarray):\n",
    "                query_tensor = torch.FloatTensor(query_data).to(DEVICE)\n",
    "            else:\n",
    "                query_tensor = query_data.float().to(DEVICE)\n",
    "                \n",
    "            query_reduced = ae_model.encode(query_tensor)\n",
    "            query_quantized = quantize_to_int3(query_reduced)\n",
    "            return ae_model, data_quantized, query_quantized\n",
    "    \n",
    "    return ae_model, data_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21951698",
   "metadata": {},
   "source": [
    "## 7. 距离计算和评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_l2_distances(queries, database):\n",
    "    \"\"\"計算L2距離\"\"\"\n",
    "    return pairwise_distances(queries, database, metric='euclidean')\n",
    "\n",
    "def evaluate_recall_at_k(distances, ground_truth, k_values=[1, 10, 100]):\n",
    "    \"\"\"計算Recall@K指標\"\"\"\n",
    "    sorted_indices = np.argsort(distances, axis=1)\n",
    "    \n",
    "    recalls = {}\n",
    "    for k in k_values:\n",
    "        top_k_predictions = sorted_indices[:, :k]\n",
    "        \n",
    "        query_recalls = []\n",
    "        for i in range(len(ground_truth)):\n",
    "            gt_i = ground_truth[i]\n",
    "            # Handle jagged arrays: check length\n",
    "            limit = min(k, len(gt_i))\n",
    "            \n",
    "            if limit == 0:\n",
    "                query_recalls.append(0.0)\n",
    "                continue\n",
    "                \n",
    "            true_neighbors = set(gt_i[:limit])\n",
    "            pred_neighbors = set(top_k_predictions[i])\n",
    "            intersection = true_neighbors & pred_neighbors\n",
    "            recall = len(intersection) / len(true_neighbors)\n",
    "            query_recalls.append(recall)\n",
    "        \n",
    "        recalls[f'recall@{k}'] = np.mean(query_recalls)\n",
    "    \n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a202e3",
   "metadata": {},
   "source": [
    "## 8. 加载 SIFT1M 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fvecs(filename):\n",
    "    \"\"\"讀取.fvecs格式文件\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        d = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "        f.seek(0)\n",
    "        # .fvecs 格式: 4 bytes int32 (dimension) + d * 4 bytes float32 (data)\n",
    "        # 读取为 float32，header 的 int32 会被读成一个 float32，reshape 后切片去掉即可\n",
    "        data = np.fromfile(f, dtype=np.float32)\n",
    "        data = data.reshape(-1, d + 1)\n",
    "        return data[:, 1:].copy()\n",
    "\n",
    "def read_ivecs(filename):\n",
    "    \"\"\"讀取.ivecs格式文件\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        d = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "        f.seek(0)\n",
    "        data = np.fromfile(f, dtype=np.int32)\n",
    "        data = data.reshape(-1, d + 1)\n",
    "        return data[:, 1:].copy()\n",
    "\n",
    "print(\"载入SIFT1M数据集...\")\n",
    "base_vectors = read_fvecs('sift1m/sift_base.fvecs')\n",
    "query_vectors = read_fvecs('sift1m/sift_query.fvecs')\n",
    "ground_truth = read_ivecs('sift1m/sift_groundtruth.ivecs')\n",
    "\n",
    "print(f\"Base vectors shape: {base_vectors.shape}\")\n",
    "print(f\"Query vectors shape: {query_vectors.shape}\")\n",
    "print(f\"Ground truth shape: {ground_truth.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70c0f9",
   "metadata": {},
   "source": [
    "## 9. 运行实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top_k_ground_truth(queries, database, k=100):\n",
    "    \"\"\"\n",
    "    計算每個查詢的前 K 個真實最近鄰居 (Ground Truth)\n",
    "    使用 GPU 加速計算，並分批處理 Query 以節省記憶體\n",
    "    \"\"\"\n",
    "    print(f\"正在計算 Top-{k} Ground Truth (Query: {len(queries)}, DB: {len(database)})...\")\n",
    "    \n",
    "    num_queries = len(queries)\n",
    "    query_batch_size = 100  # 每次處理 100 個 Query\n",
    "    \n",
    "    all_indices = []\n",
    "    \n",
    "    for i in tqdm(range(0, num_queries, query_batch_size), desc=\"Computing GT\"):\n",
    "        q_end = min(i + query_batch_size, num_queries)\n",
    "        q_batch = queries[i:q_end]\n",
    "        q_tensor = torch.FloatTensor(q_batch).to(DEVICE)\n",
    "        q_sq = torch.sum(q_tensor**2, dim=1, keepdim=True)\n",
    "        \n",
    "        dists_batch = []\n",
    "        \n",
    "        # 分批處理 DB\n",
    "        db_batch_size = 50000\n",
    "        for j in range(0, len(database), db_batch_size):\n",
    "            db_end = min(j + db_batch_size, len(database))\n",
    "            db_chunk = torch.FloatTensor(database[j:db_end]).to(DEVICE)\n",
    "            \n",
    "            db_sq_chunk = torch.sum(db_chunk**2, dim=1)\n",
    "            term2 = -2 * torch.matmul(q_tensor, db_chunk.t())\n",
    "            \n",
    "            dists_chunk = q_sq + db_sq_chunk + term2\n",
    "            dists_batch.append(dists_chunk.cpu())\n",
    "            \n",
    "            del db_chunk, db_sq_chunk, term2, dists_chunk\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        full_dists_batch = torch.cat(dists_batch, dim=1)\n",
    "        \n",
    "        # 取 Top K\n",
    "        _, indices = torch.topk(full_dists_batch, k=k, dim=1, largest=False)\n",
    "        all_indices.append(indices.numpy())\n",
    "        \n",
    "        del full_dists_batch, indices, q_tensor, q_sq\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return np.vstack(all_indices)\n",
    "\n",
    "def evaluate_recall_batched(query_vectors, db_vectors, gt_top_k, retrieval_depths, k_true_neighbors):\n",
    "    \"\"\"\n",
    "    分批計算距離並評估 Recall\n",
    "    使用 GPU 加速距離計算\n",
    "    Recall = (Retrieved & Top-K_GT) / K_GT\n",
    "    \"\"\"\n",
    "    max_depth = max(retrieval_depths)\n",
    "    num_queries = len(query_vectors)\n",
    "    query_batch_size = 100\n",
    "    \n",
    "    total_hits = {r: 0 for r in retrieval_depths}\n",
    "    \n",
    "    # Ensure inputs are tensors on GPU\n",
    "    if isinstance(query_vectors, np.ndarray):\n",
    "        query_vectors = torch.from_numpy(query_vectors).to(DEVICE)\n",
    "    elif query_vectors.device != DEVICE:\n",
    "        query_vectors = query_vectors.to(DEVICE)\n",
    "        \n",
    "    if isinstance(db_vectors, np.ndarray):\n",
    "        db_vectors = torch.from_numpy(db_vectors).to(DEVICE)\n",
    "    elif db_vectors.device != DEVICE:\n",
    "        db_vectors = db_vectors.to(DEVICE)\n",
    "    \n",
    "    # Ensure float for distance calc (INT3 needs to be float for calc)\n",
    "    if query_vectors.dtype != torch.float32:\n",
    "        query_vectors = query_vectors.float()\n",
    "    if db_vectors.dtype != torch.float32:\n",
    "        db_vectors = db_vectors.float()\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Pre-calculate DB squared norms if memory allows, or do it in chunks\n",
    "    # For 1M vectors, pre-calculating norms is fast and takes little memory (1M floats = 4MB)\n",
    "    db_sq = torch.sum(db_vectors**2, dim=1)\n",
    "    \n",
    "    for i in tqdm(range(0, num_queries, query_batch_size), desc=\"Evaluating Batches\", leave=False):\n",
    "        q_end = min(i + query_batch_size, num_queries)\n",
    "        q_batch = query_vectors[i:q_end] # (Batch, Dim)\n",
    "        gt_batch = gt_top_k[i:q_end]\n",
    "        \n",
    "        # Calculate L2 Distance on GPU: |x-y|^2 = |x|^2 + |y|^2 - 2xy\n",
    "        q_sq = torch.sum(q_batch**2, dim=1, keepdim=True) # (Batch, 1)\n",
    "        \n",
    "        # Matrix multiplication: (Batch, Dim) @ (Dim, DB_Size) -> (Batch, DB_Size)\n",
    "        # Note: db_vectors is (DB_Size, Dim), so we transpose it\n",
    "        # For very large DB, we might need to chunk this too, but 1M fits in GPU memory for matmul usually\n",
    "        # 100 * 1M * 4 bytes = 400MB result matrix. This is fine.\n",
    "        \n",
    "        term2 = -2 * torch.matmul(q_batch, db_vectors.t())\n",
    "        \n",
    "        # Broadcasting: (Batch, 1) + (DB_Size,) + (Batch, DB_Size)\n",
    "        dists = q_sq + db_sq + term2\n",
    "        \n",
    "        # Find Top-K on GPU\n",
    "        # We need the smallest distances\n",
    "        _, sorted_indices_tensor = torch.topk(dists, k=max_depth, dim=1, largest=False)\n",
    "        \n",
    "        # Move indices to CPU for set intersection (faster on CPU for small sets logic)\n",
    "        sorted_indices = sorted_indices_tensor.cpu().numpy()\n",
    "        \n",
    "        for r in retrieval_depths:\n",
    "            retrieved_indices = sorted_indices[:, :r]\n",
    "            for j in range(len(gt_batch)):\n",
    "                gt_set = set(gt_batch[j]) # 這是 Top K 真實鄰居\n",
    "                retrieved_set = set(retrieved_indices[j])\n",
    "                total_hits[r] += len(gt_set & retrieved_set)\n",
    "                \n",
    "        # Clean up intermediate tensors\n",
    "        del dists, term2, q_sq, sorted_indices_tensor\n",
    "        # torch.cuda.empty_cache() # Optional: can slow down loop if called too often\n",
    "    \n",
    "    end_time = time.time()\n",
    "    search_time = end_time - start_time\n",
    "                \n",
    "    recalls = {}\n",
    "    for r in retrieval_depths:\n",
    "        # Normalize by k_true_neighbors (e.g., 100)\n",
    "        recalls[f'recall@{r}'] = total_hits[r] / (num_queries * float(k_true_neighbors))\n",
    "        \n",
    "    recalls['search_time'] = search_time\n",
    "    recalls['qps'] = num_queries / search_time if search_time > 0 else 0\n",
    "        \n",
    "    return recalls\n",
    "\n",
    "def run_experiment_part(part_name, base_vectors, query_vectors, gt_top_k, retrieval_depths, k_true_neighbors):\n",
    "    \"\"\"\n",
    "    執行單個部分的實驗\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"實驗部分: {part_name}\")\n",
    "    print(f\"Database Size: {len(base_vectors)}\")\n",
    "    print(f\"Retrieval Depths: {retrieval_depths}\")\n",
    "    print(f\"Target GT Size (K): {k_true_neighbors}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    # 方法1\n",
    "    print(\"\\n[方法1] 直接INT3量化 (128維)...\")\n",
    "    t0 = time.time()\n",
    "    db_m1 = method1_direct_int3(base_vectors)\n",
    "    q_m1 = method1_direct_int3(query_vectors)\n",
    "    recalls_m1 = evaluate_recall_batched(q_m1, db_m1, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 1: Direct INT3', 'time': time.time()-t0, **recalls_m1})\n",
    "    print(f\"Recall: {recalls_m1}, QPS: {recalls_m1.get('qps', 0):.2f}\")\n",
    "    \n",
    "    # 方法2\n",
    "    print(\"\\n[方法2] 平均池化降維 + INT3 (64維)...\")\n",
    "    t0 = time.time()\n",
    "    db_m2 = method2_average_pooling_int3(base_vectors)\n",
    "    q_m2 = method2_average_pooling_int3(query_vectors)\n",
    "    recalls_m2 = evaluate_recall_batched(q_m2, db_m2, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 2: AvgPooling + INT3', 'time': time.time()-t0, **recalls_m2})\n",
    "    print(f\"Recall: {recalls_m2}, QPS: {recalls_m2.get('qps', 0):.2f}\")\n",
    "    \n",
    "    # 方法3\n",
    "    print(\"\\n[方法3] PCA降維 + INT3 (64維)...\")\n",
    "    t0 = time.time()\n",
    "    _, db_m3, q_m3 = method3_pca_int3(base_vectors, query_vectors)\n",
    "    recalls_m3 = evaluate_recall_batched(q_m3, db_m3, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 3: PCA + INT3', 'time': time.time()-t0, **recalls_m3})\n",
    "    print(f\"Recall: {recalls_m3}, QPS: {recalls_m3.get('qps', 0):.2f}\")\n",
    "    \n",
    "    # 方法4\n",
    "    print(f\"\\n[方法4] Max Magnitude Pooling降維 + INT3 (64維)...\")\n",
    "    t0 = time.time()\n",
    "    _, db_m4, q_m4 = method4_max_pooling_int3(base_vectors, query_vectors)\n",
    "    recalls_m4 = evaluate_recall_batched(q_m4, db_m4, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 4: Max Mag Pooling + INT3', 'time': time.time()-t0, **recalls_m4})\n",
    "    print(f\"Recall: {recalls_m4}, QPS: {recalls_m4.get('qps', 0):.2f}\")\n",
    "    \n",
    "    # 方法5\n",
    "    print(\"\\n[方法5] AutoEncoder降維 + INT3 (64維)...\")\n",
    "    t0 = time.time()\n",
    "    # Epochs 設為 100，配合 Early Stopping\n",
    "    _, db_m5, q_m5 = method5_autoencoder_int3(base_vectors, query_vectors, epochs=100)\n",
    "    recalls_m5 = evaluate_recall_batched(q_m5, db_m5, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 5: AutoEncoder + INT3', 'time': time.time()-t0, **recalls_m5})\n",
    "    print(f\"Recall: {recalls_m5}, QPS: {recalls_m5.get('qps', 0):.2f}\")\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行實驗\n",
    "# 實驗配置：\n",
    "# Part 1: 100K DB, Recall 100@1000, 100@5000, 100@10000\n",
    "# Part 2: 1M DB, Recall 100@1000, 100@5000, 100@10000\n",
    "\n",
    "all_experiment_results = []  # 存儲結果 (包含 Recall, Time, QPS)\n",
    "\n",
    "# --- Part 1: 100K Database ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Part 1: First 100K Database\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "db_100k = base_vectors[:100000].astype(np.float32)\n",
    "query_subset = query_vectors[:1000].astype(np.float32)\n",
    "\n",
    "# 計算 Ground Truth for 100K DB\n",
    "print(\"Computing Ground Truth for 100K DB...\")\n",
    "gt_100k = calculate_top_k_ground_truth(query_subset, db_100k, k=100)\n",
    "\n",
    "# 執行實驗 Part 1\n",
    "retrieval_depths_1 = [1000, 5000, 10000]\n",
    "k_true_neighbors = 100\n",
    "\n",
    "results_df_1 = run_experiment_part(\n",
    "    \"Part 1: 100K DB\",\n",
    "    db_100k,\n",
    "    query_subset,\n",
    "    gt_100k,\n",
    "    retrieval_depths_1,\n",
    "    k_true_neighbors\n",
    ")\n",
    "\n",
    "all_experiment_results.append(results_df_1)\n",
    "\n",
    "print(\"\\nPart 1 Results:\")\n",
    "display(results_df_1)\n",
    "\n",
    "# --- Part 2: 1M Database ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Part 2: Full 1M Database\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "db_1m = base_vectors.astype(np.float32)\n",
    "\n",
    "# 計算 Ground Truth for 1M DB\n",
    "print(\"Computing Ground Truth for 1M DB...\")\n",
    "gt_1m = calculate_top_k_ground_truth(query_subset, db_1m, k=100)\n",
    "\n",
    "# 執行實驗 Part 2\n",
    "retrieval_depths_2 = [1000, 5000, 10000]\n",
    "\n",
    "results_df_2 = run_experiment_part(\n",
    "    \"Part 2: 1M DB\",\n",
    "    db_1m,\n",
    "    query_subset,\n",
    "    gt_1m,\n",
    "    retrieval_depths_2,\n",
    "    k_true_neighbors\n",
    ")\n",
    "\n",
    "all_experiment_results.append(results_df_2)\n",
    "\n",
    "print(\"\\nPart 2 Results:\")\n",
    "display(results_df_2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"所有實驗完成！\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4ac29",
   "metadata": {},
   "source": [
    "## 10. 可视化结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a08e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化結果\n",
    "# 根據實驗結果的數量動態調整圖表\n",
    "\n",
    "# 確保變數名稱兼容性\n",
    "if 'all_experiment_results' not in locals() and 'all_results' in locals():\n",
    "    all_experiment_results = all_results\n",
    "\n",
    "num_experiments = len(all_experiment_results)\n",
    "print(f\"Detected {num_experiments} experiment results.\")\n",
    "\n",
    "if num_experiments > 0:\n",
    "    # 設定圖表佈局: 2列 (Recall, QPS) x N行 (實驗數量) 或者 N列 x 2行\n",
    "    # 這裡維持原本的邏輯: Row 1 = Recall, Row 2 = QPS\n",
    "    \n",
    "    # 計算需要的列數 (columns)\n",
    "    ncols = num_experiments\n",
    "    \n",
    "    fig, axes = plt.subplots(2, ncols, figsize=(6 * ncols, 12))\n",
    "    \n",
    "    # 如果只有一列，axes 是 1D array，需要轉為 2D 以便統一處理\n",
    "    if ncols == 1:\n",
    "        axes = np.array([[axes[0]], [axes[1]]]) \n",
    "        \n",
    "    fig.suptitle('Recall & QPS Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 實驗標題映射 (根據 Cell 9 的邏輯)\n",
    "    titles = []\n",
    "    if num_experiments == 2:\n",
    "        titles = ['100K DB', '1M DB']\n",
    "    elif num_experiments == 3:\n",
    "        titles = ['10K DB', '100K DB', '1M DB']\n",
    "    else:\n",
    "        titles = [f'Experiment {i+1}' for i in range(num_experiments)]\n",
    "\n",
    "    for i in range(num_experiments):\n",
    "        df = all_experiment_results[i]\n",
    "        title = titles[i] if i < len(titles) else f'Exp {i+1}'\n",
    "        \n",
    "        # --- Row 1: Recall ---\n",
    "        ax_recall = axes[0, i]\n",
    "        \n",
    "        # 準備數據\n",
    "        methods = df['method'].str.replace('Method ', 'M').str.replace(': ', '\\n', 1)\n",
    "        x = np.arange(len(df))\n",
    "        \n",
    "        # 檢查有哪些 Recall 指標\n",
    "        recall_cols = [c for c in df.columns if 'recall@' in c]\n",
    "        # 嘗試按 K 值排序\n",
    "        try:\n",
    "            recall_cols.sort(key=lambda x: int(x.split('@')[1])) \n",
    "        except:\n",
    "            pass # 如果格式不符就不排序\n",
    "        \n",
    "        # 繪製 Recall\n",
    "        if len(recall_cols) == 1:\n",
    "            # 單一 Recall\n",
    "            col = recall_cols[0]\n",
    "            bars = ax_recall.bar(x, df[col], color='#1f77b4')\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax_recall.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "            ax_recall.set_ylabel(col, fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            # 多個 Recall (Grouped Bar)\n",
    "            width = 0.8 / len(recall_cols)\n",
    "            for j, col in enumerate(recall_cols):\n",
    "                offset = (j - len(recall_cols)/2 + 0.5) * width\n",
    "                ax_recall.bar(x + offset, df[col], width, label=f'@{col.split(\"@\")[1]}')\n",
    "            ax_recall.legend()\n",
    "            ax_recall.set_ylabel('Recall', fontsize=12, fontweight='bold')\n",
    "\n",
    "        ax_recall.set_xticks(x)\n",
    "        ax_recall.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
    "        ax_recall.set_title(f'{title} (Recall)', fontsize=14, fontweight='bold')\n",
    "        ax_recall.set_ylim([0, 1.1])\n",
    "        ax_recall.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # --- Row 2: QPS ---\n",
    "        ax_qps = axes[1, i]\n",
    "        bars_qps = ax_qps.bar(x, df['qps'], color='teal')\n",
    "        \n",
    "        ax_qps.set_xticks(x)\n",
    "        ax_qps.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
    "        ax_qps.set_ylabel('QPS (Queries/sec)', fontsize=12, fontweight='bold')\n",
    "        ax_qps.set_title(f'{title} (QPS)', fontsize=14, fontweight='bold')\n",
    "        ax_qps.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        for bar in bars_qps:\n",
    "            height = bar.get_height()\n",
    "            ax_qps.text(bar.get_x() + bar.get_width()/2., height, f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92, hspace=0.4)\n",
    "    plt.savefig('sift1m_results_recall_qps.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # --- Save Data ---\n",
    "    print(\"\\nSaving results to CSV...\")\n",
    "    for i, df in enumerate(all_experiment_results):\n",
    "        safe_title = titles[i].replace(\" \", \"_\") if i < len(titles) else f\"part{i+1}\"\n",
    "        fname = f'sift1m_part{i+1}_{safe_title}.csv'\n",
    "        df.to_csv(fname, index=False)\n",
    "        print(f\"Saved {fname}\")\n",
    "else:\n",
    "    print(\"No results to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44c37c",
   "metadata": {},
   "source": [
    "## 11. 实验总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"實驗總結\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 確保變數名稱兼容性\n",
    "if 'all_experiment_results' not in locals() and 'all_results' in locals():\n",
    "    all_experiment_results = all_results\n",
    "\n",
    "if 'all_experiment_results' in locals():\n",
    "    for i, results_df in enumerate(all_experiment_results):\n",
    "        print(f\"\\nExperiment Part {i+1}\")\n",
    "        display(results_df)\n",
    "else:\n",
    "    print(\"No results found.\")\n",
    "\n",
    "print(\"\\n所有實驗完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Dimension Reduction Analysis (128 -> 16)\n",
    "# Analyzing Recall Rate vs Dimension for AvgPooling and PCA\n",
    "# Configuration: 1M Database, Recall of Top-1000 GT @ Depth 10000\n",
    "\n",
    "def adaptive_avg_pool_int3(data, target_dim):\n",
    "    \"\"\"\n",
    "    Adaptive Average Pooling to reduce to specific target dimension\n",
    "    \"\"\"\n",
    "    if isinstance(data, np.ndarray):\n",
    "        t = torch.from_numpy(data).float()\n",
    "    else:\n",
    "        t = data.float()\n",
    "    \n",
    "    t = t.to(DEVICE)\n",
    "    # Input: (N, 128) -> (N, 1, 128) for pooling\n",
    "    t = t.unsqueeze(1)\n",
    "    # Adaptive Pool to (N, 1, target_dim)\n",
    "    pooled = nn.functional.adaptive_avg_pool1d(t, target_dim)\n",
    "    # (N, target_dim)\n",
    "    pooled = pooled.squeeze(1)\n",
    "    \n",
    "    return quantize_to_int3(pooled)\n",
    "\n",
    "def run_dimension_sweep(base_vectors, query_vectors, gt_top_k, k_true_neighbors=1000, retrieval_depth=10000):\n",
    "    # Dimensions: 128, 112, 96, ..., 16\n",
    "    target_dims = list(range(128, 15, -16)) \n",
    "    results = []\n",
    "    \n",
    "    print(f\"Starting Dimension Sweep: {target_dims}\")\n",
    "    print(f\"Database Size: {len(base_vectors)}\")\n",
    "    print(f\"Target GT Size: {k_true_neighbors}\")\n",
    "    print(f\"Retrieval Depth: {retrieval_depth}\")\n",
    "    \n",
    "    for dim in target_dims:\n",
    "        print(f\"\\nTesting Target Dimension: {dim}\")\n",
    "        \n",
    "        # --- Method: Adaptive Avg Pooling ---\n",
    "        print(\"  Running Avg Pooling...\")\n",
    "        t0 = time.time()\n",
    "        db_avg = adaptive_avg_pool_int3(base_vectors, dim)\n",
    "        q_avg = adaptive_avg_pool_int3(query_vectors, dim)\n",
    "        \n",
    "        # Evaluate Recall\n",
    "        recalls_avg = evaluate_recall_batched(q_avg, db_avg, gt_top_k, [retrieval_depth], k_true_neighbors)\n",
    "        time_avg = time.time() - t0\n",
    "        \n",
    "        metric_key = f'recall@{retrieval_depth}'\n",
    "        results.append({\n",
    "            'Dimension': dim,\n",
    "            'Method': 'AvgPooling',\n",
    "            'Recall': recalls_avg[metric_key],\n",
    "            'Time': time_avg\n",
    "        })\n",
    "        print(f\"    AvgPool {metric_key}: {recalls_avg[metric_key]:.4f}\")\n",
    "        \n",
    "        # --- Method: PCA ---\n",
    "        print(\"  Running PCA...\")\n",
    "        t0 = time.time()\n",
    "        # Fit PCA on a subset for speed (max 50k)\n",
    "        fit_size = min(len(base_vectors), 50000)\n",
    "        pca = PCA(n_components=dim, whiten=True)\n",
    "        pca.fit(base_vectors[:fit_size])\n",
    "        \n",
    "        db_pca = pca.transform(base_vectors)\n",
    "        q_pca = pca.transform(query_vectors)\n",
    "        \n",
    "        db_pca_q = quantize_to_int3(db_pca)\n",
    "        q_pca_q = quantize_to_int3(q_pca)\n",
    "        \n",
    "        recalls_pca = evaluate_recall_batched(q_pca_q, db_pca_q, gt_top_k, [retrieval_depth], k_true_neighbors)\n",
    "        time_pca = time.time() - t0\n",
    "        \n",
    "        results.append({\n",
    "            'Dimension': dim,\n",
    "            'Method': 'PCA',\n",
    "            'Recall': recalls_pca[metric_key],\n",
    "            'Time': time_pca\n",
    "        })\n",
    "        print(f\"    PCA {metric_key}: {recalls_pca[metric_key]:.4f}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Setup Data for Sweep (Using 1M Full DB)\n",
    "# Ensure we have the data\n",
    "if 'base_vectors_full' not in locals():\n",
    "    base_vectors_full = base_vectors.astype(np.float32)\n",
    "    query_vectors_subset = query_vectors[:1000].astype(np.float32)\n",
    "\n",
    "# Use Full 1M Database\n",
    "db_subset_sweep = base_vectors_full \n",
    "k_gt = 1000\n",
    "depth = 10000\n",
    "\n",
    "print(f\"Computing GT for sweep (1M DB, Top-{k_gt})...\")\n",
    "# Always recalculate to be safe or check if existing gt matches requirements\n",
    "# Since previous cells might have calculated k=100, we likely need to recalc for k=1000\n",
    "gt_sweep = calculate_top_k_ground_truth(query_vectors_subset, db_subset_sweep, k=k_gt)\n",
    "\n",
    "# Run Sweep\n",
    "sweep_df = run_dimension_sweep(db_subset_sweep, query_vectors_subset, gt_sweep, k_true_neighbors=k_gt, retrieval_depth=depth)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "avg_df = sweep_df[sweep_df['Method'] == 'AvgPooling']\n",
    "pca_df = sweep_df[sweep_df['Method'] == 'PCA']\n",
    "\n",
    "plt.plot(avg_df['Dimension'], avg_df['Recall'], marker='o', linewidth=2, label='Avg Pooling + INT3')\n",
    "plt.plot(pca_df['Dimension'], pca_df['Recall'], marker='s', linewidth=2, label='PCA + INT3')\n",
    "\n",
    "plt.xlabel('Dimension', fontsize=12)\n",
    "plt.ylabel(f'Recall@{depth} (Top-{k_gt} GT)', fontsize=12)\n",
    "plt.title(f'Recall vs Dimension (1M DB, {k_gt}@{depth})', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.gca().invert_xaxis() # Display 128 on left, 16 on right\n",
    "plt.xticks(list(range(128, 15, -16)))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save Results\n",
    "csv_filename = 'sift1m_1M_1000at10000_sweep_results.csv'\n",
    "sweep_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Sweep results saved to {csv_filename}\")\n",
    "sweep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fe3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. PCA Dimension Difference Analysis\n",
    "# Analyze absolute difference between Query and Top-100 NN in PCA space per dimension\n",
    "\n",
    "def analyze_pca_diff(base_vectors, query_vectors, gt_indices, n_components=128):\n",
    "    print(\"Fitting PCA...\")\n",
    "    # Fit PCA on a subset for speed\n",
    "    fit_size = min(len(base_vectors), 50000)\n",
    "    pca = PCA(n_components=n_components, whiten=True)\n",
    "    pca.fit(base_vectors[:fit_size])\n",
    "    \n",
    "    print(\"Transforming data...\")\n",
    "    # Transform all needed data\n",
    "    # We only need the specific base vectors that are in the GT of the queries\n",
    "    # But transforming all might be easier if memory allows (1M * 128 * 4 bytes ~ 512MB)\n",
    "    q_pca = pca.transform(query_vectors)\n",
    "    db_pca = pca.transform(base_vectors)\n",
    "    \n",
    "    n_queries = len(query_vectors)\n",
    "    k_neighbors = gt_indices.shape[1] # Should be 100\n",
    "    n_dims = n_components\n",
    "    \n",
    "    # Array to store sum of absolute differences per dimension\n",
    "    total_abs_diff = np.zeros(n_dims)\n",
    "    count = 0\n",
    "    \n",
    "    print(\"Calculating differences...\")\n",
    "    for i in tqdm(range(n_queries), desc=\"Analyzing Queries\"):\n",
    "        q_vec = q_pca[i] # (128,)\n",
    "        neighbor_indices = gt_indices[i] # (100,)\n",
    "        \n",
    "        # Get neighbor vectors\n",
    "        neighbor_vecs = db_pca[neighbor_indices] # (100, 128)\n",
    "        \n",
    "        # Calculate absolute difference\n",
    "        # |q - n|\n",
    "        abs_diff = np.abs(neighbor_vecs - q_vec) # (100, 128)\n",
    "        \n",
    "        # Sum over neighbors\n",
    "        total_abs_diff += np.sum(abs_diff, axis=0)\n",
    "        count += k_neighbors\n",
    "        \n",
    "    # Mean absolute difference per dimension\n",
    "    mean_abs_diff = total_abs_diff / count\n",
    "    \n",
    "    return mean_abs_diff, pca.explained_variance_ratio_\n",
    "\n",
    "# Setup Data\n",
    "if 'base_vectors_full' not in locals():\n",
    "    base_vectors_full = base_vectors.astype(np.float32)\n",
    "    query_vectors_subset = query_vectors[:1000].astype(np.float32)\n",
    "\n",
    "# Ensure GT for Top-100 exists\n",
    "k_target = 100\n",
    "if 'gt_sweep' in locals() and gt_sweep.shape[1] >= k_target:\n",
    "    gt_100 = gt_sweep[:, :k_target]\n",
    "else:\n",
    "    print(\"Computing GT for Top-100...\")\n",
    "    gt_100 = calculate_top_k_ground_truth(query_vectors_subset, base_vectors_full, k=k_target)\n",
    "\n",
    "# Run Analysis\n",
    "mean_diffs, explained_var = analyze_pca_diff(base_vectors_full, query_vectors_subset, gt_100)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Mean Absolute Difference\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, 129), mean_diffs, color='skyblue')\n",
    "plt.xlabel('PCA Component (Dimension)')\n",
    "plt.ylabel('Mean Absolute Difference')\n",
    "plt.title('Mean Abs Diff between Query and Top-100 NN per PCA Dimension')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot 2: Explained Variance (for context)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 129), explained_var, marker='.', linestyle='-', color='orange')\n",
    "plt.xlabel('PCA Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Explained Variance Ratio')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save to CSV\n",
    "diff_df = pd.DataFrame({\n",
    "    'Dimension': range(1, 129),\n",
    "    'Mean_Abs_Diff': mean_diffs,\n",
    "    'Explained_Variance': explained_var\n",
    "})\n",
    "diff_df.to_csv('sift1m_pca_128_diff_analysis.csv', index=False)\n",
    "print(\"Analysis saved to sift1m_pca_128_diff_analysis.csv\")\n",
    "diff_df.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
