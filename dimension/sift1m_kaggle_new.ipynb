{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05eba1cf",
   "metadata": {},
   "source": [
    "## 2. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a51583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import umap\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "# 设置设备\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 参数设置\n",
    "SIFT_DIM = 128\n",
    "REDUCED_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d4f1a",
   "metadata": {},
   "source": [
    "## 3. 下载和解压 SIFT1M 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sift1m():\n",
    "    \"\"\"\n",
    "    下载SIFT1M数据集\n",
    "    \"\"\"\n",
    "    # 创建数据目录\n",
    "    os.makedirs('sift1m', exist_ok=True)\n",
    "    \n",
    "    files = [\n",
    "        \"sift_base.fvecs\",\n",
    "        \"sift_query.fvecs\",\n",
    "        \"sift_groundtruth.ivecs\"\n",
    "    ]\n",
    "    \n",
    "    # 检查文件是否已存在\n",
    "    all_exist = True\n",
    "    for filename in files:\n",
    "        if not os.path.exists(os.path.join('sift1m', filename)):\n",
    "            all_exist = False\n",
    "            break\n",
    "            \n",
    "    if all_exist:\n",
    "        print(\"所有文件已存在，跳过下载\")\n",
    "        return True\n",
    "        \n",
    "    print(\"正在下载 SIFT1M 数据集...\")\n",
    "    \n",
    "    # 尝试下载 tar.gz 文件\n",
    "    tar_url = \"ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz\"\n",
    "    tar_path = \"sift1m/sift.tar.gz\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"尝试从 {tar_url} 下载...\")\n",
    "        # 增加 timeout\n",
    "        import socket\n",
    "        socket.setdefaulttimeout(30)\n",
    "        urllib.request.urlretrieve(tar_url, tar_path)\n",
    "        print(\"下载完成，正在解压...\")\n",
    "        \n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=\"sift1m\")\n",
    "            \n",
    "        # 移动文件到 sift1m 根目录 (解压后会在 sift/ 目录下)\n",
    "        extracted_dir = os.path.join(\"sift1m\", \"sift\")\n",
    "        if os.path.exists(extracted_dir):\n",
    "            for filename in files:\n",
    "                src = os.path.join(extracted_dir, filename)\n",
    "                dst = os.path.join(\"sift1m\", filename)\n",
    "                if os.path.exists(src):\n",
    "                    if os.path.exists(dst):\n",
    "                        os.remove(dst)\n",
    "                    os.rename(src, dst)\n",
    "            # 清理\n",
    "            try:\n",
    "                import shutil\n",
    "                shutil.rmtree(extracted_dir)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # 删除 tar 文件\n",
    "        if os.path.exists(tar_path):\n",
    "            os.remove(tar_path)\n",
    "        print(\"数据集准备完成\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"FTP下载失败: {e}\")\n",
    "        print(\"尝试使用 wget 下载...\")\n",
    "        \n",
    "        try:\n",
    "            # 尝试使用 wget\n",
    "            res = os.system(f\"wget {tar_url} -O {tar_path}\")\n",
    "            if res == 0 and os.path.exists(tar_path):\n",
    "                print(\"wget 下载成功，正在解压...\")\n",
    "                with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "                    tar.extractall(path=\"sift1m\")\n",
    "                \n",
    "                # 移动文件\n",
    "                extracted_dir = os.path.join(\"sift1m\", \"sift\")\n",
    "                if os.path.exists(extracted_dir):\n",
    "                    for filename in files:\n",
    "                        src = os.path.join(extracted_dir, filename)\n",
    "                        dst = os.path.join(\"sift1m\", filename)\n",
    "                        if os.path.exists(src):\n",
    "                            if os.path.exists(dst):\n",
    "                                os.remove(dst)\n",
    "                            os.rename(src, dst)\n",
    "                    try:\n",
    "                        import shutil\n",
    "                        shutil.rmtree(extracted_dir)\n",
    "                    except:\n",
    "                        pass\n",
    "                if os.path.exists(tar_path):\n",
    "                    os.remove(tar_path)\n",
    "                return True\n",
    "        except Exception as e2:\n",
    "            print(f\"wget 失败: {e2}\")\n",
    "            \n",
    "        print(\"无法自动下载数据集。\")\n",
    "        print(\"请手动下载 sift.tar.gz 从 http://corpus-texmex.irisa.fr/\")\n",
    "        print(\"并解压到 sift1m/ 目录下。\")\n",
    "        return False\n",
    "\n",
    "# 下载数据集\n",
    "print(\"开始下载SIFT1M数据集...\")\n",
    "download_sift1m()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692adc9a",
   "metadata": {},
   "source": [
    "## 4. INT3 量化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_to_int3(data):\n",
    "    \"\"\"\n",
    "    將數據量化到INT3 (-4 到 3，共8個離散值)\n",
    "    改進：使用百分位數 (1% - 99%) 進行截斷，減少極端值對量化的影響\n",
    "    支援 PyTorch GPU 運算\n",
    "    \"\"\"\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        # PyTorch implementation\n",
    "        # Ensure data is on the correct device\n",
    "        if data.device != DEVICE:\n",
    "            data = data.to(DEVICE)\n",
    "            \n",
    "        arr = data.float() # Ensure float for quantile\n",
    "        \n",
    "        # Calculate percentiles on GPU\n",
    "        # Note: quantile requires float\n",
    "        # Optimization: For large tensors, estimate quantiles from a subset\n",
    "        # torch.quantile can fail on very large tensors (RuntimeError: quantile() input tensor is too large)\n",
    "        if arr.numel() > 100000:\n",
    "            # Sample ~100k elements uniformly\n",
    "            step = max(1, arr.numel() // 100000)\n",
    "            # Use view(-1) to flatten and slice\n",
    "            sample = arr.view(-1)[::step]\n",
    "            min_val = torch.quantile(sample, 0.01)\n",
    "            max_val = torch.quantile(sample, 0.99)\n",
    "        else:\n",
    "            min_val = torch.quantile(arr, 0.01)\n",
    "            max_val = torch.quantile(arr, 0.99)\n",
    "        \n",
    "        # Scale to [-4, 3]\n",
    "        scaled = (arr - min_val) / (max_val - min_val + 1e-8) * 7 - 4\n",
    "        \n",
    "        # Round and clip\n",
    "        quantized = torch.clamp(torch.round(scaled), -4, 3).to(torch.int8)\n",
    "        return quantized\n",
    "    else:\n",
    "        # Numpy implementation\n",
    "        arr = data\n",
    "        # Similar optimization for numpy to speed up\n",
    "        if arr.size > 100000:\n",
    "             step = max(1, arr.size // 100000)\n",
    "             sample = arr.ravel()[::step]\n",
    "             min_val = np.percentile(sample, 1)\n",
    "             max_val = np.percentile(sample, 99)\n",
    "        else:\n",
    "             min_val = np.percentile(arr, 1)\n",
    "             max_val = np.percentile(arr, 99)\n",
    "             \n",
    "        scaled = (arr - min_val) / (max_val - min_val + 1e-8) * 7 - 4\n",
    "        quantized = np.clip(np.round(scaled), -4, 3).astype(np.int8)\n",
    "        return quantized\n",
    "\n",
    "class PerDimensionQuantileQuantizer:\n",
    "    \"\"\"\n",
    "    Quantize to INT3 using per-dimension quantiles.\n",
    "    For each dimension, it calculates 7 thresholds (1/8, 2/8, ..., 7/8 quantiles)\n",
    "    from the training data, and maps values to bins -4 to 3.\n",
    "    This ensures that for each dimension, the distribution of quantized values\n",
    "    is approximately uniform across the 8 bins.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.thresholds = None\n",
    "        self.device = DEVICE\n",
    "\n",
    "    def fit(self, data):\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = torch.from_numpy(data)\n",
    "        if data.device != self.device:\n",
    "            data = data.to(self.device)\n",
    "        \n",
    "        data = data.float()\n",
    "        N, D = data.shape\n",
    "        \n",
    "        # Calculate quantiles for each dimension\n",
    "        # quantiles: (7, D)\n",
    "        q_vals = torch.tensor([0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875], device=self.device)\n",
    "        \n",
    "        # Use torch.quantile with dim=0 (requires reasonably new pytorch)\n",
    "        # If data is very large, we might want to sample, but for 1M x 128 it fits in GPU memory usually.\n",
    "        # 1M * 128 * 4 bytes = 512MB.\n",
    "        try:\n",
    "            self.thresholds = torch.quantile(data, q_vals, dim=0) # (7, D)\n",
    "        except RuntimeError:\n",
    "            # Fallback if memory issue or old pytorch\n",
    "            # Process per dimension or sample\n",
    "            if N > 100000:\n",
    "                step = N // 100000\n",
    "                sample = data[::step]\n",
    "                self.thresholds = torch.quantile(sample, q_vals, dim=0)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.thresholds is None:\n",
    "            raise ValueError(\"Quantizer not fitted\")\n",
    "            \n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = torch.from_numpy(data)\n",
    "        if data.device != self.device:\n",
    "            data = data.to(self.device)\n",
    "            \n",
    "        data = data.float()\n",
    "        \n",
    "        # Broadcast comparison\n",
    "        # data: (N, D) -> (N, 1, D)\n",
    "        # thresholds: (7, D) -> (1, 7, D)\n",
    "        # comparison: (N, 7, D)\n",
    "        \n",
    "        # We want to count how many thresholds are smaller than data\n",
    "        # If x < t1 (smallest), count is 0 -> map to -4\n",
    "        # If x >= t7 (largest), count is 7 -> map to 3\n",
    "        \n",
    "        comparison = data.unsqueeze(1) >= self.thresholds.unsqueeze(0)\n",
    "        rank = torch.sum(comparison, dim=1).to(torch.int8) # (N, D), values 0-7\n",
    "        \n",
    "        return rank - 4\n",
    "        \n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea22ff0",
   "metadata": {},
   "source": [
    "## 5. AutoEncoder 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 定义AutoEncoder模型和训练函数\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 96),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(96, encoding_dim),\n",
    "            nn.Tanh() # 輸出範圍 [-1, 1]，適合量化\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 96),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(96, input_dim),\n",
    "            # nn.Sigmoid() # SIFT 數據大概是 0-255 或 0-1? \n",
    "            # SIFT特徵通常是已標準化的，包含負數嗎？SIFT通常非負。\n",
    "            # 這裡不加 Sigmoid，直接輸出線性值，配合 MSE\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "def train_autoencoder(data, input_dim, encoding_dim, epochs=50, batch_size=256, \n",
    "                      distance_loss_weight=0.1, use_grouped_latent_dist=False, threshold_config=None):\n",
    "    \"\"\"\n",
    "    训练AutoEncoder\n",
    "    threshold_config: dict, optional. \n",
    "        {'target_threshold': float, 'input_limit': float}\n",
    "        If provided, adds a penalty for group distances > target_threshold for neighbors (input_dist < input_limit).\n",
    "        Penalty based on user idea: d_g * exp(ReLU(d_g - T))\n",
    "    \"\"\"\n",
    "    input_dim = data.shape[1]\n",
    "    \n",
    "    model = AutoEncoder(input_dim, encoding_dim).to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Learn Rate Scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    # Early Stopping params\n",
    "    patience = 10\n",
    "    min_delta = 1e-4\n",
    "    \n",
    "    # Prepare Data\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data_tensor = torch.FloatTensor(data).to(DEVICE)\n",
    "    else:\n",
    "        # If already tensor (e.g. on GPU)\n",
    "        data_tensor = data.to(DEVICE)\n",
    "        \n",
    "    dataset = torch.utils.data.TensorDataset(data_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    history = {'loss': [], 'recon_loss': [], 'dist_loss': []}\n",
    "    \n",
    "    pbar = tqdm(range(epochs), desc=\"训练AutoEncoder\")\n",
    "    for epoch in pbar:\n",
    "        total_loss = 0\n",
    "        total_recon_loss = 0\n",
    "        total_dist_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch_data = batch[0]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            encoded, reconstructed = model(batch_data)\n",
    "            \n",
    "            # 1. Reconstruction Loss\n",
    "            recon_loss = criterion(reconstructed, batch_data)\n",
    "            \n",
    "            # 2. Distance Preservation Loss\n",
    "            # Input space distance: Standard L2\n",
    "            dist_input = torch.cdist(batch_data, batch_data, p=2)\n",
    "            \n",
    "            threshold_term = 0\n",
    "            \n",
    "            # Latent space distance\n",
    "            if use_grouped_latent_dist:\n",
    "                # Calculate Grouped L2 Norm for Latent Space\n",
    "                # Group Size = 6\n",
    "                group_dists = []\n",
    "                D = encoded.shape[1]\n",
    "                for start_idx in range(0, D, 6):\n",
    "                    end_idx = min(start_idx + 6, D)\n",
    "                    sub = encoded[:, start_idx:end_idx]\n",
    "                    d_g = torch.cdist(sub, sub, p=2)\n",
    "                    group_dists.append(d_g)\n",
    "                \n",
    "                dist_latent = sum(group_dists)\n",
    "                \n",
    "                # --- Threshold Awareness Penalty ---\n",
    "                if threshold_config and isinstance(threshold_config, dict):\n",
    "                    T = threshold_config.get('target_threshold', 0.5)\n",
    "                    limit = threshold_config.get('input_limit', 150.0)\n",
    "                    \n",
    "                    # Identify neighbors in input space\n",
    "                    neighbor_mask = (dist_input <= limit)\n",
    "                    \n",
    "                    if neighbor_mask.sum() > 0:\n",
    "                        penalty_accum = 0.0\n",
    "                        for d_g in group_dists:\n",
    "                            d_val = d_g[neighbor_mask]\n",
    "                            \n",
    "                            # User formula: multiply by exp(ReLU(d - T))\n",
    "                            # We formulate penalty as adding to loss:\n",
    "                            # Loss += mean( d * (exp(ReLU(d-T)) - 1) )\n",
    "                            # If d <= T, exp(0)=1, penalty=0.\n",
    "                            # If d > T, penalty grows exponentially.\n",
    "                            excess = torch.relu(d_val - T)\n",
    "                            term = d_val * (torch.exp(excess) - 1.0)\n",
    "                            penalty_accum += term.mean()\n",
    "                            \n",
    "                        threshold_term = penalty_accum * 0.1 # Weight it? Assume included in logic.\n",
    "            else:\n",
    "                # Standard L2\n",
    "                dist_latent = torch.cdist(encoded, encoded, p=2)\n",
    "            \n",
    "            # MSE between Latent and Input Distance\n",
    "            dist_loss = nn.functional.mse_loss(dist_latent, dist_input)\n",
    "            \n",
    "            # Total Loss\n",
    "            loss = (1-distance_loss_weight) * recon_loss + distance_loss_weight * dist_loss + threshold_term\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_dist_loss += dist_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_recon = total_recon_loss / num_batches\n",
    "        avg_dist = total_dist_loss / num_batches\n",
    "        \n",
    "        history['loss'].append(avg_loss)\n",
    "        history['recon_loss'].append(avg_recon)\n",
    "        history['dist_loss'].append(avg_dist)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{avg_loss:.4f}', \n",
    "            'recon': f'{avg_recon:.4f}', \n",
    "            'dist': f'{avg_dist:.4f}'\n",
    "        })\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # Early Stopping Check (based on total loss)\n",
    "        if avg_loss < best_loss - min_delta:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1} (Best Loss: {best_loss:.6f})\")\n",
    "            break\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97cf329",
   "metadata": {},
   "source": [
    "## 6. 五种数据处理方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6cb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quantization(train_data, query_data, quantizer):\n",
    "    \"\"\"Helper to apply quantizer (function or class) to train and query data\"\"\"\n",
    "    # Check if quantizer is a class instance (has fit/transform)\n",
    "    if hasattr(quantizer, 'fit') and hasattr(quantizer, 'transform'):\n",
    "        # Stateful quantizer: Fit on Train, Transform Train & Query\n",
    "        # Note: We must fit on the specific data passed here (which might be reduced dim)\n",
    "        train_q = quantizer.fit_transform(train_data)\n",
    "        query_q = quantizer.transform(query_data) if query_data is not None else None\n",
    "    else:\n",
    "        # Functional quantizer: Stateless\n",
    "        train_q = quantizer(train_data)\n",
    "        query_q = quantizer(query_data) if query_data is not None else None\n",
    "    return train_q, query_q\n",
    "\n",
    "def method1_direct_int3(data, query_data=None, quantizer=quantize_to_int3):\n",
    "    \"\"\"方法1: 直接將128維向量量化為INT3\"\"\"\n",
    "    # Convert to Tensor if numpy\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = torch.from_numpy(data).to(DEVICE)\n",
    "    if query_data is not None and isinstance(query_data, np.ndarray):\n",
    "        query_data = torch.from_numpy(query_data).to(DEVICE)\n",
    "        \n",
    "    db_q, q_q = apply_quantization(data, query_data, quantizer)\n",
    "    \n",
    "    if query_data is not None:\n",
    "        return None, db_q, q_q\n",
    "    return db_q\n",
    "\n",
    "def method2_average_pooling_int3(data, query_data=None, quantizer=quantize_to_int3):\n",
    "    \"\"\"方法2: 將相鄰兩維做平均，從128維降到64維，然後量化為INT3\"\"\"\n",
    "    def process(arr):\n",
    "        if isinstance(arr, np.ndarray):\n",
    "            arr = torch.from_numpy(arr).to(DEVICE)\n",
    "        # Reshape (N, 64, 2)\n",
    "        reshaped = arr.reshape(arr.shape[0], 64, 2)\n",
    "        # PyTorch mean\n",
    "        return torch.mean(reshaped.float(), dim=2)\n",
    "\n",
    "    data_reduced = process(data)\n",
    "    query_reduced = process(query_data) if query_data is not None else None\n",
    "    \n",
    "    db_q, q_q = apply_quantization(data_reduced, query_reduced, quantizer)\n",
    "    \n",
    "    if query_data is not None:\n",
    "        return None, db_q, q_q\n",
    "    return db_q\n",
    "\n",
    "def method3_pca_int3(data, query_data=None, quantizer=quantize_to_int3):\n",
    "    \"\"\"方法3: 使用PCA降維到64維，然後量化為INT3\"\"\"\n",
    "    print(\"  - 训练PCA (CPU)...\")\n",
    "    \n",
    "    # Ensure data is numpy for sklearn fitting\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data_np = data.cpu().numpy()\n",
    "    else:\n",
    "        data_np = data\n",
    "        \n",
    "    # 使用 whiten=True 來標準化分量\n",
    "    pca = PCA(n_components=REDUCED_DIM, whiten=True)\n",
    "    pca.fit(data_np)\n",
    "    \n",
    "    # Prepare for GPU transform\n",
    "    mean = torch.from_numpy(pca.mean_).float().to(DEVICE)\n",
    "    components = torch.from_numpy(pca.components_).float().to(DEVICE)\n",
    "    explained_variance = torch.from_numpy(pca.explained_variance_).float().to(DEVICE)\n",
    "    \n",
    "    def transform_gpu(X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = torch.from_numpy(X).to(DEVICE)\n",
    "        X = X.float()\n",
    "        \n",
    "        # Center\n",
    "        X_centered = X - mean\n",
    "        # Project\n",
    "        X_transformed = torch.matmul(X_centered, components.T)\n",
    "        \n",
    "        # Whiten\n",
    "        if pca.whiten:\n",
    "            scale = torch.sqrt(explained_variance)\n",
    "            X_transformed = X_transformed / scale\n",
    "            \n",
    "        return X_transformed\n",
    "\n",
    "    data_reduced = transform_gpu(data_np)\n",
    "    query_reduced = transform_gpu(query_data) if query_data is not None else None\n",
    "    \n",
    "    db_q, q_q = apply_quantization(data_reduced, query_reduced, quantizer)\n",
    "    \n",
    "    if query_data is not None:\n",
    "        return pca, db_q, q_q\n",
    "    \n",
    "    return pca, db_q\n",
    "\n",
    "def method4_max_pooling_int3(data, query_data=None, quantizer=quantize_to_int3):\n",
    "    \"\"\"方法4: 將相鄰兩維做最大絕對值池化 (Max Magnitude Pooling)，從128維降到64維，然後量化為INT3\"\"\"\n",
    "    \n",
    "    def max_magnitude_pool_torch(arr):\n",
    "        if isinstance(arr, np.ndarray):\n",
    "            arr = torch.from_numpy(arr).to(DEVICE)\n",
    "            \n",
    "        # Reshape to (N, 64, 2)\n",
    "        reshaped = arr.reshape(arr.shape[0], 64, 2)\n",
    "        a = reshaped[:, :, 0]\n",
    "        b = reshaped[:, :, 1]\n",
    "        # 比較絕對值大小\n",
    "        mask = torch.abs(a) >= torch.abs(b)\n",
    "        # 選擇絕對值較大的那個原始值\n",
    "        return torch.where(mask, a, b)\n",
    "\n",
    "    data_reduced = max_magnitude_pool_torch(data)\n",
    "    query_reduced = max_magnitude_pool_torch(query_data) if query_data is not None else None\n",
    "    \n",
    "    db_q, q_q = apply_quantization(data_reduced, query_reduced, quantizer)\n",
    "    \n",
    "    if query_data is not None:\n",
    "        return None, db_q, q_q\n",
    "    \n",
    "    return None, db_q\n",
    "\n",
    "def method5_autoencoder_int3(data, query_data=None, epochs=100, quantizer=quantize_to_int3, distance_loss_weight=0.1, use_grouped_latent_dist=False, threshold_config=None):\n",
    "    \"\"\"方法5: 使用AutoEncoder降維到64維，然後量化為INT3\"\"\"\n",
    "    print(f\"  - 训练AutoEncoder (Dist Weight: {distance_loss_weight}, Grouped: {use_grouped_latent_dist}, Threshold: {threshold_config is not None})...\")\n",
    "    # Ensure numpy for training (dataloader handles conversion)\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data_np = data.cpu().numpy()\n",
    "    else:\n",
    "        data_np = data\n",
    "        \n",
    "    # 這裡 epochs 預設為 100，配合 Early Stopping\n",
    "    ae_model, history = train_autoencoder(data_np, SIFT_DIM, REDUCED_DIM, epochs=epochs, \n",
    "                                          distance_loss_weight=distance_loss_weight, \n",
    "                                          use_grouped_latent_dist=use_grouped_latent_dist,\n",
    "                                          threshold_config=threshold_config)\n",
    "    ae_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode on GPU\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data_tensor = torch.FloatTensor(data).to(DEVICE)\n",
    "        else:\n",
    "            data_tensor = data.float().to(DEVICE)\n",
    "            \n",
    "        data_reduced = ae_model.encode(data_tensor) # Returns tensor on GPU\n",
    "        \n",
    "        if query_data is not None:\n",
    "            if isinstance(query_data, np.ndarray):\n",
    "                query_tensor = torch.FloatTensor(query_data).to(DEVICE)\n",
    "            else:\n",
    "                query_tensor = query_data.float().to(DEVICE)\n",
    "            query_reduced = ae_model.encode(query_tensor)\n",
    "        else:\n",
    "            query_reduced = None\n",
    "            \n",
    "        db_q, q_q = apply_quantization(data_reduced, query_reduced, quantizer)\n",
    "        \n",
    "        if query_data is not None:\n",
    "            return ae_model, db_q, q_q, history\n",
    "    \n",
    "    return ae_model, db_q, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21951698",
   "metadata": {},
   "source": [
    "## 7. 距离计算和评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_l2_distances(queries, database):\n",
    "    \"\"\"計算L2距離\"\"\"\n",
    "    return pairwise_distances(queries, database, metric='euclidean')\n",
    "\n",
    "def evaluate_recall_at_k(distances, ground_truth, k_values=[1, 10, 100]):\n",
    "    \"\"\"計算Recall@K指標\"\"\"\n",
    "    sorted_indices = np.argsort(distances, axis=1)\n",
    "    \n",
    "    recalls = {}\n",
    "    for k in k_values:\n",
    "        top_k_predictions = sorted_indices[:, :k]\n",
    "        \n",
    "        query_recalls = []\n",
    "        for i in range(len(ground_truth)):\n",
    "            gt_i = ground_truth[i]\n",
    "            # Handle jagged arrays: check length\n",
    "            limit = min(k, len(gt_i))\n",
    "            \n",
    "            if limit == 0:\n",
    "                query_recalls.append(0.0)\n",
    "                continue\n",
    "                \n",
    "            true_neighbors = set(gt_i[:limit])\n",
    "            pred_neighbors = set(top_k_predictions[i])\n",
    "            intersection = true_neighbors & pred_neighbors\n",
    "            recall = len(intersection) / len(true_neighbors)\n",
    "            query_recalls.append(recall)\n",
    "        \n",
    "        recalls[f'recall@{k}'] = np.mean(query_recalls)\n",
    "    \n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a202e3",
   "metadata": {},
   "source": [
    "## 8. 加载 SIFT1M 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fvecs(filename):\n",
    "    \"\"\"讀取.fvecs格式文件\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        d = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "        f.seek(0)\n",
    "        # .fvecs 格式: 4 bytes int32 (dimension) + d * 4 bytes float32 (data)\n",
    "        # 读取为 float32，header 的 int32 会被读成一个 float32，reshape 后切片去掉即可\n",
    "        data = np.fromfile(f, dtype=np.float32)\n",
    "        data = data.reshape(-1, d + 1)\n",
    "        return data[:, 1:].copy()\n",
    "\n",
    "def read_ivecs(filename):\n",
    "    \"\"\"讀取.ivecs格式文件\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        d = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "        f.seek(0)\n",
    "        data = np.fromfile(f, dtype=np.int32)\n",
    "        data = data.reshape(-1, d + 1)\n",
    "        return data[:, 1:].copy()\n",
    "\n",
    "print(\"载入SIFT1M数据集...\")\n",
    "base_vectors = read_fvecs('sift1m/sift_base.fvecs')\n",
    "query_vectors = read_fvecs('sift1m/sift_query.fvecs')\n",
    "ground_truth = read_ivecs('sift1m/sift_groundtruth.ivecs')\n",
    "\n",
    "print(f\"Base vectors shape: {base_vectors.shape}\")\n",
    "print(f\"Query vectors shape: {query_vectors.shape}\")\n",
    "print(f\"Ground truth shape: {ground_truth.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70c0f9",
   "metadata": {},
   "source": [
    "## 9. 运行实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top_k_ground_truth(queries, database, k=100):\n",
    "    \"\"\"\n",
    "    計算每個查詢的前 K 個真實最近鄰居 (Ground Truth)\n",
    "    使用 GPU 加速計算，並分批處理 Query 以節省記憶體\n",
    "    \"\"\"\n",
    "    print(f\"正在計算 Top-{k} Ground Truth (Query: {len(queries)}, DB: {len(database)})...\")\n",
    "    \n",
    "    num_queries = len(queries)\n",
    "    query_batch_size = 100  # 每次處理 100 個 Query\n",
    "    \n",
    "    all_indices = []\n",
    "    \n",
    "    for i in tqdm(range(0, num_queries, query_batch_size), desc=\"Computing GT\"):\n",
    "        q_end = min(i + query_batch_size, num_queries)\n",
    "        q_batch = queries[i:q_end]\n",
    "        q_tensor = torch.FloatTensor(q_batch).to(DEVICE)\n",
    "        q_sq = torch.sum(q_tensor**2, dim=1, keepdim=True)\n",
    "        \n",
    "        dists_batch = []\n",
    "        \n",
    "        # 分批處理 DB\n",
    "        db_batch_size = 50000\n",
    "        for j in range(0, len(database), db_batch_size):\n",
    "            db_end = min(j + db_batch_size, len(database))\n",
    "            db_chunk = torch.FloatTensor(database[j:db_end]).to(DEVICE)\n",
    "            \n",
    "            db_sq_chunk = torch.sum(db_chunk**2, dim=1)\n",
    "            term2 = -2 * torch.matmul(q_tensor, db_chunk.t())\n",
    "            \n",
    "            dists_chunk = q_sq + db_sq_chunk + term2\n",
    "            dists_batch.append(dists_chunk.cpu())\n",
    "            \n",
    "            del db_chunk, db_sq_chunk, term2, dists_chunk\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        full_dists_batch = torch.cat(dists_batch, dim=1)\n",
    "        \n",
    "        # 取 Top K\n",
    "        _, indices = torch.topk(full_dists_batch, k=k, dim=1, largest=False)\n",
    "        all_indices.append(indices.numpy())\n",
    "        \n",
    "        del full_dists_batch, indices, q_tensor, q_sq\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return np.vstack(all_indices)\n",
    "\n",
    "def evaluate_recall_batched(query_vectors, db_vectors, gt_top_k, retrieval_depths, k_true_neighbors):\n",
    "    \"\"\"\n",
    "    分批計算距離並評估 Recall\n",
    "    使用 GPU 加速距離計算\n",
    "    Recall = (Retrieved & Top-K_GT) / K_GT\n",
    "    \"\"\"\n",
    "    max_depth = max(retrieval_depths)\n",
    "    num_queries = len(query_vectors)\n",
    "    query_batch_size = 100\n",
    "    \n",
    "    total_hits = {r: 0 for r in retrieval_depths}\n",
    "    \n",
    "    # Ensure inputs are tensors on GPU\n",
    "    if isinstance(query_vectors, np.ndarray):\n",
    "        query_vectors = torch.from_numpy(query_vectors).to(DEVICE)\n",
    "    elif query_vectors.device != DEVICE:\n",
    "        query_vectors = query_vectors.to(DEVICE)\n",
    "        \n",
    "    if isinstance(db_vectors, np.ndarray):\n",
    "        db_vectors = torch.from_numpy(db_vectors).to(DEVICE)\n",
    "    elif db_vectors.device != DEVICE:\n",
    "        db_vectors = db_vectors.to(DEVICE)\n",
    "    \n",
    "    # Ensure float for distance calc (INT3 needs to be float for calc)\n",
    "    if query_vectors.dtype != torch.float32:\n",
    "        query_vectors = query_vectors.float()\n",
    "    if db_vectors.dtype != torch.float32:\n",
    "        db_vectors = db_vectors.float()\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Pre-calculate DB squared norms if memory allows, or do it in chunks\n",
    "    # For 1M vectors, pre-calculating norms is fast and takes little memory (1M floats = 4MB)\n",
    "    db_sq = torch.sum(db_vectors**2, dim=1)\n",
    "    \n",
    "    for i in tqdm(range(0, num_queries, query_batch_size), desc=\"Evaluating Batches\", leave=False):\n",
    "        q_end = min(i + query_batch_size, num_queries)\n",
    "        q_batch = query_vectors[i:q_end] # (Batch, Dim)\n",
    "        gt_batch = gt_top_k[i:q_end]\n",
    "        \n",
    "        # Calculate L2 Distance on GPU: |x-y|^2 = |x|^2 + |y|^2 - 2xy\n",
    "        q_sq = torch.sum(q_batch**2, dim=1, keepdim=True) # (Batch, 1)\n",
    "        \n",
    "        # Matrix multiplication: (Batch, Dim) @ (Dim, DB_Size) -> (Batch, DB_Size)\n",
    "        # Note: db_vectors is (DB_Size, Dim), so we transpose it\n",
    "        # For very large DB, we might need to chunk this too, but 1M fits in GPU memory for matmul usually\n",
    "        # 100 * 1M * 4 bytes = 400MB result matrix. This is fine.\n",
    "        \n",
    "        term2 = -2 * torch.matmul(q_batch, db_vectors.t())\n",
    "        \n",
    "        # Broadcasting: (Batch, 1) + (DB_Size,) + (Batch, DB_Size)\n",
    "        dists = q_sq + db_sq + term2\n",
    "        \n",
    "        # Find Top-K on GPU\n",
    "        # We need the smallest distances\n",
    "        _, sorted_indices_tensor = torch.topk(dists, k=max_depth, dim=1, largest=False)\n",
    "        \n",
    "        # Move indices to CPU for set intersection (faster on CPU for small sets logic)\n",
    "        sorted_indices = sorted_indices_tensor.cpu().numpy()\n",
    "        \n",
    "        for r in retrieval_depths:\n",
    "            retrieved_indices = sorted_indices[:, :r]\n",
    "            for j in range(len(gt_batch)):\n",
    "                gt_set = set(gt_batch[j]) # 這是 Top K 真實鄰居\n",
    "                retrieved_set = set(retrieved_indices[j])\n",
    "                total_hits[r] += len(gt_set & retrieved_set)\n",
    "                \n",
    "        # Clean up intermediate tensors\n",
    "        del dists, term2, q_sq, sorted_indices_tensor\n",
    "        # torch.cuda.empty_cache() # Optional: can slow down loop if called too often\n",
    "    \n",
    "    end_time = time.time()\n",
    "    search_time = end_time - start_time\n",
    "                \n",
    "    recalls = {}\n",
    "    for r in retrieval_depths:\n",
    "        # Normalize by k_true_neighbors (e.g., 100)\n",
    "        recalls[f'recall@{r}'] = total_hits[r] / (num_queries * float(k_true_neighbors))\n",
    "        \n",
    "    recalls['search_time'] = search_time\n",
    "    recalls['qps'] = num_queries / search_time if search_time > 0 else 0\n",
    "        \n",
    "    return recalls\n",
    "\n",
    "def method5_autoencoder_int3_with_timing(data, query_data=None, epochs=100):\n",
    "    \"\"\"\n",
    "    方法5: 使用AutoEncoder降維到64維，然後量化為INT3\n",
    "    返回額外的訓練時間和推理時間\n",
    "    \"\"\"\n",
    "    print(\"  - 训练AutoEncoder...\")\n",
    "    \n",
    "    # 訓練時間\n",
    "    train_start = time.time()\n",
    "    ae_model = train_autoencoder(data, SIFT_DIM, REDUCED_DIM, epochs=epochs)\n",
    "    train_time = time.time() - train_start\n",
    "    \n",
    "    ae_model.eval()\n",
    "    \n",
    "    # 推理時間\n",
    "    inference_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        data_tensor = torch.FloatTensor(data).to(DEVICE)\n",
    "        data_reduced = ae_model.encode(data_tensor).cpu().numpy()\n",
    "        data_quantized = quantize_to_int3(data_reduced)\n",
    "        \n",
    "        if query_data is not None:\n",
    "            query_tensor = torch.FloatTensor(query_data).to(DEVICE)\n",
    "            query_reduced = ae_model.encode(query_tensor).cpu().numpy()\n",
    "            query_quantized = quantize_to_int3(query_reduced)\n",
    "            inference_time = time.time() - inference_start\n",
    "            return ae_model, data_quantized, query_quantized, train_time, inference_time\n",
    "    \n",
    "    inference_time = time.time() - inference_start\n",
    "    return ae_model, data_quantized, train_time, inference_time\n",
    "\n",
    "def run_experiment_part(part_name, base_vectors, query_vectors, gt_top_k, retrieval_depths, k_true_neighbors, quantizer=quantize_to_int3, quantizer_name=\"Standard\"):\n",
    "    \"\"\"\n",
    "    執行單個部分的實驗\n",
    "    返回 recalls DataFrame 和 times DataFrame (不含 AE 訓練時間)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"實驗部分: {part_name} (Quantizer: {quantizer_name})\")\n",
    "    print(f\"Database Size: {len(base_vectors)}\")\n",
    "    print(f\"Retrieval Depths: {retrieval_depths}\")\n",
    "    print(f\"Target GT Size (K): {k_true_neighbors}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    # 方法1\n",
    "    print(f\"\\n[方法1] 直接INT3量化 (128維) - {quantizer_name}...\")\n",
    "    t0 = time.time()\n",
    "    db_m1 = method1_direct_int3(base_vectors)\n",
    "    q_m1 = method1_direct_int3(query_vectors)\n",
    "    recalls_m1 = evaluate_recall_batched(q_m1, db_m1, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 1: Direct INT3', 'time': time.time()-t0, **recalls_m1})\n",
    "    print(recalls_m1)\n",
    "    \n",
    "    # 方法2\n",
    "    print(f\"\\n[方法2] 平均池化降維 + INT3 (64維) - {quantizer_name}...\")\n",
    "    t0 = time.time()\n",
    "    db_m2 = method2_average_pooling_int3(base_vectors)\n",
    "    q_m2 = method2_average_pooling_int3(query_vectors)\n",
    "    recalls_m2 = evaluate_recall_batched(q_m2, db_m2, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 2: AvgPooling + INT3', 'time': time.time()-t0, **recalls_m2})\n",
    "    print(recalls_m2)\n",
    "    \n",
    "    # 方法3\n",
    "    print(f\"\\n[方法3] PCA降維 + INT3 (64維) - {quantizer_name}...\")\n",
    "    t0 = time.time()\n",
    "    _, db_m3, q_m3 = method3_pca_int3(base_vectors, query_vectors)\n",
    "    recalls_m3 = evaluate_recall_batched(q_m3, db_m3, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 3: PCA + INT3', 'time': time.time()-t0, **recalls_m3})\n",
    "    print(recalls_m3)\n",
    "    \n",
    "    # 方法4\n",
    "    print(f\"\\n[方法4] Max Magnitude Pooling降維 + INT3 (64維) - {quantizer_name}...\")\n",
    "    t0 = time.time()\n",
    "    _, db_m4, q_m4 = method4_max_pooling_int3(base_vectors, query_vectors)\n",
    "    recalls_m4 = evaluate_recall_batched(q_m4, db_m4, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 4: Max Mag Pooling + INT3', 'time': time.time()-t0, **recalls_m4})\n",
    "    print(recalls_m4)\n",
    "    \n",
    "    # 方法5\n",
    "    print(\"\\n[方法5] AutoEncoder降維 + INT3 (64維)...\")\n",
    "    t0 = time.time()\n",
    "    # Epochs 設為 100，配合 Early Stopping\n",
    "    _, db_m5, q_m5 = method5_autoencoder_int3(base_vectors, query_vectors, epochs=100)\n",
    "    recalls_m5 = evaluate_recall_batched(q_m5, db_m5, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 5: AutoEncoder + INT3', 'time': time.time()-t0, **recalls_m5})\n",
    "    print(recalls_m5)\n",
    "    \n",
    "    return pd.DataFrame(results), pd.DataFrame(time_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b596229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行實驗\n",
    "# 實驗配置：\n",
    "# Part 1: 100K DB, Recall 100@1000, 100@5000, 100@10000\n",
    "# Part 2: 1M DB, Recall 100@1000, 100@5000, 100@10000\n",
    "\n",
    "all_experiment_results = []  # 存儲 recall 結果\n",
    "all_time_results = []  # 存儲時間結果\n",
    "\n",
    "# --- Part 1: 100K Database ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Part 1: First 100K Database\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "db_100k = base_vectors[:100000].astype(np.float32)\n",
    "query_subset = query_vectors[:1000].astype(np.float32)\n",
    "\n",
    "# 計算 Ground Truth for 100K DB\n",
    "print(\"Computing Ground Truth for 100K DB...\")\n",
    "gt_100k = calculate_top_k_ground_truth(query_subset, db_100k, k=100)\n",
    "\n",
    "# 執行實驗 Part 1\n",
    "retrieval_depths_1 = [1000, 5000, 10000]\n",
    "k_true_neighbors = 100\n",
    "\n",
    "results_df_1, time_df_1 = run_experiment_part(\n",
    "    \"Part 1: 100K DB\",\n",
    "    db_100k,\n",
    "    query_subset,\n",
    "    gt_100k,\n",
    "    retrieval_depths_1,\n",
    "    k_true_neighbors\n",
    ")\n",
    "\n",
    "all_experiment_results.append(results_df_1)\n",
    "all_time_results.append(time_df_1)\n",
    "\n",
    "print(\"\\nPart 1 Recall Results:\")\n",
    "display(results_df_1)\n",
    "print(\"\\nPart 1 Time Results (不含 AE 訓練時間):\")\n",
    "display(time_df_1)\n",
    "\n",
    "# --- Part 2: 1M Database ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Part 2: Full 1M Database\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "db_1m = base_vectors.astype(np.float32)\n",
    "\n",
    "# 計算 Ground Truth for 1M DB\n",
    "print(\"Computing Ground Truth for 1M DB...\")\n",
    "gt_1m = calculate_top_k_ground_truth(query_subset, db_1m, k=100)\n",
    "\n",
    "# 執行實驗 Part 2\n",
    "retrieval_depths_2 = [1000, 5000, 10000]\n",
    "\n",
    "results_df_2, time_df_2 = run_experiment_part(\n",
    "    \"Part 2: 1M DB\",\n",
    "    db_1m,\n",
    "    query_subset,\n",
    "    gt_1m,\n",
    "    retrieval_depths_2,\n",
    "    k_true_neighbors\n",
    ")\n",
    "\n",
    "all_experiment_results.append(results_df_2)\n",
    "all_time_results.append(time_df_2)\n",
    "\n",
    "print(\"\\nPart 2 Recall Results:\")\n",
    "display(results_df_2)\n",
    "print(\"\\nPart 2 Time Results (不含 AE 訓練時間):\")\n",
    "display(time_df_2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"所有實驗完成！\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行實驗\n",
    "# 實驗配置：\n",
    "# Part 1: 100K DB, Recall 100@1000, 100@5000, 100@10000\n",
    "# Part 2: 1M DB, Recall 100@1000, 100@5000, 100@10000\n",
    "# 比較兩種 Quantization 方法：Standard (Percentile) vs Quantile (Per-Dimension Rank-based)\n",
    "\n",
    "all_experiment_results = []  # 存儲結果 (包含 Recall, Time, QPS)\n",
    "\n",
    "# 定義要比較的 Quantizers\n",
    "# 注意：對於 Class 類型的 Quantizer，我們傳遞實例，run_experiment_part 會重複使用它 (每次 fit 會覆蓋)\n",
    "quantizers = [\n",
    "    (\"Standard\", quantize_to_int3),\n",
    "    (\"Quantile\", PerDimensionQuantileQuantizer())\n",
    "]\n",
    "\n",
    "# --- Part 1: 100K Database ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Part 1: First 100K Database\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "db_100k = base_vectors[:100000].astype(np.float32)\n",
    "query_subset = query_vectors[:1000].astype(np.float32)\n",
    "\n",
    "# 計算 Ground Truth for 100K DB\n",
    "print(\"Computing Ground Truth for 100K DB...\")\n",
    "gt_100k = calculate_top_k_ground_truth(query_subset, db_100k, k=100)\n",
    "\n",
    "# 執行實驗 Part 1 (兩種 Quantization)\n",
    "retrieval_depths_1 = [1000, 5000, 10000]\n",
    "k_true_neighbors = 100\n",
    "\n",
    "for q_name, q_obj in quantizers:\n",
    "    results_df = run_experiment_part(\n",
    "        f\"Part 1: 100K DB ({q_name})\",\n",
    "        db_100k,\n",
    "        query_subset,\n",
    "        gt_100k,\n",
    "        retrieval_depths_1,\n",
    "        k_true_neighbors,\n",
    "        quantizer=q_obj,\n",
    "        quantizer_name=q_name\n",
    "    )\n",
    "    all_experiment_results.append(results_df)\n",
    "    print(f\"\\nPart 1 ({q_name}) Results:\")\n",
    "    display(results_df)\n",
    "\n",
    "\n",
    "# --- Part 2: 1M Database ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Part 2: Full 1M Database\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "db_1m = base_vectors.astype(np.float32)\n",
    "\n",
    "# 計算 Ground Truth for 1M DB\n",
    "print(\"Computing Ground Truth for 1M DB...\")\n",
    "gt_1m = calculate_top_k_ground_truth(query_subset, db_1m, k=100)\n",
    "\n",
    "# 執行實驗 Part 2 (兩種 Quantization)\n",
    "retrieval_depths_2 = [1000, 5000, 10000]\n",
    "\n",
    "for q_name, q_obj in quantizers:\n",
    "    results_df = run_experiment_part(\n",
    "        f\"Part 2: 1M DB ({q_name})\",\n",
    "        db_1m,\n",
    "        query_subset,\n",
    "        gt_1m,\n",
    "        retrieval_depths_2,\n",
    "        k_true_neighbors,\n",
    "        quantizer=q_obj,\n",
    "        quantizer_name=q_name\n",
    "    )\n",
    "    all_experiment_results.append(results_df)\n",
    "    print(f\"\\nPart 2 ({q_name}) Results:\")\n",
    "    display(results_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"所有實驗完成！\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4ac29",
   "metadata": {},
   "source": [
    "## 10. 可视化结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a08e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化結果\n",
    "# 我們需要畫 3 張圖 (或 3 組圖)，分別對應 Part 1, Part 2, Part 3\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('Recall of Top-100 True Neighbors @ Different Retrieval Depths', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Part 1: 10k DB, Recall@1000\n",
    "ax1 = axes[0]\n",
    "df1 = all_experiment_results[0]\n",
    "methods = df1['method'].str.replace('Method ', 'M').str.replace(': ', '\\n', 1)\n",
    "bars1 = ax1.bar(range(len(df1)), df1['recall@1000'], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "ax1.set_xticks(range(len(df1)))\n",
    "ax1.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
    "ax1.set_ylabel('Recall@1000', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Part 1: 10k DB (100@1000)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim([0, 1.05])\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Part 2: 100k DB, Recall@1000 & Recall@10000\n",
    "# 這裡我們畫 Recall@10000 作為代表，或者畫兩個柱狀圖\n",
    "ax2 = axes[1]\n",
    "df2 = all_experiment_results[1]\n",
    "# 為了同時顯示 @1000 和 @10000，我們使用分組柱狀圖\n",
    "x = np.arange(len(df2))\n",
    "width = 0.35\n",
    "bars2_1 = ax2.bar(x - width/2, df2['recall@1000'], width, label='@1000', color='#1f77b4')\n",
    "bars2_2 = ax2.bar(x + width/2, df2['recall@10000'], width, label='@10000', color='#ff7f0e')\n",
    "\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
    "ax2.set_ylabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Part 2: 100k DB', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim([0, 1.05])\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Part 3: 1M DB, Recall@1000 & Recall@10000\n",
    "ax3 = axes[2]\n",
    "df3 = all_experiment_results[2]\n",
    "x = np.arange(len(df3))\n",
    "bars3_1 = ax3.bar(x - width/2, df3['recall@1000'], width, label='@1000', color='#1f77b4')\n",
    "bars3_2 = ax3.bar(x + width/2, df3['recall@10000'], width, label='@10000', color='#ff7f0e')\n",
    "\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
    "ax3.set_ylabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Part 3: 1M DB', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylim([0, 1.05])\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sift1m_3parts_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44c37c",
   "metadata": {},
   "source": [
    "## 11. 实验总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"實驗總結\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 確保變數名稱兼容性\n",
    "if 'all_experiment_results' not in locals() and 'all_results' in locals():\n",
    "    all_experiment_results = all_results\n",
    "\n",
    "if 'all_experiment_results' in locals():\n",
    "    for i, results_df in enumerate(all_experiment_results):\n",
    "        print(f\"\\nExperiment Part {i+1}\")\n",
    "        display(results_df)\n",
    "else:\n",
    "    print(\"No results found.\")\n",
    "\n",
    "print(\"\\n所有實驗完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Dimension Reduction Analysis (128 -> 16)\n",
    "# Analyzing Recall Rate vs Dimension for AvgPooling and PCA\n",
    "# Configuration: 1M Database, Recall of Top-1000 GT @ Depth 10000\n",
    "\n",
    "def adaptive_avg_pool_int3(data, target_dim):\n",
    "    \"\"\"\n",
    "    Adaptive Average Pooling to reduce to specific target dimension\n",
    "    \"\"\"\n",
    "    if isinstance(data, np.ndarray):\n",
    "        t = torch.from_numpy(data).float()\n",
    "    else:\n",
    "        t = data.float()\n",
    "    \n",
    "    t = t.to(DEVICE)\n",
    "    # Input: (N, 128) -> (N, 1, 128) for pooling\n",
    "    t = t.unsqueeze(1)\n",
    "    # Adaptive Pool to (N, 1, target_dim)\n",
    "    pooled = nn.functional.adaptive_avg_pool1d(t, target_dim)\n",
    "    # (N, target_dim)\n",
    "    pooled = pooled.squeeze(1)\n",
    "    \n",
    "    return quantize_to_int3(pooled)\n",
    "\n",
    "def run_dimension_sweep(base_vectors, query_vectors, gt_top_k, k_true_neighbors=1000, retrieval_depth=10000):\n",
    "    # Dimensions: 128, 112, 96, ..., 16\n",
    "    target_dims = list(range(128, 15, -16)) \n",
    "    results = []\n",
    "    \n",
    "    print(f\"Starting Dimension Sweep: {target_dims}\")\n",
    "    print(f\"Database Size: {len(base_vectors)}\")\n",
    "    print(f\"Target GT Size: {k_true_neighbors}\")\n",
    "    print(f\"Retrieval Depth: {retrieval_depth}\")\n",
    "    \n",
    "    for dim in target_dims:\n",
    "        print(f\"\\nTesting Target Dimension: {dim}\")\n",
    "        \n",
    "        # --- Method: Adaptive Avg Pooling ---\n",
    "        print(\"  Running Avg Pooling...\")\n",
    "        t0 = time.time()\n",
    "        db_avg = adaptive_avg_pool_int3(base_vectors, dim)\n",
    "        q_avg = adaptive_avg_pool_int3(query_vectors, dim)\n",
    "        \n",
    "        # Evaluate Recall\n",
    "        recalls_avg = evaluate_recall_batched(q_avg, db_avg, gt_top_k, [retrieval_depth], k_true_neighbors)\n",
    "        time_avg = time.time() - t0\n",
    "        \n",
    "        metric_key = f'recall@{retrieval_depth}'\n",
    "        results.append({\n",
    "            'Dimension': dim,\n",
    "            'Method': 'AvgPooling',\n",
    "            'Recall': recalls_avg[metric_key],\n",
    "            'Time': time_avg\n",
    "        })\n",
    "        print(f\"    AvgPool {metric_key}: {recalls_avg[metric_key]:.4f}\")\n",
    "        \n",
    "        # --- Method: PCA ---\n",
    "        print(\"  Running PCA...\")\n",
    "        t0 = time.time()\n",
    "        # Fit PCA on a subset for speed (max 50k)\n",
    "        fit_size = min(len(base_vectors), 50000)\n",
    "        pca = PCA(n_components=dim, whiten=True)\n",
    "        pca.fit(base_vectors[:fit_size])\n",
    "        \n",
    "        db_pca = pca.transform(base_vectors)\n",
    "        q_pca = pca.transform(query_vectors)\n",
    "        \n",
    "        db_pca_q = quantize_to_int3(db_pca)\n",
    "        q_pca_q = quantize_to_int3(q_pca)\n",
    "        \n",
    "        recalls_pca = evaluate_recall_batched(q_pca_q, db_pca_q, gt_top_k, [retrieval_depth], k_true_neighbors)\n",
    "        time_pca = time.time() - t0\n",
    "        \n",
    "        results.append({\n",
    "            'Dimension': dim,\n",
    "            'Method': 'PCA',\n",
    "            'Recall': recalls_pca[metric_key],\n",
    "            'Time': time_pca\n",
    "        })\n",
    "        print(f\"    PCA {metric_key}: {recalls_pca[metric_key]:.4f}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Setup Data for Sweep (Using 1M Full DB)\n",
    "# Ensure we have the data\n",
    "if 'base_vectors_full' not in locals():\n",
    "    base_vectors_full = base_vectors.astype(np.float32)\n",
    "    query_vectors_subset = query_vectors[:1000].astype(np.float32)\n",
    "\n",
    "# Use Full 1M Database\n",
    "db_subset_sweep = base_vectors_full \n",
    "k_gt = 1000\n",
    "depth = 10000\n",
    "\n",
    "print(f\"Computing GT for sweep (1M DB, Top-{k_gt})...\")\n",
    "# Always recalculate to be safe or check if existing gt matches requirements\n",
    "# Since previous cells might have calculated k=100, we likely need to recalc for k=1000\n",
    "gt_sweep = calculate_top_k_ground_truth(query_vectors_subset, db_subset_sweep, k=k_gt)\n",
    "\n",
    "# Run Sweep\n",
    "sweep_df = run_dimension_sweep(db_subset_sweep, query_vectors_subset, gt_sweep, k_true_neighbors=k_gt, retrieval_depth=depth)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "avg_df = sweep_df[sweep_df['Method'] == 'AvgPooling']\n",
    "pca_df = sweep_df[sweep_df['Method'] == 'PCA']\n",
    "\n",
    "plt.plot(avg_df['Dimension'], avg_df['Recall'], marker='o', linewidth=2, label='Avg Pooling + INT3')\n",
    "plt.plot(pca_df['Dimension'], pca_df['Recall'], marker='s', linewidth=2, label='PCA + INT3')\n",
    "\n",
    "plt.xlabel('Dimension', fontsize=12)\n",
    "plt.ylabel(f'Recall@{depth} (Top-{k_gt} GT)', fontsize=12)\n",
    "plt.title(f'Recall vs Dimension (1M DB, {k_gt}@{depth})', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.gca().invert_xaxis() # Display 128 on left, 16 on right\n",
    "plt.xticks(list(range(128, 15, -16)))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save Results\n",
    "csv_filename = 'sift1m_1M_1000at10000_sweep_results.csv'\n",
    "sweep_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Sweep results saved to {csv_filename}\")\n",
    "sweep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fe3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. PCA Dimension Difference Analysis\n",
    "# Analyze absolute difference between Query and Top-100 NN in PCA space per dimension\n",
    "\n",
    "def analyze_pca_diff(base_vectors, query_vectors, gt_indices, n_components=128):\n",
    "    print(\"Fitting PCA...\")\n",
    "    # Fit PCA on a subset for speed\n",
    "    fit_size = min(len(base_vectors), 50000)\n",
    "    pca = PCA(n_components=n_components, whiten=True)\n",
    "    pca.fit(base_vectors[:fit_size])\n",
    "    \n",
    "    print(\"Transforming data...\")\n",
    "    # Transform all needed data\n",
    "    # We only need the specific base vectors that are in the GT of the queries\n",
    "    # But transforming all might be easier if memory allows (1M * 128 * 4 bytes ~ 512MB)\n",
    "    q_pca = pca.transform(query_vectors)\n",
    "    db_pca = pca.transform(base_vectors)\n",
    "    \n",
    "    n_queries = len(query_vectors)\n",
    "    k_neighbors = gt_indices.shape[1] # Should be 100\n",
    "    n_dims = n_components\n",
    "    \n",
    "    # Array to store sum of absolute differences per dimension\n",
    "    total_abs_diff = np.zeros(n_dims)\n",
    "    count = 0\n",
    "    \n",
    "    print(\"Calculating differences...\")\n",
    "    for i in tqdm(range(n_queries), desc=\"Analyzing Queries\"):\n",
    "        q_vec = q_pca[i] # (128,)\n",
    "        neighbor_indices = gt_indices[i] # (100,)\n",
    "        \n",
    "        # Get neighbor vectors\n",
    "        neighbor_vecs = db_pca[neighbor_indices] # (100, 128)\n",
    "        \n",
    "        # Calculate absolute difference\n",
    "        # |q - n|\n",
    "        abs_diff = np.abs(neighbor_vecs - q_vec) # (100, 128)\n",
    "        \n",
    "        # Sum over neighbors\n",
    "        total_abs_diff += np.sum(abs_diff, axis=0)\n",
    "        count += k_neighbors\n",
    "        \n",
    "    # Mean absolute difference per dimension\n",
    "    mean_abs_diff = total_abs_diff / count\n",
    "    \n",
    "    return mean_abs_diff, pca.explained_variance_ratio_\n",
    "\n",
    "# Setup Data\n",
    "if 'base_vectors_full' not in locals():\n",
    "    base_vectors_full = base_vectors.astype(np.float32)\n",
    "    query_vectors_subset = query_vectors[:1000].astype(np.float32)\n",
    "\n",
    "# Ensure GT for Top-100 exists\n",
    "k_target = 100\n",
    "if 'gt_sweep' in locals() and gt_sweep.shape[1] >= k_target:\n",
    "    gt_100 = gt_sweep[:, :k_target]\n",
    "else:\n",
    "    print(\"Computing GT for Top-100...\")\n",
    "    gt_100 = calculate_top_k_ground_truth(query_vectors_subset, base_vectors_full, k=k_target)\n",
    "\n",
    "# Run Analysis\n",
    "mean_diffs, explained_var = analyze_pca_diff(base_vectors_full, query_vectors_subset, gt_100)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Mean Absolute Difference\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, 129), mean_diffs, color='skyblue')\n",
    "plt.xlabel('PCA Component (Dimension)')\n",
    "plt.ylabel('Mean Absolute Difference')\n",
    "plt.title('Mean Abs Diff between Query and Top-100 NN per PCA Dimension')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot 2: Explained Variance (for context)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 129), explained_var, marker='.', linestyle='-', color='orange')\n",
    "plt.xlabel('PCA Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Explained Variance Ratio')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save to CSV\n",
    "diff_df = pd.DataFrame({\n",
    "    'Dimension': range(1, 129),\n",
    "    'Mean_Abs_Diff': mean_diffs,\n",
    "    'Explained_Variance': explained_var\n",
    "})\n",
    "diff_df.to_csv('sift1m_pca_128_diff_analysis.csv', index=False)\n",
    "print(\"Analysis saved to sift1m_pca_128_diff_analysis.csv\")\n",
    "diff_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36d9c63",
   "metadata": {},
   "source": [
    "## 14. 降維後資料分佈分析 (尚未量化)\n",
    "分析各種降維方法處理後，但尚未進行 INT3 量化前的資料分佈情形。\n",
    "這有助於了解不同方法產生的數值範圍與分佈特性，進而評估量化策略的適用性。\n",
    "- **Overall Histogram**: 所有維度數值的整體分佈。\n",
    "- **Per-Dimension Heatmap**: 每一維度的數值分佈 (X軸為數值，Y軸為維度索引，顏色為頻率)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Data Distribution Analysis (Before Quantization)\n",
    "\n",
    "def analyze_distribution(data, method_name, save_prefix):\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes the distribution of data.\n",
    "    1. Overall histogram of all values.\n",
    "    2. Heatmap of distributions per dimension (Dimension vs Value).\n",
    "    \"\"\"\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data = data.cpu().numpy()\n",
    "        \n",
    "    N, D = data.shape\n",
    "    print(f\"Analyzing {method_name}: Shape {data.shape}\")\n",
    "    \n",
    "    # 1. Overall Histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # 使用 100 個 bins\n",
    "    plt.hist(data.flatten(), bins=100, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'{method_name} - Overall Value Distribution')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    filename_hist = f'{save_prefix}_overall_hist.png'\n",
    "    plt.savefig(filename_hist, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved {filename_hist}\")\n",
    "    \n",
    "    # 2. Per-Dimension Distribution (Heatmap)\n",
    "    # 計算每個維度的 Histogram 並堆疊成 Heatmap\n",
    "    \n",
    "    # Determine global min/max for binning\n",
    "    v_min, v_max = np.min(data), np.max(data)\n",
    "    bins = np.linspace(v_min, v_max, 101) # 100 bins\n",
    "    \n",
    "    hist_matrix = np.zeros((D, 100))\n",
    "    \n",
    "    for d in range(D):\n",
    "        hist, _ = np.histogram(data[:, d], bins=bins)\n",
    "        hist_matrix[d, :] = hist\n",
    "        \n",
    "    # Normalize for better visualization (顯示相對分佈形狀)\n",
    "    hist_matrix_norm = hist_matrix / (hist_matrix.max(axis=1, keepdims=True) + 1e-9)\n",
    "    \n",
    "    plt.figure(figsize=(12, max(6, D/4))) # 動態調整高度\n",
    "    # Extent: [left, right, bottom, top]\n",
    "    plt.imshow(hist_matrix_norm, aspect='auto', origin='lower', \n",
    "               extent=[v_min, v_max, 0, D], cmap='viridis')\n",
    "    plt.colorbar(label='Normalized Frequency')\n",
    "    plt.title(f'{method_name} - Per-Dimension Distribution Heatmap')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Dimension Index')\n",
    "    \n",
    "    filename_heatmap = f'{save_prefix}_dim_heatmap.png'\n",
    "    plt.savefig(filename_heatmap, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved {filename_heatmap}\")\n",
    "\n",
    "# 準備數據 (使用 10000 筆子集進行快速分析)\n",
    "subset_size = 10000\n",
    "# 確保 base_vectors 存在\n",
    "if 'base_vectors' not in locals():\n",
    "    print(\"Error: base_vectors not found. Please run previous cells to load data.\")\n",
    "else:\n",
    "    data_subset = base_vectors[:subset_size].astype(np.float32)\n",
    "    data_tensor = torch.from_numpy(data_subset).to(DEVICE)\n",
    "\n",
    "    print(f\"Using subset of {subset_size} vectors for distribution analysis.\")\n",
    "\n",
    "    # --- Method 1: Original (128 dims) ---\n",
    "    print(\"\\nAnalyzing Method 1 (Original)...\")\n",
    "    analyze_distribution(data_subset, \"Method 1 (Original 128D)\", \"dist_m1\")\n",
    "\n",
    "    # --- Method 2: Avg Pooling (64 dims) ---\n",
    "    print(\"\\nAnalyzing Method 2 (Avg Pooling)...\")\n",
    "    reshaped = data_tensor.reshape(data_tensor.shape[0], 64, 2)\n",
    "    m2_data = torch.mean(reshaped, dim=2)\n",
    "    analyze_distribution(m2_data, \"Method 2 (Avg Pooling 64D)\", \"dist_m2\")\n",
    "\n",
    "    # --- Method 3: PCA (64 dims) ---\n",
    "    print(\"\\nAnalyzing Method 3 (PCA)...\")\n",
    "    pca = PCA(n_components=64, whiten=True)\n",
    "    m3_data = pca.fit_transform(data_subset)\n",
    "    analyze_distribution(m3_data, \"Method 3 (PCA 64D)\", \"dist_m3\")\n",
    "\n",
    "    # --- Method 4: Max Mag Pooling (64 dims) ---\n",
    "    print(\"\\nAnalyzing Method 4 (Max Mag Pooling)...\")\n",
    "    reshaped = data_tensor.reshape(data_tensor.shape[0], 64, 2)\n",
    "    a = reshaped[:, :, 0]\n",
    "    b = reshaped[:, :, 1]\n",
    "    mask = torch.abs(a) >= torch.abs(b)\n",
    "    m4_data = torch.where(mask, a, b)\n",
    "    analyze_distribution(m4_data, \"Method 4 (Max Mag Pooling 64D)\", \"dist_m4\")\n",
    "\n",
    "    # --- Method 5: AutoEncoder (64 dims) ---\n",
    "    print(\"\\nAnalyzing Method 5 (AutoEncoder)...\")\n",
    "    # 訓練一個臨時的 AE 用於分析\n",
    "    ae_model = train_autoencoder(data_subset, 128, 64, epochs=50) \n",
    "    ae_model.eval()\n",
    "    with torch.no_grad():\n",
    "        m5_data = ae_model.encode(data_tensor)\n",
    "    analyze_distribution(m5_data, \"Method 5 (AutoEncoder 64D)\", \"dist_m5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461464a2",
   "metadata": {},
   "source": [
    "## 15. AutoEncoder Loss Comparison Experiment\n",
    "比較有無加入距離保留損失 (Distance Preservation Loss) 的 AutoEncoder 訓練過程與最終效果。\n",
    "- **Baseline**: Distance Loss Weight = 0.0\n",
    "- **Proposed**: Distance Loss Weight = 0.1 (or other value)\n",
    "使用 100K Database 進行快速驗證。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. AutoEncoder Loss Comparison Experiment\n",
    "\n",
    "def run_ae_loss_comparison(base_vectors, query_vectors, gt_top_k, retrieval_depths, k_true_neighbors, quantizer=quantize_to_int3):\n",
    "    print(\"Running AutoEncoder Loss Comparison Experiment...\")\n",
    "    \n",
    "    # Settings\n",
    "    weights = [0.0, 0.1] # Compare 0 (No Dist Loss) vs 0.1 (With Dist Loss)\n",
    "    results = []\n",
    "    histories = {}\n",
    "    \n",
    "    # Use 100K subset for training/testing to be faster\n",
    "    db_subset = base_vectors[:100000].astype(np.float32)\n",
    "    query_subset = query_vectors[:1000].astype(np.float32)\n",
    "    \n",
    "    # Ensure GT is for this subset\n",
    "    # Assuming gt_top_k passed in is already for 100K DB (from Part 1 of main experiment)\n",
    "    # If not, we should recalculate. Let's assume the user runs this after Part 1.\n",
    "    \n",
    "    for w in weights:\n",
    "        label = f\"AE (w={w})\"\n",
    "        print(f\"\\nTraining {label}...\")\n",
    "        \n",
    "        # Train and Transform\n",
    "        # Note: method5_autoencoder_int3 now returns history as 4th element\n",
    "        _, db_q, q_q, history = method5_autoencoder_int3(\n",
    "            db_subset, \n",
    "            query_subset, \n",
    "            epochs=50, # 50 epochs for comparison\n",
    "            quantizer=quantizer,\n",
    "            distance_loss_weight=w\n",
    "        )\n",
    "        \n",
    "        histories[label] = history\n",
    "        \n",
    "        # Evaluate Recall\n",
    "        print(f\"Evaluating {label}...\")\n",
    "        recalls = evaluate_recall_batched(q_q, db_q, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "        \n",
    "        res_entry = {'Method': label, **recalls}\n",
    "        results.append(res_entry)\n",
    "        print(f\"Result: {recalls}\")\n",
    "\n",
    "    # --- Visualization of Loss Curves ---\n",
    "    plt.figure(figsize=(18, 5))\n",
    "    \n",
    "    # Plot 1: Total Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    for label, hist in histories.items():\n",
    "        plt.plot(hist['loss'], label=label)\n",
    "    plt.title('Total Loss vs Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Plot 2: Reconstruction Loss\n",
    "    plt.subplot(1, 3, 2)\n",
    "    for label, hist in histories.items():\n",
    "        plt.plot(hist['recon_loss'], label=label)\n",
    "    plt.title('Reconstruction Loss vs Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Plot 3: Distance Loss\n",
    "    plt.subplot(1, 3, 3)\n",
    "    for label, hist in histories.items():\n",
    "        plt.plot(hist['dist_loss'], label=label)\n",
    "    plt.title('Distance Preservation Loss vs Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ae_loss_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Display Results Table ---\n",
    "    df_res = pd.DataFrame(results)\n",
    "    print(\"\\nComparison Results (Recall):\")\n",
    "    display(df_res)\n",
    "    return df_res\n",
    "\n",
    "# Run the comparison\n",
    "# We need GT for 100K DB. If 'gt_100k' exists from previous cells, use it.\n",
    "if 'gt_100k' in locals() and 'db_100k' in locals():\n",
    "    print(\"Using existing 100K GT and DB...\")\n",
    "    run_ae_loss_comparison(base_vectors, query_vectors, gt_100k, [1000, 5000, 10000], 100)\n",
    "else:\n",
    "    print(\"Calculating GT for 100K DB...\")\n",
    "    db_100k = base_vectors[:100000].astype(np.float32)\n",
    "    query_subset = query_vectors[:1000].astype(np.float32)\n",
    "    gt_100k = calculate_top_k_ground_truth(query_subset, db_100k, k=100)\n",
    "    run_ae_loss_comparison(base_vectors, query_vectors, gt_100k, [1000, 5000, 10000], 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae53acc",
   "metadata": {},
   "source": [
    "## 16. PCA Whitening vs No Whitening Comparison\n",
    "比較 PCA 降維時是否使用 Whitening 對最終檢索召回率的影響。\n",
    "- **配置**: PCA (64 dims) + INT3 Quantile Quantization (Per-Dimension) + L2 Distance\n",
    "- **比較**: `whiten=True` vs `whiten=False`\n",
    "- **指標**: Recall@100, @500, @1000 (Top-100 GT)\n",
    "- **數據集**: 100K Database Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. PCA Whitening Comparison\n",
    "\n",
    "def method3_pca_int3_configurable(data, query_data=None, quantizer=quantize_to_int3, whiten=True):\n",
    "    \"\"\"\n",
    "    Configurable PCA method (allows toggling whitening)\n",
    "    \"\"\"\n",
    "    label = \"PCA+Whitening\" if whiten else \"PCA (No Whitening)\"\n",
    "    print(f\"  - Training {label}...\")\n",
    "    \n",
    "    # Ensure data is numpy for sklearn fitting\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data_np = data.cpu().numpy()\n",
    "    else:\n",
    "        data_np = data\n",
    "        \n",
    "    # Fit PCA\n",
    "    pca = PCA(n_components=REDUCED_DIM, whiten=whiten)\n",
    "    pca.fit(data_np)\n",
    "    \n",
    "    # Prepare for GPU transform\n",
    "    mean = torch.from_numpy(pca.mean_).float().to(DEVICE)\n",
    "    components = torch.from_numpy(pca.components_).float().to(DEVICE)\n",
    "    explained_variance = torch.from_numpy(pca.explained_variance_).float().to(DEVICE)\n",
    "    \n",
    "    def transform_gpu(X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = torch.from_numpy(X).to(DEVICE)\n",
    "        X = X.float()\n",
    "        \n",
    "        # Center\n",
    "        X_centered = X - mean\n",
    "        # Project\n",
    "        X_transformed = torch.matmul(X_centered, components.T)\n",
    "        \n",
    "        # Whiten\n",
    "        if pca.whiten:\n",
    "            scale = torch.sqrt(explained_variance)\n",
    "            X_transformed = X_transformed / scale\n",
    "            \n",
    "        return X_transformed\n",
    "\n",
    "    data_reduced = transform_gpu(data_np)\n",
    "    query_reduced = transform_gpu(query_data) if query_data is not None else None\n",
    "    \n",
    "    # Apply Quantization\n",
    "    db_q, q_q = apply_quantization(data_reduced, query_reduced, quantizer)\n",
    "    \n",
    "    return db_q, q_q\n",
    "\n",
    "def run_pca_whitening_comparison(base_vectors, query_vectors, gt_top_k, retrieval_depths, k_true_neighbors):\n",
    "    print(\"Running PCA Whitening vs No Whitening Comparison...\")\n",
    "    \n",
    "    # Settings\n",
    "    configs = [False, True]\n",
    "    results = []\n",
    "    \n",
    "    # Use 100K subset\n",
    "    db_subset = base_vectors[:100000].astype(np.float32)\n",
    "    query_subset = query_vectors[:1000].astype(np.float32)\n",
    "    \n",
    "    for use_whiten in configs:\n",
    "        label = \"PCA + Whitening\" if use_whiten else \"PCA (No Whitening)\"\n",
    "        \n",
    "        # Switch to Min-Max Quantization (Standard Percentile) instead of Quartile\n",
    "        # quantize_to_int3 is the function defined earlier for 1%-99% min-max clipping\n",
    "        quantizer = quantize_to_int3 \n",
    "        \n",
    "        t0 = time.time()\n",
    "        db_q, q_q = method3_pca_int3_configurable(\n",
    "            db_subset, \n",
    "            query_subset, \n",
    "            quantizer=quantizer, \n",
    "            whiten=use_whiten\n",
    "        )\n",
    "        t_enc = time.time() - t0\n",
    "        \n",
    "        # Evaluate\n",
    "        print(f\"Evaluating {label}...\")\n",
    "        recalls = evaluate_recall_batched(q_q, db_q, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "        \n",
    "        results.append({\n",
    "            'Method': label,\n",
    "            'Quantizer': 'Min-Max (Standard)',\n",
    "            'Encoding Time': t_enc,\n",
    "            **recalls\n",
    "        })\n",
    "        print(f\"Result: {recalls}\")\n",
    "        \n",
    "    # Create DataFrame\n",
    "    df_res = pd.DataFrame(results)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(len(retrieval_depths))\n",
    "    width = 0.35\n",
    "    \n",
    "    row_no_white = df_res[df_res['Method'] == 'PCA (No Whitening)'].iloc[0]\n",
    "    row_white = df_res[df_res['Method'] == 'PCA + Whitening'].iloc[0]\n",
    "    \n",
    "    vals_no_white = [row_no_white[f'recall@{k}'] for k in retrieval_depths]\n",
    "    vals_white = [row_white[f'recall@{k}'] for k in retrieval_depths]\n",
    "    \n",
    "    plt.bar(x - width/2, vals_no_white, width, label='PCA (No Whitening)', color='#1f77b4')\n",
    "    plt.bar(x + width/2, vals_white, width, label='PCA + Whitening', color='#ff7f0e')\n",
    "    \n",
    "    plt.xlabel('Retrieval Depth')\n",
    "    plt.ylabel(f'Recall (Top-{k_true_neighbors} GT)')\n",
    "    plt.title('PCA Whitening Comparison (100K DB, Min-Max Quantization)')\n",
    "    plt.xticks(x, [f'@{k}' for k in retrieval_depths])\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for i, v in enumerate(vals_no_white):\n",
    "        plt.text(i - width/2, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    for i, v in enumerate(vals_white):\n",
    "        plt.text(i + width/2, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pca_whitening_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nDetailed Results:\")\n",
    "    display(df_res)\n",
    "    return df_res\n",
    "\n",
    "# Run the experiment\n",
    "# Depths: 100, 500, 1000\n",
    "# GT: Top 100\n",
    "depths = [100, 500, 1000]\n",
    "k_gt = 100\n",
    "\n",
    "if 'gt_100k' in locals() and 'db_100k' in locals():\n",
    "    print(\"Using existing 100K GT and DB...\")\n",
    "    # Check if gt_100k has k=100\n",
    "    if gt_100k.shape[1] < k_gt:\n",
    "         print(f\"Existing GT has k={gt_100k.shape[1]}, need {k_gt}. Recalculating...\")\n",
    "         gt_100k = calculate_top_k_ground_truth(query_subset, db_100k, k=k_gt)\n",
    "    run_pca_whitening_comparison(base_vectors, query_vectors, gt_100k, depths, k_gt)\n",
    "else:\n",
    "    print(\"Calculating GT for 100K DB...\")\n",
    "    db_100k = base_vectors[:100000].astype(np.float32)\n",
    "    query_subset = query_vectors[:1000].astype(np.float32)\n",
    "    gt_100k = calculate_top_k_ground_truth(query_subset, db_100k, k=k_gt)\n",
    "    run_pca_whitening_comparison(base_vectors, query_vectors, gt_100k, depths, k_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b12a0",
   "metadata": {},
   "source": [
    "## 17. Nearest Neighbor Distribution Analysis (Single Query)\n",
    "分析特定查詢向量 (Query Vector) 的 Top-100 Nearest Neighbors 在整體數據空間中的分佈情形。\n",
    "1. **Distance Distribution**: 該查詢向量到資料庫所有點的距離分佈，並標示出 Top-100 NN 的位置。\n",
    "2. **2D PCA Projection**: 將 **(隨機採樣背景點 + Top-100 NN + Query)** 組成的數據集，使用 **PCA** 降維到 2 維平面。\n",
    "   - **降維方法**: 使用 `sklearn.decomposition.PCA(n_components=2)`。\n",
    "   - **數據來源**: 用於擬合 PCA 的數據包含了 10000 個隨機背景點、100 個目標近鄰點和 1 個查詢點。這確保了投影平面能夠捕捉到該局部查詢區域和整體分佈的主要變異方向。\n",
    "   - **目的**: 觀察在高維空間中與 Query 最近的那些點，在降維後的 2D 平面上是否依然聚集在 Query 附近，或者因為維度壓縮而與背景點混雜在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Top-100 NN Distribution Analysis\n",
    "\n",
    "def analyze_nn_distribution(query_idx, db_vectors, query_vectors, gt_indices, sample_size=10000):\n",
    "    \"\"\"\n",
    "    Analyzes the distribution of Top-100 NN for a specific query.\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing Query Index: {query_idx}\")\n",
    "    \n",
    "    q_vec = query_vectors[query_idx].reshape(1, -1) # (1, 128)\n",
    "    nn_indices = gt_indices[query_idx] # (100,)\n",
    "    \n",
    "    # 1. Distance Distribution Analysis\n",
    "    # Calculate distances to ALL DB vectors (or a large subset if DB is huge)\n",
    "    # Using CPU for simplicity in plotting logic, or GPU if available\n",
    "    dists = pairwise_distances(q_vec, db_vectors, metric='euclidean').flatten() # (N,)\n",
    "    \n",
    "    nn_dists = dists[nn_indices]\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Histogram of Distances\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    # Background: All distances\n",
    "    plt.hist(dists, bins=100, color='lightgray', label='All DB Vectors', density=True)\n",
    "    \n",
    "    # Foreground: NN distances (Use a different scale or just rug plot/distinct hist)\n",
    "    # Since NN counts are small compared to N, density=True helps, but they might still be invisible.\n",
    "    # Let's use a vertical line for the max NN distance (search radius)\n",
    "    radius = np.max(nn_dists)\n",
    "    plt.axvline(x=radius, color='r', linestyle='--', linewidth=2, label=f'radius (Top-100) = {radius:.2f}')\n",
    "    \n",
    "    # Zoom in on the histogram for the small distances?\n",
    "    # Or just overlay a histogram of the NNs?\n",
    "    plt.hist(nn_dists, bins=20, color='red', alpha=0.5, label='Top-100 NN', density=True)\n",
    "    \n",
    "    plt.title(f'Distance Distribution (Query {query_idx})')\n",
    "    plt.xlabel('L2 Distance')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=':', alpha=0.6)\n",
    "    \n",
    "    # Plot 2: 2D PCA Projection\n",
    "    # Project specific points: Random Subset + Top 100 NN + Query\n",
    "    \n",
    "    # Create mask for NNs to exclude them from random sample to avoid duplicates drawing\n",
    "    mask = np.ones(len(db_vectors), dtype=bool)\n",
    "    mask[nn_indices] = False\n",
    "    background_indices = np.where(mask)[0]\n",
    "    \n",
    "    # Sample background\n",
    "    if len(background_indices) > sample_size:\n",
    "        bg_sample_indices = np.random.choice(background_indices, sample_size, replace=False)\n",
    "    else:\n",
    "        bg_sample_indices = background_indices\n",
    "        \n",
    "    bg_vectors = db_vectors[bg_sample_indices]\n",
    "    nn_vectors = db_vectors[nn_indices]\n",
    "    \n",
    "    # Combine for PCA fitting\n",
    "    combined = np.vstack([bg_vectors, nn_vectors, q_vec])\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    combined_2d = pca.fit_transform(combined)\n",
    "    \n",
    "    # Split back\n",
    "    bg_2d = combined_2d[:len(bg_vectors)]\n",
    "    nn_2d = combined_2d[len(bg_vectors):-1]\n",
    "    q_2d = combined_2d[-1]\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(bg_2d[:, 0], bg_2d[:, 1], c='lightgray', alpha=0.5, s=10, label='Background (Random Subset)')\n",
    "    plt.scatter(nn_2d[:, 0], nn_2d[:, 1], c='red', alpha=0.8, s=20, label='Top-100 NN')\n",
    "    plt.scatter(q_2d[0], q_2d[1], c='blue', marker='*', s=200, label='Query')\n",
    "    \n",
    "    plt.title(f'PCA 2D Projection (Query {query_idx})')\n",
    "    plt.xlabel('PC 1')\n",
    "    plt.ylabel('PC 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=':', alpha=0.6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'query_{query_idx}_distribution.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    print(f\"Saved visualization to {filename}\")\n",
    "\n",
    "# Run Analysis on a few random queries\n",
    "# Using 100K subset for speed\n",
    "if 'db_100k' not in locals():\n",
    "    db_100k = base_vectors[:100000].astype(np.float32)\n",
    "    query_subset = query_vectors[:1000].astype(np.float32)\n",
    "    gt_100k = calculate_top_k_ground_truth(query_subset, db_100k, k=100)\n",
    "\n",
    "# Pick 3 random queries\n",
    "import random\n",
    "random.seed(42)\n",
    "test_queries = [0, 10, 50] # Check first few or random\n",
    "\n",
    "for q_idx in test_queries:\n",
    "    analyze_nn_distribution(q_idx, db_100k, query_subset, gt_100k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb80f88",
   "metadata": {},
   "source": [
    "## 18. Aggregate Dimension Analysis (NN vs Avg vs Farthest)\n",
    "統計分析所有 Query 的 Top-100 Nearest Neighbors、整體分佈 (Average)、以及最遠點 (Farthest) 在每個維度上的距離差異。\n",
    "- **Metric**: Absolute Difference in each dimension ($|q_d - x_d|$).\n",
    "- **Scope**: Average over all queries.\n",
    "- **Targets**:\n",
    "    1.  **Top-100 NN**: Average distance of top-100 neighbors.\n",
    "    2.  **Average Data**: Average distance of randomly sampled background data.\n",
    "    3.  **Farthest Data**: Distance of the single farthest data point (L2-based).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Aggregate Dimension Analysis\n",
    "\n",
    "def analyze_dimension_stats(db_vectors, query_vectors, gt_indices, sample_size_for_avg=5000, batch_size=100):\n",
    "    \"\"\"\n",
    "    Calculates average absolute difference per dimension for:\n",
    "    1. Top-100 NNs\n",
    "    2. Random Sample (representing Average Data)\n",
    "    3. Farthest Data (in terms of L2 distance)\n",
    "    \n",
    "    Returns metrics averaged over all queries.\n",
    "    \"\"\"\n",
    "    print(\"Running Aggregate Dimension Analysis...\")\n",
    "    \n",
    "    num_queries = len(query_vectors)\n",
    "    dim = query_vectors.shape[1]\n",
    "    \n",
    "    # Initialize accumulators\n",
    "    total_nn_diff = torch.zeros(dim).to(DEVICE)\n",
    "    total_avg_diff = torch.zeros(dim).to(DEVICE)\n",
    "    total_far_diff = torch.zeros(dim).to(DEVICE)\n",
    "    \n",
    "    # Prepare DB tensor\n",
    "    if isinstance(db_vectors, np.ndarray):\n",
    "        db_tensor = torch.from_numpy(db_vectors).to(DEVICE)\n",
    "    else:\n",
    "        db_tensor = db_vectors.to(DEVICE)\n",
    "        \n",
    "    # Sample DB for Average Stats (to speed up)\n",
    "    # We use a fixed sample for 'Average Data' metric estimation\n",
    "    perm = torch.randperm(len(db_tensor))[:sample_size_for_avg]\n",
    "    db_sample = db_tensor[perm]\n",
    "    \n",
    "    # Pre-calc DB norms for Farthest search\n",
    "    db_sq = torch.sum(db_tensor**2, dim=1)\n",
    "    \n",
    "    # Process queries in batches\n",
    "    for i in tqdm(range(0, num_queries, batch_size), desc=\"Analyzing Queries\"):\n",
    "        q_end = min(i + batch_size, num_queries)\n",
    "        q_batch_np = query_vectors[i:q_end]\n",
    "        gt_batch = gt_indices[i:q_end] # (Batch, 100)\n",
    "        \n",
    "        q_batch = torch.from_numpy(q_batch_np).to(DEVICE) # (Batch, D)\n",
    "        \n",
    "        # --- 1. Top-100 NN Diff ---\n",
    "        # Gather NN vectors: (Batch, 100, D)\n",
    "        # We need to flatten indices to gather efficiently or loop\n",
    "        # For simplicity in logic:\n",
    "        batch_nn_diff = torch.zeros(q_batch.shape[0], dim).to(DEVICE)\n",
    "        \n",
    "        for b in range(len(q_batch)):\n",
    "            nn_idxs = gt_batch[b] # (100,)\n",
    "            nn_vecs = db_tensor[nn_idxs] # (100, D)\n",
    "            # Abs Diff per dim: (100, D)\n",
    "            diffs = torch.abs(nn_vecs - q_batch[b].unsqueeze(0))\n",
    "            # Mean over 100 neighbors\n",
    "            batch_nn_diff[b] = torch.mean(diffs, dim=0)\n",
    "            \n",
    "        total_nn_diff += torch.sum(batch_nn_diff, dim=0)\n",
    "        \n",
    "        # --- 2. Average Data Diff ---\n",
    "        # Compare q_batch (Batch, D) vs db_sample (Sample, D)\n",
    "        # Result: (Batch, D) -> mean abs diff with all sample points\n",
    "        # efficiently: expand dims\n",
    "        # (Batch, 1, D) - (1, Sample, D) -> (Batch, Sample, D)\n",
    "        # Be careful with memory. 100 * 5000 * 128 * 4 bytes ~ 250MB. OK.\n",
    "        avg_diffs = torch.abs(q_batch.unsqueeze(1) - db_sample.unsqueeze(0))\n",
    "        batch_avg_diff = torch.mean(avg_diffs, dim=1) # (Batch, D)\n",
    "        total_avg_diff += torch.sum(batch_avg_diff, dim=0)\n",
    "        \n",
    "        # --- 3. Farthest Data Diff ---\n",
    "        # Find farthest L2 point\n",
    "        q_sq = torch.sum(q_batch**2, dim=1, keepdim=True)\n",
    "        # Dist Matrix: (Batch, DB_Full)\n",
    "        # If DB is huge (1M), we need to chunk this calculation or use a larger sample\n",
    "        # Assuming we can iterate chunks of DB to find max\n",
    "        \n",
    "        # To strictly find the farthest in 1M DB, we need to scan all.\n",
    "        # Let's do a chunked scan for max distance.\n",
    "        max_dists = torch.full((len(q_batch),), -1.0).to(DEVICE)\n",
    "        max_indices = torch.full((len(q_batch),), -1, dtype=torch.long).to(DEVICE)\n",
    "        \n",
    "        chunk_size = 50000\n",
    "        for j in range(0, len(db_tensor), chunk_size):\n",
    "            db_end = min(j + chunk_size, len(db_tensor))\n",
    "            db_chunk = db_tensor[j:db_end]\n",
    "            db_sq_chunk = db_sq[j:db_end]\n",
    "            \n",
    "            # L2^2 = q^2 + db^2 - 2q.db\n",
    "            term2 = -2 * torch.matmul(q_batch, db_chunk.t())\n",
    "            dists = q_sq + db_sq_chunk + term2\n",
    "            \n",
    "            # Find max in chunk\n",
    "            curr_max, curr_idx = torch.max(dists, dim=1)\n",
    "            \n",
    "            # Update global max\n",
    "            mask = curr_max > max_dists\n",
    "            max_dists[mask] = curr_max[mask]\n",
    "            max_indices[mask] = curr_idx[mask] + j\n",
    "            \n",
    "        # Retrieve farthest vectors\n",
    "        farthest_vecs = db_tensor[max_indices] # (Batch, D)\n",
    "        batch_far_diff = torch.abs(farthest_vecs - q_batch) # (Batch, D)\n",
    "        total_far_diff += torch.sum(batch_far_diff, dim=0)\n",
    "        \n",
    "    # Average over all queries\n",
    "    avg_nn_per_dim = total_nn_diff / num_queries\n",
    "    avg_data_per_dim = total_avg_diff / num_queries\n",
    "    avg_far_per_dim = total_far_diff / num_queries\n",
    "    \n",
    "    return avg_nn_per_dim.cpu().numpy(), avg_data_per_dim.cpu().numpy(), avg_far_per_dim.cpu().numpy()\n",
    "\n",
    "# Run it\n",
    "# Use 100K DB for faster execution, or 1M if required. \n",
    "# Prompt implies \"doing the final experiment\", likely referring to the dataset used recently.\n",
    "# Let's use 100K subset defined previously as 'db_100k' and 'query_subset' (1000 queries).\n",
    "if 'db_100k' not in locals():\n",
    "    # Fallback setup\n",
    "    db_100k = base_vectors[:100000].astype(np.float32)\n",
    "    query_subset = query_vectors[:1000].astype(np.float32)\n",
    "    # Recalc GT\n",
    "    gt_100k = calculate_top_k_ground_truth(query_subset, db_100k, k=100)\n",
    "\n",
    "nn_stats, avg_stats, far_stats = analyze_dimension_stats(db_100k, query_subset, gt_100k)\n",
    "\n",
    "# Statistics Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Aggregate Dimension Statistics (Averaged over Queries)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Mean Abs Diff (Top-100 NN): {np.mean(nn_stats):.4f}\")\n",
    "print(f\"Mean Abs Diff (Average Data): {np.mean(avg_stats):.4f}\")\n",
    "print(f\"Mean Abs Diff (Farthest Data): {np.mean(far_stats):.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "dims = range(len(nn_stats))\n",
    "\n",
    "plt.plot(dims, nn_stats, label='Top-100 NNs', color='red', linewidth=1.5)\n",
    "plt.plot(dims, avg_stats, label='Average Data (Random Sample)', color='blue', alpha=0.6, linewidth=1)\n",
    "plt.plot(dims, far_stats, label='Farthest Data', color='green', alpha=0.6, linewidth=1)\n",
    "\n",
    "plt.xlabel('Dimension Index')\n",
    "plt.ylabel('Mean Absolute Difference')\n",
    "plt.title('Mean Diff per Dimension: NN vs Avg vs Far')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('aggregate_dim_stats.png')\n",
    "plt.show()\n",
    "\n",
    "# Visualize Summary Bars\n",
    "plt.figure(figsize=(8, 5))\n",
    "labels = ['Top-100 NN', 'Average Data', 'Farthest Data']\n",
    "means = [np.mean(nn_stats), np.mean(avg_stats), np.mean(far_stats)]\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "plt.bar(labels, means, color=colors, alpha=0.7)\n",
    "plt.ylabel('Mean Absolute Diff (Averaged across Dims)')\n",
    "plt.title('Overall Comparison of Distances')\n",
    "for i, v in enumerate(means):\n",
    "    plt.text(i, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424db400",
   "metadata": {},
   "source": [
    "## 19. Grouped Distance Experiment (Full SIFT1M Dataset)\n",
    "在 **完整 SIFT1M (1M Vectors)** 數據集上比較四種方法。\n",
    "針對 64維的方法，使用 **Grouped L2 Distance** 作為檢索度量。\n",
    "\n",
    "**Grouped L2 Distance 實現方式**:\n",
    "將 64 維向量切分為多個小組 (Group)。\n",
    "- 設定 `group_size = 6`。\n",
    "- 分組方式：Dimension 0-5, 6-11, ..., 最後一組為 60-63 (4維)。\n",
    "- 計算公式：先計算每個 Group 的 L2 Distance，然後將所有 Groups 的距離相加。\n",
    "  $D_{total}(x, y) = \\sum_{g} ||x_g - y_g||_2$\n",
    "\n",
    "**Methods**:\n",
    "1. **Direct INT3 (128D)**: Standard L2.\n",
    "2. **AvgPool (64D)**: Min-Max Quantization + Grouped L2.\n",
    "3. **PCA (64D)**: Percentile Quantization + Grouped L2.\n",
    "4. **AutoEncoder (64D)**: Recon+Dist Loss + Percentile Quantization + Grouped L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3575da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. Threshold Experiment (Grouped L2 with Threshold Filtering)\n",
    "\n",
    "def estimate_safe_threshold(q_vecs, db_vecs, gt, group_size=6, sample_size=1000, percentile=98):\n",
    "    \"\"\"\n",
    "    Estimates:\n",
    "    1. Grouped L2 Threshold (safe_threshold): Target for Latent Space\n",
    "    2. Input Space L2 Limit (input_limit): Standard L2 distance for neighbors in original space\n",
    "    \"\"\"\n",
    "    print(f\"Estimating thresholds from {len(q_vecs)} queries (target sample {sample_size})...\")\n",
    "    \n",
    "    n_samples = min(len(q_vecs), sample_size)\n",
    "    indices = np.random.choice(len(q_vecs), n_samples, replace=False)\n",
    "    \n",
    "    if isinstance(q_vecs, np.ndarray):\n",
    "        q_sample = torch.from_numpy(q_vecs[indices]).to(DEVICE).float()\n",
    "    else:\n",
    "        q_sample = q_vecs[indices].to(DEVICE).float()\n",
    "        \n",
    "    gt_sample = gt[indices]\n",
    "    \n",
    "    all_group_dists = []\n",
    "    max_input_dists = [] # Standard L2 distance for True Neighbors\n",
    "    \n",
    "    D = q_sample.shape[1]\n",
    "    \n",
    "    for i in range(len(q_sample)):\n",
    "        q = q_sample[i:i+1] # (1, D)\n",
    "        neighbor_indices = gt_sample[i] # Indices\n",
    "        \n",
    "        # Access neighbors from DB\n",
    "        # Warning: db_vecs might be huge, use careful indexing\n",
    "        if isinstance(db_vecs, torch.Tensor):\n",
    "             neighbors = db_vecs[neighbor_indices].float()\n",
    "        else:\n",
    "             neighbors = torch.from_numpy(db_vecs[neighbor_indices]).to(DEVICE).float()\n",
    "             \n",
    "        # 1. Calc Grouped distances\n",
    "        for start_idx in range(0, D, group_size):\n",
    "            end_idx = min(start_idx + group_size, D)\n",
    "            current_group_size = end_idx - start_idx\n",
    "            \n",
    "            q_sub = q[:, start_idx:end_idx]\n",
    "            db_sub = neighbors[:, start_idx:end_idx]\n",
    "            \n",
    "            dists = torch.cdist(q_sub, db_sub, p=2) # (1, 100)\n",
    "            vals = dists.flatten()\n",
    "            \n",
    "            # Normalize small groups\n",
    "            if current_group_size < group_size:\n",
    "                vals = vals * (group_size / current_group_size)\n",
    "            \n",
    "            all_group_dists.append(vals)\n",
    "            \n",
    "        # 2. Calc Standard Input Distance (Input Limit estimation)\n",
    "        # Note: If input is 64D (compressed), this estimates limit for compressed space\n",
    "        # If input is 128D (original), estimates for original space\n",
    "        # Here we perform it on whatever vectors are passed.\n",
    "        # If we want 128D limit, we must pass 128D vectors.\n",
    "        # In the context of AE training, we need limit for 128D Input.\n",
    "        \n",
    "        input_dists = torch.cdist(q, neighbors, p=2).flatten()\n",
    "        max_input_dists.append(input_dists)\n",
    "            \n",
    "    all_dists = torch.cat(all_group_dists)\n",
    "    all_input_dists = torch.cat(max_input_dists)\n",
    "    \n",
    "    threshold = torch.quantile(all_dists, percentile / 100.0).item()\n",
    "    input_limit = torch.quantile(all_input_dists, percentile / 100.0).item()\n",
    "    \n",
    "    print(f\"Estimated Base Threshold (P{percentile}): {threshold:.4f}\")\n",
    "    print(f\"Estimated Input Neighbor Limit (P{percentile}): {input_limit:.4f}\")\n",
    "    return threshold, input_limit\n",
    "\n",
    "def evaluate_recall_grouped_threshold_batched(query_vectors, db_vectors, gt_top_k, retrieval_depths, k_true_neighbors, group_size=6, threshold=None):\n",
    "    \"\"\"\n",
    "    Evaluate Recall using Grouped L2 distance with Hard Threshold Filtering.\n",
    "    \"\"\"\n",
    "    max_depth = max(retrieval_depths)\n",
    "    num_queries = len(query_vectors)\n",
    "    query_batch_size = 100\n",
    "    \n",
    "    total_hits = {r: 0 for r in retrieval_depths}\n",
    "    \n",
    "    if isinstance(query_vectors, np.ndarray):\n",
    "        q_vectors = torch.from_numpy(query_vectors).to(DEVICE).float()\n",
    "    else:\n",
    "        q_vectors = query_vectors.to(DEVICE).float()\n",
    "        \n",
    "    if isinstance(db_vectors, np.ndarray):\n",
    "        d_vectors = torch.from_numpy(db_vectors).to(DEVICE).float()\n",
    "    else:\n",
    "        d_vectors = db_vectors.to(DEVICE).float()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in tqdm(range(0, num_queries, query_batch_size), desc=f\"Eval Threshold={threshold if threshold else 'None'}\", leave=False):\n",
    "        q_end = min(i + query_batch_size, num_queries)\n",
    "        q_batch = q_vectors[i:q_end] # (B, D)\n",
    "        gt_batch = gt_top_k[i:q_end]\n",
    "        \n",
    "        B = q_batch.shape[0]\n",
    "        N = d_vectors.shape[0]\n",
    "        D = q_batch.shape[1]\n",
    "        \n",
    "        # Initialize Total Distance and Valid Mask\n",
    "        total_dist = torch.zeros((B, N), device=DEVICE)\n",
    "        valid_mask = torch.ones((B, N), dtype=torch.bool, device=DEVICE)\n",
    "        \n",
    "        if threshold is not None:\n",
    "             for start_idx in range(0, D, group_size):\n",
    "                end_idx = min(start_idx + group_size, D)\n",
    "                current_group_size = end_idx - start_idx\n",
    "                \n",
    "                # Determine local threshold\n",
    "                current_threshold = threshold\n",
    "                if current_group_size < group_size:\n",
    "                    # Scale threshold for smaller groups: T * (current / base)\n",
    "                    current_threshold = threshold * (current_group_size / group_size)\n",
    "                \n",
    "                q_sub = q_batch[:, start_idx:end_idx]\n",
    "                db_sub = d_vectors[:, start_idx:end_idx]\n",
    "                \n",
    "                dist_sub = torch.cdist(q_sub, db_sub, p=2)\n",
    "                \n",
    "                # Filter\n",
    "                valid_mask &= (dist_sub <= current_threshold)\n",
    "                total_dist += dist_sub\n",
    "        else:\n",
    "             for start_idx in range(0, D, group_size):\n",
    "                end_idx = min(start_idx + group_size, D)\n",
    "                q_sub = q_batch[:, start_idx:end_idx]\n",
    "                db_sub = d_vectors[:, start_idx:end_idx]\n",
    "                total_dist += torch.cdist(q_sub, db_sub, p=2)\n",
    "                \n",
    "        # Apply Infinity to invalid\n",
    "        if threshold is not None:\n",
    "            total_dist.masked_fill_(~valid_mask, float('inf'))\n",
    "            \n",
    "        # Top K\n",
    "        _, sorted_indices = torch.topk(total_dist, k=max_depth, dim=1, largest=False)\n",
    "        sorted_indices = sorted_indices.cpu().numpy()\n",
    "        \n",
    "        for r in retrieval_depths:\n",
    "            retrieved = sorted_indices[:, :r]\n",
    "            for j in range(len(gt_batch)):\n",
    "                hits = len(set(gt_batch[j]) & set(retrieved[j]))\n",
    "                total_hits[r] += hits\n",
    "                \n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    recalls = {}\n",
    "    for r in retrieval_depths:\n",
    "        recalls[f'recall@{r}'] = total_hits[r] / (num_queries * float(k_true_neighbors))\n",
    "        \n",
    "    recalls['time'] = time_taken\n",
    "    return recalls\n",
    "\n",
    "def run_threshold_experiment(base_vectors, query_vectors, gt_top_k, retrieval_depths, k_true_neighbors):\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Running Threshold Experiment (Full SIFT1M Dataset)...\")\n",
    "    \n",
    "    db_full = base_vectors\n",
    "    query_full = query_vectors\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Setup: Estimating Threshold\n",
    "    # We need 2 estimates:\n",
    "    # 1. 'safe_threshold' (Group L2) - calculated using quant/reduced data\n",
    "    # 2. 'input_limit' (Standard L2) - calculated using ORIGINAL 128D data\n",
    "    \n",
    "    # A. Estimate Input Limit using Original Vectors\n",
    "    print(\"\\n[Setup] Estimating Input Space Limit (128D)...\")\n",
    "    q_sub = query_full[:1000]\n",
    "    gt_sub = gt_top_k[:1000]\n",
    "    \n",
    "    # Fake call to estimate_safe_threshold but passing 128D data and group_size=128 (so global L2)\n",
    "    # Actually, estimate_safe_threshold returns both.\n",
    "    # But for 'Group L2' threshold we need reduced data.\n",
    "    # For 'Input Limit' we need original data.\n",
    "    \n",
    "    # Let's perform estimate on Original Data first to get input_limit\n",
    "    # Using group_size=128 means \"Single Group\" = Standard L2\n",
    "    _, input_limit_est = estimate_safe_threshold(q_sub, db_full, gt_sub, group_size=128, percentile=98)\n",
    "    \n",
    "    # B. Estimate Group Threshold using Quantized (AvgPool) Data\n",
    "    print(\"\\n[Setup] Estimating Group Threshold using AvgPool (64D) Quantized Data...\")\n",
    "    quantizer = quantize_to_int3\n",
    "    _, db_est, q_est = method2_average_pooling_int3(db_full, q_sub, quantizer=quantizer)\n",
    "    \n",
    "    # Note: Pass Float for DB if using method2 output which might be quantized\n",
    "    # method2 returns quantized. Cast to float handled inside estimator.\n",
    "    safe_threshold_est, _ = estimate_safe_threshold(q_est, db_est, gt_sub, group_size=6, percentile=95) # P95 tighter for thresholding? Or P98.\n",
    "    \n",
    "    print(f\"Set Global Threshold T={safe_threshold_est:.4f}\")\n",
    "    print(f\"Set Input Neighbor Limit L={input_limit_est:.4f}\")\n",
    "    \n",
    "    # Config for AE training\n",
    "    threshold_config = {\n",
    "        'target_threshold': safe_threshold_est,\n",
    "        'input_limit': input_limit_est\n",
    "    }\n",
    "    \n",
    "    # --- 1. Baseline INT3 (128D) ---\n",
    "    print(\"\\n[Baseline] Direct INT3 (128D)...\")\n",
    "    quantizer = quantize_to_int3\n",
    "    _, db_q, q_q = method1_direct_int3(db_full, query_full, quantizer=quantizer)\n",
    "    res = evaluate_recall_grouped_threshold_batched(q_q, db_q, gt_top_k, retrieval_depths, k_true_neighbors, threshold=None)\n",
    "    results.append({'Method': 'Baseline (128D)', 'Config': 'Standard', **res})\n",
    "    \n",
    "    # Define Methods\n",
    "    methods_config = [\n",
    "        ('AvgPool (64D)', method2_average_pooling_int3, quantize_to_int3, {}),\n",
    "        ('PCA (64D)', method3_pca_int3, PerDimensionQuantileQuantizer(), {}),\n",
    "        ('AutoEncoder (64D, No Penalty)', method5_autoencoder_int3, PerDimensionQuantileQuantizer(), \n",
    "             {'train_subset': True, 'distance_loss_weight': 0.1, 'use_grouped_latent_dist': True, 'threshold_config': None}),\n",
    "        ('AutoEncoder (64D, Threshold Penalty)', method5_autoencoder_int3, PerDimensionQuantileQuantizer(), \n",
    "             {'train_subset': True, 'distance_loss_weight': 0.1, 'use_grouped_latent_dist': True, 'threshold_config': threshold_config})\n",
    "    ]\n",
    "    \n",
    "    for name, method_func, quantizer_inst, kwargs in methods_config:\n",
    "        print(f\"\\n[{name}] Processing...\")\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if 'train_subset' in kwargs:\n",
    "            train_sub = db_full[:100000]\n",
    "            if isinstance(train_sub, torch.Tensor): train_sub = train_sub.cpu().numpy()\n",
    "            \n",
    "            # Call method\n",
    "            ae_model, _, _ = method_func(train_sub, None, epochs=50, quantizer=quantizer_inst, \n",
    "                                         distance_loss_weight=kwargs.get('distance_loss_weight', 0.1),\n",
    "                                         use_grouped_latent_dist=kwargs.get('use_grouped_latent_dist', False),\n",
    "                                         threshold_config=kwargs.get('threshold_config', None))\n",
    "            \n",
    "            # Apply\n",
    "            print(\"  Encoding Full DB...\")\n",
    "            ae_model.eval()\n",
    "            with torch.no_grad():\n",
    "                if isinstance(db_full, np.ndarray): db_tensor = torch.from_numpy(db_full).to(DEVICE)\n",
    "                else: db_tensor = db_full.to(DEVICE)\n",
    "                full_db_reduced = ae_model.encode(db_tensor.float())\n",
    "                \n",
    "                if isinstance(query_full, np.ndarray): q_tensor = torch.from_numpy(query_full).to(DEVICE)\n",
    "                else: q_tensor = query_full.to(DEVICE)\n",
    "                full_q_reduced = ae_model.encode(q_tensor.float())\n",
    "                \n",
    "            db_q = quantizer_inst.fit_transform(full_db_reduced)\n",
    "            q_q = quantizer_inst.transform(full_q_reduced)\n",
    "            \n",
    "        else:\n",
    "            _, db_q, q_q = method_func(db_full, query_full, quantizer=quantizer_inst)\n",
    "            \n",
    "        # 1. Grouped No Threshold\n",
    "        print(f\"  Eval: Grouped No Threshold...\")\n",
    "        res_no = evaluate_recall_grouped_threshold_batched(q_q, db_q, gt_top_k, retrieval_depths, k_true_neighbors, group_size=6, threshold=None)\n",
    "        results.append({'Method': name, 'Config': 'Grouped No Threshold', **res_no})\n",
    "        print(f\"  -> {res_no['recall@1000']:.4f}\")\n",
    "        \n",
    "        # 2. Grouped With Threshold\n",
    "        print(f\"  Eval: Grouped With Threshold (T={safe_threshold_est:.4f})...\")\n",
    "        res_th = evaluate_recall_grouped_threshold_batched(q_q, db_q, gt_top_k, retrieval_depths, k_true_neighbors, group_size=6, threshold=safe_threshold_est)\n",
    "        results.append({'Method': name, 'Config': 'Grouped + Threshold', **res_th})\n",
    "        print(f\"  -> {res_th['recall@1000']:.4f}\")\n",
    "\n",
    "    # Summary\n",
    "    df_res = pd.DataFrame(results)\n",
    "    csv_filename = \"result_threshold_experiment.csv\"\n",
    "    df_res.to_csv(csv_filename, index=False)\n",
    "    display(df_res)\n",
    "    plot_threshold_results(df_res)\n",
    "    \n",
    "if 'base_vectors' in locals() and 'query_vectors' in locals():\n",
    "    if 'gt_1m' not in locals():\n",
    "         gt_1m = calculate_top_k_ground_truth(query_vectors, base_vectors, k=100)\n",
    "    run_threshold_experiment(base_vectors, query_vectors, gt_1m, [100, 500, 1000], 100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
