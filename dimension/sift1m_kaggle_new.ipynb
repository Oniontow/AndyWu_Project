{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05eba1cf",
   "metadata": {},
   "source": [
    "## 2. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a51583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import umap\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "# 设置设备\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 参数设置\n",
    "SIFT_DIM = 128\n",
    "REDUCED_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d4f1a",
   "metadata": {},
   "source": [
    "## 3. 下载和解压 SIFT1M 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sift1m():\n",
    "    \"\"\"\n",
    "    下载SIFT1M数据集\n",
    "    \"\"\"\n",
    "    # 创建数据目录\n",
    "    os.makedirs('sift1m', exist_ok=True)\n",
    "    \n",
    "    files = [\n",
    "        \"sift_base.fvecs\",\n",
    "        \"sift_query.fvecs\",\n",
    "        \"sift_groundtruth.ivecs\"\n",
    "    ]\n",
    "    \n",
    "    # 检查文件是否已存在\n",
    "    all_exist = True\n",
    "    for filename in files:\n",
    "        if not os.path.exists(os.path.join('sift1m', filename)):\n",
    "            all_exist = False\n",
    "            break\n",
    "            \n",
    "    if all_exist:\n",
    "        print(\"所有文件已存在，跳过下载\")\n",
    "        return True\n",
    "        \n",
    "    print(\"正在下载 SIFT1M 数据集...\")\n",
    "    \n",
    "    # 尝试下载 tar.gz 文件\n",
    "    tar_url = \"ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz\"\n",
    "    tar_path = \"sift1m/sift.tar.gz\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"尝试从 {tar_url} 下载...\")\n",
    "        # 增加 timeout\n",
    "        import socket\n",
    "        socket.setdefaulttimeout(30)\n",
    "        urllib.request.urlretrieve(tar_url, tar_path)\n",
    "        print(\"下载完成，正在解压...\")\n",
    "        \n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=\"sift1m\")\n",
    "            \n",
    "        # 移动文件到 sift1m 根目录 (解压后会在 sift/ 目录下)\n",
    "        extracted_dir = os.path.join(\"sift1m\", \"sift\")\n",
    "        if os.path.exists(extracted_dir):\n",
    "            for filename in files:\n",
    "                src = os.path.join(extracted_dir, filename)\n",
    "                dst = os.path.join(\"sift1m\", filename)\n",
    "                if os.path.exists(src):\n",
    "                    if os.path.exists(dst):\n",
    "                        os.remove(dst)\n",
    "                    os.rename(src, dst)\n",
    "            # 清理\n",
    "            try:\n",
    "                import shutil\n",
    "                shutil.rmtree(extracted_dir)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # 删除 tar 文件\n",
    "        if os.path.exists(tar_path):\n",
    "            os.remove(tar_path)\n",
    "        print(\"数据集准备完成\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"FTP下载失败: {e}\")\n",
    "        print(\"尝试使用 wget 下载...\")\n",
    "        \n",
    "        try:\n",
    "            # 尝试使用 wget\n",
    "            res = os.system(f\"wget {tar_url} -O {tar_path}\")\n",
    "            if res == 0 and os.path.exists(tar_path):\n",
    "                print(\"wget 下载成功，正在解压...\")\n",
    "                with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "                    tar.extractall(path=\"sift1m\")\n",
    "                \n",
    "                # 移动文件\n",
    "                extracted_dir = os.path.join(\"sift1m\", \"sift\")\n",
    "                if os.path.exists(extracted_dir):\n",
    "                    for filename in files:\n",
    "                        src = os.path.join(extracted_dir, filename)\n",
    "                        dst = os.path.join(\"sift1m\", filename)\n",
    "                        if os.path.exists(src):\n",
    "                            if os.path.exists(dst):\n",
    "                                os.remove(dst)\n",
    "                            os.rename(src, dst)\n",
    "                    try:\n",
    "                        import shutil\n",
    "                        shutil.rmtree(extracted_dir)\n",
    "                    except:\n",
    "                        pass\n",
    "                if os.path.exists(tar_path):\n",
    "                    os.remove(tar_path)\n",
    "                return True\n",
    "        except Exception as e2:\n",
    "            print(f\"wget 失败: {e2}\")\n",
    "            \n",
    "        print(\"无法自动下载数据集。\")\n",
    "        print(\"请手动下载 sift.tar.gz 从 http://corpus-texmex.irisa.fr/\")\n",
    "        print(\"并解压到 sift1m/ 目录下。\")\n",
    "        return False\n",
    "\n",
    "# 下载数据集\n",
    "print(\"开始下载SIFT1M数据集...\")\n",
    "download_sift1m()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692adc9a",
   "metadata": {},
   "source": [
    "## 4. INT3 量化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_to_int3(data):\n",
    "    \"\"\"\n",
    "    將數據量化到INT3 (-4 到 3，共8個離散值)\n",
    "    改進：使用百分位數 (1% - 99%) 進行截斷，減少極端值對量化的影響\n",
    "    \"\"\"\n",
    "    is_torch = isinstance(data, torch.Tensor)\n",
    "    \n",
    "    if is_torch:\n",
    "        arr = data.cpu().numpy()\n",
    "    else:\n",
    "        arr = data\n",
    "    \n",
    "    # 使用百分位數來決定範圍，避免 outlier 影響\n",
    "    # SIFT 特徵通常會有少數極大值\n",
    "    min_val = np.percentile(arr, 1)\n",
    "    max_val = np.percentile(arr, 99)\n",
    "    \n",
    "    # 將數據縮放到 [-4, 3] 範圍\n",
    "    scaled = (arr - min_val) / (max_val - min_val + 1e-8) * 7 - 4\n",
    "    \n",
    "    # 四捨五入並裁剪到 INT3 範圍\n",
    "    quantized = np.clip(np.round(scaled), -4, 3).astype(np.int8)\n",
    "    \n",
    "    if is_torch:\n",
    "        return torch.from_numpy(quantized).to(data.device)\n",
    "    else:\n",
    "        return quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea22ff0",
   "metadata": {},
   "source": [
    "## 5. AutoEncoder 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=128, latent_dim=64):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder (加深並加入 BatchNorm)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 96),\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 96),\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "def train_autoencoder(data, input_dim=128, latent_dim=64, epochs=100, batch_size=256, patience=5, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    訓練自編碼器 (加入 Early Stopping)\n",
    "    \"\"\"\n",
    "    model = AutoEncoder(input_dim, latent_dim).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(data).to(DEVICE)\n",
    "    \n",
    "    # 創建 DataLoader 以便於 batch 處理\n",
    "    dataset = torch.utils.data.TensorDataset(data_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    pbar = tqdm(range(epochs), desc=\"训练AutoEncoder\")\n",
    "    for epoch in pbar:\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch_data = batch[0]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            _, reconstructed = model(batch_data)\n",
    "            loss = criterion(reconstructed, batch_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        pbar.set_postfix({'loss': f'{avg_loss:.6f}'})\n",
    "        \n",
    "        # Early Stopping Check\n",
    "        if avg_loss < best_loss - min_delta:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1} (Best Loss: {best_loss:.6f})\")\n",
    "            break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97cf329",
   "metadata": {},
   "source": [
    "## 6. 五种数据处理方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6cb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method1_direct_int3(data):\n",
    "    \"\"\"方法1: 直接將128維向量量化為INT3\"\"\"\n",
    "    return quantize_to_int3(data)\n",
    "\n",
    "def method2_average_pooling_int3(data):\n",
    "    \"\"\"方法2: 將相鄰兩維做平均，從128維降到64維，然後量化為INT3\"\"\"\n",
    "    reshaped = data.reshape(data.shape[0], 64, 2)\n",
    "    averaged = np.mean(reshaped, axis=2)\n",
    "    return quantize_to_int3(averaged)\n",
    "\n",
    "def method3_pca_int3(data, query_data=None):\n",
    "    \"\"\"方法3: 使用PCA降維到64維，然後量化為INT3\"\"\"\n",
    "    print(\"  - 训练PCA...\")\n",
    "    # 使用 whiten=True 來標準化分量，通常能改善後續量化效果\n",
    "    pca = PCA(n_components=REDUCED_DIM, whiten=True)\n",
    "    pca.fit(data)\n",
    "    \n",
    "    data_reduced = pca.transform(data)\n",
    "    data_quantized = quantize_to_int3(data_reduced)\n",
    "    \n",
    "    if query_data is not None:\n",
    "        query_reduced = pca.transform(query_data)\n",
    "        query_quantized = quantize_to_int3(query_reduced)\n",
    "        return pca, data_quantized, query_quantized\n",
    "    \n",
    "    return pca, data_quantized\n",
    "\n",
    "def method4_max_pooling_int3(data, query_data=None):\n",
    "    \"\"\"方法4: 將相鄰兩維做最大絕對值池化 (Max Magnitude Pooling)，從128維降到64維，然後量化為INT3\"\"\"\n",
    "    \n",
    "    def max_magnitude_pool(arr):\n",
    "        # Reshape to (N, 64, 2)\n",
    "        reshaped = arr.reshape(arr.shape[0], 64, 2)\n",
    "        a = reshaped[:, :, 0]\n",
    "        b = reshaped[:, :, 1]\n",
    "        # 比較絕對值大小\n",
    "        mask = np.abs(a) >= np.abs(b)\n",
    "        # 選擇絕對值較大的那個原始值\n",
    "        return np.where(mask, a, b)\n",
    "\n",
    "    max_pooled = max_magnitude_pool(data)\n",
    "    data_quantized = quantize_to_int3(max_pooled)\n",
    "    \n",
    "    if query_data is not None:\n",
    "        q_max_pooled = max_magnitude_pool(query_data)\n",
    "        query_quantized = quantize_to_int3(q_max_pooled)\n",
    "        return None, data_quantized, query_quantized\n",
    "    \n",
    "    return None, data_quantized\n",
    "\n",
    "def method5_autoencoder_int3(data, query_data=None, epochs=100):\n",
    "    \"\"\"方法5: 使用AutoEncoder降維到64維，然後量化為INT3\"\"\"\n",
    "    print(\"  - 训练AutoEncoder...\")\n",
    "    # 這裡 epochs 預設為 100，配合 Early Stopping\n",
    "    ae_model = train_autoencoder(data, SIFT_DIM, REDUCED_DIM, epochs=epochs)\n",
    "    ae_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        data_tensor = torch.FloatTensor(data).to(DEVICE)\n",
    "        data_reduced = ae_model.encode(data_tensor).cpu().numpy()\n",
    "        data_quantized = quantize_to_int3(data_reduced)\n",
    "        \n",
    "        if query_data is not None:\n",
    "            query_tensor = torch.FloatTensor(query_data).to(DEVICE)\n",
    "            query_reduced = ae_model.encode(query_tensor).cpu().numpy()\n",
    "            query_quantized = quantize_to_int3(query_reduced)\n",
    "            return ae_model, data_quantized, query_quantized\n",
    "    \n",
    "    return ae_model, data_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21951698",
   "metadata": {},
   "source": [
    "## 7. 距离计算和评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_l2_distances(queries, database):\n",
    "    \"\"\"計算L2距離\"\"\"\n",
    "    return pairwise_distances(queries, database, metric='euclidean')\n",
    "\n",
    "def evaluate_recall_at_k(distances, ground_truth, k_values=[1, 10, 100]):\n",
    "    \"\"\"計算Recall@K指標\"\"\"\n",
    "    sorted_indices = np.argsort(distances, axis=1)\n",
    "    \n",
    "    recalls = {}\n",
    "    for k in k_values:\n",
    "        top_k_predictions = sorted_indices[:, :k]\n",
    "        \n",
    "        query_recalls = []\n",
    "        for i in range(len(ground_truth)):\n",
    "            gt_i = ground_truth[i]\n",
    "            # Handle jagged arrays: check length\n",
    "            limit = min(k, len(gt_i))\n",
    "            \n",
    "            if limit == 0:\n",
    "                query_recalls.append(0.0)\n",
    "                continue\n",
    "                \n",
    "            true_neighbors = set(gt_i[:limit])\n",
    "            pred_neighbors = set(top_k_predictions[i])\n",
    "            intersection = true_neighbors & pred_neighbors\n",
    "            recall = len(intersection) / len(true_neighbors)\n",
    "            query_recalls.append(recall)\n",
    "        \n",
    "        recalls[f'recall@{k}'] = np.mean(query_recalls)\n",
    "    \n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a202e3",
   "metadata": {},
   "source": [
    "## 8. 加载 SIFT1M 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fvecs(filename):\n",
    "    \"\"\"讀取.fvecs格式文件\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        d = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "        f.seek(0)\n",
    "        # .fvecs 格式: 4 bytes int32 (dimension) + d * 4 bytes float32 (data)\n",
    "        # 读取为 float32，header 的 int32 会被读成一个 float32，reshape 后切片去掉即可\n",
    "        data = np.fromfile(f, dtype=np.float32)\n",
    "        data = data.reshape(-1, d + 1)\n",
    "        return data[:, 1:].copy()\n",
    "\n",
    "def read_ivecs(filename):\n",
    "    \"\"\"讀取.ivecs格式文件\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        d = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "        f.seek(0)\n",
    "        data = np.fromfile(f, dtype=np.int32)\n",
    "        data = data.reshape(-1, d + 1)\n",
    "        return data[:, 1:].copy()\n",
    "\n",
    "print(\"载入SIFT1M数据集...\")\n",
    "base_vectors = read_fvecs('sift1m/sift_base.fvecs')\n",
    "query_vectors = read_fvecs('sift1m/sift_query.fvecs')\n",
    "ground_truth = read_ivecs('sift1m/sift_groundtruth.ivecs')\n",
    "\n",
    "print(f\"Base vectors shape: {base_vectors.shape}\")\n",
    "print(f\"Query vectors shape: {query_vectors.shape}\")\n",
    "print(f\"Ground truth shape: {ground_truth.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70c0f9",
   "metadata": {},
   "source": [
    "## 9. 运行实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top_k_ground_truth(queries, database, k=100):\n",
    "    \"\"\"\n",
    "    計算每個查詢的前 K 個真實最近鄰居 (Ground Truth)\n",
    "    使用 GPU 加速計算，並分批處理 Query 以節省記憶體\n",
    "    \"\"\"\n",
    "    print(f\"正在計算 Top-{k} Ground Truth (Query: {len(queries)}, DB: {len(database)})...\")\n",
    "    \n",
    "    num_queries = len(queries)\n",
    "    query_batch_size = 100  # 每次處理 100 個 Query\n",
    "    \n",
    "    all_indices = []\n",
    "    \n",
    "    for i in tqdm(range(0, num_queries, query_batch_size), desc=\"Computing GT\"):\n",
    "        q_end = min(i + query_batch_size, num_queries)\n",
    "        q_batch = queries[i:q_end]\n",
    "        q_tensor = torch.FloatTensor(q_batch).to(DEVICE)\n",
    "        q_sq = torch.sum(q_tensor**2, dim=1, keepdim=True)\n",
    "        \n",
    "        dists_batch = []\n",
    "        \n",
    "        # 分批處理 DB\n",
    "        db_batch_size = 50000\n",
    "        for j in range(0, len(database), db_batch_size):\n",
    "            db_end = min(j + db_batch_size, len(database))\n",
    "            db_chunk = torch.FloatTensor(database[j:db_end]).to(DEVICE)\n",
    "            \n",
    "            db_sq_chunk = torch.sum(db_chunk**2, dim=1)\n",
    "            term2 = -2 * torch.matmul(q_tensor, db_chunk.t())\n",
    "            \n",
    "            dists_chunk = q_sq + db_sq_chunk + term2\n",
    "            dists_batch.append(dists_chunk.cpu())\n",
    "            \n",
    "            del db_chunk, db_sq_chunk, term2, dists_chunk\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        full_dists_batch = torch.cat(dists_batch, dim=1)\n",
    "        \n",
    "        # 取 Top K\n",
    "        _, indices = torch.topk(full_dists_batch, k=k, dim=1, largest=False)\n",
    "        all_indices.append(indices.numpy())\n",
    "        \n",
    "        del full_dists_batch, indices, q_tensor, q_sq\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return np.vstack(all_indices)\n",
    "\n",
    "def evaluate_recall_batched(query_vectors, db_vectors, gt_top_k, retrieval_depths, k_true_neighbors):\n",
    "    \"\"\"\n",
    "    分批計算距離並評估 Recall\n",
    "    Recall = (Retrieved & Top-K_GT) / K_GT\n",
    "    \"\"\"\n",
    "    max_depth = max(retrieval_depths)\n",
    "    num_queries = len(query_vectors)\n",
    "    query_batch_size = 100\n",
    "    \n",
    "    total_hits = {r: 0 for r in retrieval_depths}\n",
    "    \n",
    "    # Handle Tensor inputs\n",
    "    if isinstance(query_vectors, torch.Tensor):\n",
    "        query_vectors = query_vectors.cpu().numpy()\n",
    "    if isinstance(db_vectors, torch.Tensor):\n",
    "        db_vectors = db_vectors.cpu().numpy()\n",
    "    \n",
    "    if query_vectors.dtype != np.float32:\n",
    "        query_vectors = query_vectors.astype(np.float32)\n",
    "    if db_vectors.dtype != np.float32:\n",
    "        db_vectors = db_vectors.astype(np.float32)\n",
    "        \n",
    "    for i in tqdm(range(0, num_queries, query_batch_size), desc=\"Evaluating Batches\", leave=False):\n",
    "        q_end = min(i + query_batch_size, num_queries)\n",
    "        q_batch = query_vectors[i:q_end]\n",
    "        gt_batch = gt_top_k[i:q_end]\n",
    "        \n",
    "        dists = pairwise_distances(q_batch, db_vectors, metric='euclidean')\n",
    "        sorted_indices = np.argsort(dists, axis=1)[:, :max_depth]\n",
    "        \n",
    "        for r in retrieval_depths:\n",
    "            retrieved_indices = sorted_indices[:, :r]\n",
    "            for j in range(len(gt_batch)):\n",
    "                gt_set = set(gt_batch[j]) # 這是 Top K 真實鄰居\n",
    "                retrieved_set = set(retrieved_indices[j])\n",
    "                total_hits[r] += len(gt_set & retrieved_set)\n",
    "                \n",
    "    recalls = {}\n",
    "    for r in retrieval_depths:\n",
    "        # Normalize by k_true_neighbors (e.g., 100)\n",
    "        recalls[f'recall@{r}'] = total_hits[r] / (num_queries * float(k_true_neighbors))\n",
    "        \n",
    "    return recalls\n",
    "\n",
    "def method5_autoencoder_int3_with_timing(data, query_data=None, epochs=100):\n",
    "    \"\"\"\n",
    "    方法5: 使用AutoEncoder降維到64維，然後量化為INT3\n",
    "    返回額外的訓練時間和推理時間\n",
    "    \"\"\"\n",
    "    print(\"  - 训练AutoEncoder...\")\n",
    "    \n",
    "    # 訓練時間\n",
    "    train_start = time.time()\n",
    "    ae_model = train_autoencoder(data, SIFT_DIM, REDUCED_DIM, epochs=epochs)\n",
    "    train_time = time.time() - train_start\n",
    "    \n",
    "    ae_model.eval()\n",
    "    \n",
    "    # 推理時間\n",
    "    inference_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        data_tensor = torch.FloatTensor(data).to(DEVICE)\n",
    "        data_reduced = ae_model.encode(data_tensor).cpu().numpy()\n",
    "        data_quantized = quantize_to_int3(data_reduced)\n",
    "        \n",
    "        if query_data is not None:\n",
    "            query_tensor = torch.FloatTensor(query_data).to(DEVICE)\n",
    "            query_reduced = ae_model.encode(query_tensor).cpu().numpy()\n",
    "            query_quantized = quantize_to_int3(query_reduced)\n",
    "            inference_time = time.time() - inference_start\n",
    "            return ae_model, data_quantized, query_quantized, train_time, inference_time\n",
    "    \n",
    "    inference_time = time.time() - inference_start\n",
    "    return ae_model, data_quantized, train_time, inference_time\n",
    "\n",
    "def run_experiment_part(part_name, base_vectors, query_vectors, gt_top_k, retrieval_depths, k_true_neighbors):\n",
    "    \"\"\"\n",
    "    執行單個部分的實驗\n",
    "    返回 recalls DataFrame 和 times DataFrame (不含 AE 訓練時間)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"實驗部分: {part_name}\")\n",
    "    print(f\"Database Size: {len(base_vectors)}\")\n",
    "    print(f\"Retrieval Depths: {retrieval_depths}\")\n",
    "    print(f\"Target GT Size (K): {k_true_neighbors}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    time_results = []  # 存儲時間結果 (不含 AE 訓練時間)\n",
    "\n",
    "    # 方法1\n",
    "    print(\"\\n[方法1] 直接INT3量化 (128維)...\")\n",
    "    t0 = time.time()\n",
    "    db_m1 = method1_direct_int3(base_vectors)\n",
    "    q_m1 = method1_direct_int3(query_vectors)\n",
    "    process_time_m1 = time.time() - t0\n",
    "    \n",
    "    t0 = time.time()\n",
    "    recalls_m1 = evaluate_recall_batched(q_m1, db_m1, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    eval_time_m1 = time.time() - t0\n",
    "    total_time_m1 = process_time_m1 + eval_time_m1\n",
    "    \n",
    "    results.append({'method': 'Method 1: Direct INT3', 'time': total_time_m1, **recalls_m1})\n",
    "    time_results.append({'method': 'M1: Direct INT3', 'process_time': process_time_m1, 'eval_time': eval_time_m1, 'total_time': total_time_m1})\n",
    "    print(recalls_m1)\n",
    "    \n",
    "    # 方法2\n",
    "    print(\"\\n[方法2] 平均池化降維 + INT3 (64維)...\")\n",
    "    t0 = time.time()\n",
    "    db_m2 = method2_average_pooling_int3(base_vectors)\n",
    "    q_m2 = method2_average_pooling_int3(query_vectors)\n",
    "    process_time_m2 = time.time() - t0\n",
    "    \n",
    "    t0 = time.time()\n",
    "    recalls_m2 = evaluate_recall_batched(q_m2, db_m2, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    eval_time_m2 = time.time() - t0\n",
    "    total_time_m2 = process_time_m2 + eval_time_m2\n",
    "    \n",
    "    results.append({'method': 'Method 2: AvgPooling + INT3', 'time': total_time_m2, **recalls_m2})\n",
    "    time_results.append({'method': 'M2: AvgPooling', 'process_time': process_time_m2, 'eval_time': eval_time_m2, 'total_time': total_time_m2})\n",
    "    print(recalls_m2)\n",
    "    \n",
    "    # 方法3\n",
    "    print(\"\\n[方法3] PCA降維 + INT3 (64維)...\")\n",
    "    t0 = time.time()\n",
    "    _, db_m3, q_m3 = method3_pca_int3(base_vectors, query_vectors)\n",
    "    process_time_m3 = time.time() - t0\n",
    "    \n",
    "    t0 = time.time()\n",
    "    recalls_m3 = evaluate_recall_batched(q_m3, db_m3, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    eval_time_m3 = time.time() - t0\n",
    "    total_time_m3 = process_time_m3 + eval_time_m3\n",
    "    \n",
    "    results.append({'method': 'Method 3: PCA + INT3', 'time': total_time_m3, **recalls_m3})\n",
    "    time_results.append({'method': 'M3: PCA', 'process_time': process_time_m3, 'eval_time': eval_time_m3, 'total_time': total_time_m3})\n",
    "    print(recalls_m3)\n",
    "    \n",
    "    # 方法4\n",
    "    print(f\"\\n[方法4] Max Magnitude Pooling降維 + INT3 (64維)...\")\n",
    "    t0 = time.time()\n",
    "    _, db_m4, q_m4 = method4_max_pooling_int3(base_vectors, query_vectors)\n",
    "    process_time_m4 = time.time() - t0\n",
    "    \n",
    "    t0 = time.time()\n",
    "    recalls_m4 = evaluate_recall_batched(q_m4, db_m4, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    eval_time_m4 = time.time() - t0\n",
    "    total_time_m4 = process_time_m4 + eval_time_m4\n",
    "    \n",
    "    results.append({'method': 'Method 4: Max Mag Pooling + INT3', 'time': total_time_m4, **recalls_m4})\n",
    "    time_results.append({'method': 'M4: MaxMagPool', 'process_time': process_time_m4, 'eval_time': eval_time_m4, 'total_time': total_time_m4})\n",
    "    print(recalls_m4)\n",
    "    \n",
    "    # 方法5 - 使用新的 timing 版本\n",
    "    print(\"\\n[方法5] AutoEncoder降維 + INT3 (64維)...\")\n",
    "    _, db_m5, q_m5, ae_train_time, ae_inference_time = method5_autoencoder_int3_with_timing(base_vectors, query_vectors, epochs=100)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    recalls_m5 = evaluate_recall_batched(q_m5, db_m5, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    eval_time_m5 = time.time() - t0\n",
    "    \n",
    "    # 總時間不含訓練時間\n",
    "    total_time_m5_no_train = ae_inference_time + eval_time_m5\n",
    "    total_time_m5_with_train = ae_train_time + ae_inference_time + eval_time_m5\n",
    "    \n",
    "    results.append({'method': 'Method 5: AutoEncoder + INT3', 'time': total_time_m5_with_train, \n",
    "                   'time_no_train': total_time_m5_no_train, 'train_time': ae_train_time, **recalls_m5})\n",
    "    time_results.append({'method': 'M5: AutoEncoder', 'process_time': ae_inference_time, 'eval_time': eval_time_m5, \n",
    "                        'total_time': total_time_m5_no_train, 'train_time': ae_train_time})\n",
    "    print(recalls_m5)\n",
    "    print(f\"  AE 訓練時間: {ae_train_time:.2f}s, 推理時間: {ae_inference_time:.2f}s\")\n",
    "    \n",
    "    return pd.DataFrame(results), pd.DataFrame(time_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b596229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行實驗\n",
    "# 實驗配置：\n",
    "# Part 1: 100K DB, Recall 100@1000, 100@5000, 100@10000\n",
    "# Part 2: 1M DB, Recall 100@1000, 100@5000, 100@10000\n",
    "\n",
    "all_experiment_results = []  # 存儲 recall 結果\n",
    "all_time_results = []  # 存儲時間結果\n",
    "\n",
    "# --- Part 1: 100K Database ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Part 1: First 100K Database\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "db_100k = base_vectors[:100000].astype(np.float32)\n",
    "query_subset = query_vectors[:1000].astype(np.float32)\n",
    "\n",
    "# 計算 Ground Truth for 100K DB\n",
    "print(\"Computing Ground Truth for 100K DB...\")\n",
    "gt_100k = calculate_top_k_ground_truth(query_subset, db_100k, k=100)\n",
    "\n",
    "# 執行實驗 Part 1\n",
    "retrieval_depths_1 = [1000, 5000, 10000]\n",
    "k_true_neighbors = 100\n",
    "\n",
    "results_df_1, time_df_1 = run_experiment_part(\n",
    "    \"Part 1: 100K DB\",\n",
    "    db_100k,\n",
    "    query_subset,\n",
    "    gt_100k,\n",
    "    retrieval_depths_1,\n",
    "    k_true_neighbors\n",
    ")\n",
    "\n",
    "all_experiment_results.append(results_df_1)\n",
    "all_time_results.append(time_df_1)\n",
    "\n",
    "print(\"\\nPart 1 Recall Results:\")\n",
    "display(results_df_1)\n",
    "print(\"\\nPart 1 Time Results (不含 AE 訓練時間):\")\n",
    "display(time_df_1)\n",
    "\n",
    "# --- Part 2: 1M Database ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Part 2: Full 1M Database\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "db_1m = base_vectors.astype(np.float32)\n",
    "\n",
    "# 計算 Ground Truth for 1M DB\n",
    "print(\"Computing Ground Truth for 1M DB...\")\n",
    "gt_1m = calculate_top_k_ground_truth(query_subset, db_1m, k=100)\n",
    "\n",
    "# 執行實驗 Part 2\n",
    "retrieval_depths_2 = [1000, 5000, 10000]\n",
    "\n",
    "results_df_2, time_df_2 = run_experiment_part(\n",
    "    \"Part 2: 1M DB\",\n",
    "    db_1m,\n",
    "    query_subset,\n",
    "    gt_1m,\n",
    "    retrieval_depths_2,\n",
    "    k_true_neighbors\n",
    ")\n",
    "\n",
    "all_experiment_results.append(results_df_2)\n",
    "all_time_results.append(time_df_2)\n",
    "\n",
    "print(\"\\nPart 2 Recall Results:\")\n",
    "display(results_df_2)\n",
    "print(\"\\nPart 2 Time Results (不含 AE 訓練時間):\")\n",
    "display(time_df_2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"所有實驗完成！\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4ac29",
   "metadata": {},
   "source": [
    "## 10. 可视化结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a08e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化結果\n",
    "# Part 1: 100K DB, Part 2: 1M DB\n",
    "# Recall 100 @1000, @5000, @10000\n",
    "\n",
    "# --- 圖一：Recall 作圖 ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "fig.suptitle('Recall of Top-100 True Neighbors @ Different Retrieval Depths', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "# Part 1: 100K DB\n",
    "ax1 = axes[0]\n",
    "df1 = all_experiment_results[0]\n",
    "methods = df1['method'].str.replace('Method ', 'M').str.replace(': ', '\\n', 1)\n",
    "x = np.arange(len(df1))\n",
    "width = 0.25\n",
    "\n",
    "bars1_1 = ax1.bar(x - width, df1['recall@1000'], width, label='@1000', color=colors[0])\n",
    "bars1_2 = ax1.bar(x, df1['recall@5000'], width, label='@5000', color=colors[1])\n",
    "bars1_3 = ax1.bar(x + width, df1['recall@10000'], width, label='@10000', color=colors[2])\n",
    "\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
    "ax1.set_ylabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Part 1: 100K DB (100@1000, 100@5000, 100@10000)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim([0, 1.05])\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Part 2: 1M DB\n",
    "ax2 = axes[1]\n",
    "df2 = all_experiment_results[1]\n",
    "\n",
    "bars2_1 = ax2.bar(x - width, df2['recall@1000'], width, label='@1000', color=colors[0])\n",
    "bars2_2 = ax2.bar(x, df2['recall@5000'], width, label='@5000', color=colors[1])\n",
    "bars2_3 = ax2.bar(x + width, df2['recall@10000'], width, label='@10000', color=colors[2])\n",
    "\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
    "ax2.set_ylabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Part 2: 1M DB (100@1000, 100@5000, 100@10000)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim([0, 1.05])\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sift1m_2parts_recall_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- 圖二：時間作圖 (不含 AE 訓練時間) ---\n",
    "fig2, axes2 = plt.subplots(1, 2, figsize=(16, 7))\n",
    "fig2.suptitle('Processing Time Comparison (Excluding AE Training Time)', fontsize=16, fontweight='bold')\n",
    "\n",
    "time_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "# Part 1: 100K DB 時間\n",
    "ax3 = axes2[0]\n",
    "time_df_1 = all_time_results[0]\n",
    "time_methods = time_df_1['method']\n",
    "bars3 = ax3.bar(range(len(time_df_1)), time_df_1['total_time'], color=time_colors)\n",
    "ax3.set_xticks(range(len(time_df_1)))\n",
    "ax3.set_xticklabels(time_methods, rotation=45, ha='right', fontsize=9)\n",
    "ax3.set_ylabel('Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Part 1: 100K DB - Processing Time', fontsize=14, fontweight='bold')\n",
    "ax3.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# 在柱狀圖上顯示數值\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5, f'{height:.2f}s', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Part 2: 1M DB 時間\n",
    "ax4 = axes2[1]\n",
    "time_df_2 = all_time_results[1]\n",
    "bars4 = ax4.bar(range(len(time_df_2)), time_df_2['total_time'], color=time_colors)\n",
    "ax4.set_xticks(range(len(time_df_2)))\n",
    "ax4.set_xticklabels(time_methods, rotation=45, ha='right', fontsize=9)\n",
    "ax4.set_ylabel('Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Part 2: 1M DB - Processing Time', fontsize=14, fontweight='bold')\n",
    "ax4.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# 在柱狀圖上顯示數值\n",
    "for bar in bars4:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5, f'{height:.2f}s', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sift1m_2parts_time_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- 圖三：詳細時間分解（包含 process_time 和 eval_time） ---\n",
    "fig3, axes3 = plt.subplots(1, 2, figsize=(16, 7))\n",
    "fig3.suptitle('Detailed Time Breakdown (Excluding AE Training Time)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Part 1: 100K DB 詳細時間\n",
    "ax5 = axes3[0]\n",
    "x = np.arange(len(time_df_1))\n",
    "width = 0.35\n",
    "\n",
    "bars5_1 = ax5.bar(x - width/2, time_df_1['process_time'], width, label='Processing', color='#2ca02c')\n",
    "bars5_2 = ax5.bar(x + width/2, time_df_1['eval_time'], width, label='Evaluation', color='#d62728')\n",
    "\n",
    "ax5.set_xticks(x)\n",
    "ax5.set_xticklabels(time_methods, rotation=45, ha='right', fontsize=9)\n",
    "ax5.set_ylabel('Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax5.set_title('Part 1: 100K DB - Time Breakdown', fontsize=14, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Part 2: 1M DB 詳細時間\n",
    "ax6 = axes3[1]\n",
    "\n",
    "bars6_1 = ax6.bar(x - width/2, time_df_2['process_time'], width, label='Processing', color='#2ca02c')\n",
    "bars6_2 = ax6.bar(x + width/2, time_df_2['eval_time'], width, label='Evaluation', color='#d62728')\n",
    "\n",
    "ax6.set_xticks(x)\n",
    "ax6.set_xticklabels(time_methods, rotation=45, ha='right', fontsize=9)\n",
    "ax6.set_ylabel('Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax6.set_title('Part 2: 1M DB - Time Breakdown', fontsize=14, fontweight='bold')\n",
    "ax6.legend()\n",
    "ax6.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sift1m_2parts_time_breakdown.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n圖表已儲存！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44c37c",
   "metadata": {},
   "source": [
    "## 11. 实验总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"實驗總結\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config, results_df in zip(EXPERIMENTS, all_results):\n",
    "    print(f\"\\n配置: {config['query_size']}@{config['database_size']}\")\n",
    "    display(results_df)\n",
    "\n",
    "print(\"\\n所有實驗完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Dimension Reduction Analysis (128 -> 16)\n",
    "# Analyzing Recall Rate vs Dimension for AvgPooling and PCA\n",
    "# Configuration: 1M Database, Recall of Top-1000 GT @ Depth 10000\n",
    "\n",
    "def adaptive_avg_pool_int3(data, target_dim):\n",
    "    \"\"\"\n",
    "    Adaptive Average Pooling to reduce to specific target dimension\n",
    "    \"\"\"\n",
    "    if isinstance(data, np.ndarray):\n",
    "        t = torch.from_numpy(data).float()\n",
    "    else:\n",
    "        t = data.float()\n",
    "    \n",
    "    t = t.to(DEVICE)\n",
    "    # Input: (N, 128) -> (N, 1, 128) for pooling\n",
    "    t = t.unsqueeze(1)\n",
    "    # Adaptive Pool to (N, 1, target_dim)\n",
    "    pooled = nn.functional.adaptive_avg_pool1d(t, target_dim)\n",
    "    # (N, target_dim)\n",
    "    pooled = pooled.squeeze(1)\n",
    "    \n",
    "    return quantize_to_int3(pooled)\n",
    "\n",
    "def run_dimension_sweep(base_vectors, query_vectors, gt_top_k, k_true_neighbors=1000, retrieval_depth=10000):\n",
    "    # Dimensions: 128, 112, 96, ..., 16\n",
    "    target_dims = list(range(128, 15, -16)) \n",
    "    results = []\n",
    "    \n",
    "    print(f\"Starting Dimension Sweep: {target_dims}\")\n",
    "    print(f\"Database Size: {len(base_vectors)}\")\n",
    "    print(f\"Target GT Size: {k_true_neighbors}\")\n",
    "    print(f\"Retrieval Depth: {retrieval_depth}\")\n",
    "    \n",
    "    for dim in target_dims:\n",
    "        print(f\"\\nTesting Target Dimension: {dim}\")\n",
    "        \n",
    "        # --- Method: Adaptive Avg Pooling ---\n",
    "        print(\"  Running Avg Pooling...\")\n",
    "        t0 = time.time()\n",
    "        db_avg = adaptive_avg_pool_int3(base_vectors, dim)\n",
    "        q_avg = adaptive_avg_pool_int3(query_vectors, dim)\n",
    "        \n",
    "        # Evaluate Recall\n",
    "        recalls_avg = evaluate_recall_batched(q_avg, db_avg, gt_top_k, [retrieval_depth], k_true_neighbors)\n",
    "        time_avg = time.time() - t0\n",
    "        \n",
    "        metric_key = f'recall@{retrieval_depth}'\n",
    "        results.append({\n",
    "            'Dimension': dim,\n",
    "            'Method': 'AvgPooling',\n",
    "            'Recall': recalls_avg[metric_key],\n",
    "            'Time': time_avg\n",
    "        })\n",
    "        print(f\"    AvgPool {metric_key}: {recalls_avg[metric_key]:.4f}\")\n",
    "        \n",
    "        # --- Method: PCA ---\n",
    "        print(\"  Running PCA...\")\n",
    "        t0 = time.time()\n",
    "        # Fit PCA on a subset for speed (max 50k)\n",
    "        fit_size = min(len(base_vectors), 50000)\n",
    "        pca = PCA(n_components=dim, whiten=True)\n",
    "        pca.fit(base_vectors[:fit_size])\n",
    "        \n",
    "        db_pca = pca.transform(base_vectors)\n",
    "        q_pca = pca.transform(query_vectors)\n",
    "        \n",
    "        db_pca_q = quantize_to_int3(db_pca)\n",
    "        q_pca_q = quantize_to_int3(q_pca)\n",
    "        \n",
    "        recalls_pca = evaluate_recall_batched(q_pca_q, db_pca_q, gt_top_k, [retrieval_depth], k_true_neighbors)\n",
    "        time_pca = time.time() - t0\n",
    "        \n",
    "        results.append({\n",
    "            'Dimension': dim,\n",
    "            'Method': 'PCA',\n",
    "            'Recall': recalls_pca[metric_key],\n",
    "            'Time': time_pca\n",
    "        })\n",
    "        print(f\"    PCA {metric_key}: {recalls_pca[metric_key]:.4f}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Setup Data for Sweep (Using 1M Full DB)\n",
    "# Ensure we have the data\n",
    "if 'base_vectors_full' not in locals():\n",
    "    base_vectors_full = base_vectors.astype(np.float32)\n",
    "    query_vectors_subset = query_vectors[:1000].astype(np.float32)\n",
    "\n",
    "# Use Full 1M Database\n",
    "db_subset_sweep = base_vectors_full \n",
    "k_gt = 1000\n",
    "depth = 10000\n",
    "\n",
    "print(f\"Computing GT for sweep (1M DB, Top-{k_gt})...\")\n",
    "# Always recalculate to be safe or check if existing gt matches requirements\n",
    "# Since previous cells might have calculated k=100, we likely need to recalc for k=1000\n",
    "gt_sweep = calculate_top_k_ground_truth(query_vectors_subset, db_subset_sweep, k=k_gt)\n",
    "\n",
    "# Run Sweep\n",
    "sweep_df = run_dimension_sweep(db_subset_sweep, query_vectors_subset, gt_sweep, k_true_neighbors=k_gt, retrieval_depth=depth)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "avg_df = sweep_df[sweep_df['Method'] == 'AvgPooling']\n",
    "pca_df = sweep_df[sweep_df['Method'] == 'PCA']\n",
    "\n",
    "plt.plot(avg_df['Dimension'], avg_df['Recall'], marker='o', linewidth=2, label='Avg Pooling + INT3')\n",
    "plt.plot(pca_df['Dimension'], pca_df['Recall'], marker='s', linewidth=2, label='PCA + INT3')\n",
    "\n",
    "plt.xlabel('Dimension', fontsize=12)\n",
    "plt.ylabel(f'Recall@{depth} (Top-{k_gt} GT)', fontsize=12)\n",
    "plt.title(f'Recall vs Dimension (1M DB, {k_gt}@{depth})', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.gca().invert_xaxis() # Display 128 on left, 16 on right\n",
    "plt.xticks(list(range(128, 15, -16)))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save Results\n",
    "csv_filename = 'sift1m_1M_1000at10000_sweep_results.csv'\n",
    "sweep_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Sweep results saved to {csv_filename}\")\n",
    "sweep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fe3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. PCA Dimension Difference Analysis\n",
    "# Analyze absolute difference between Query and Top-100 NN in PCA space per dimension\n",
    "\n",
    "def analyze_pca_diff(base_vectors, query_vectors, gt_indices, n_components=128):\n",
    "    print(\"Fitting PCA...\")\n",
    "    # Fit PCA on a subset for speed\n",
    "    fit_size = min(len(base_vectors), 50000)\n",
    "    pca = PCA(n_components=n_components, whiten=True)\n",
    "    pca.fit(base_vectors[:fit_size])\n",
    "    \n",
    "    print(\"Transforming data...\")\n",
    "    # Transform all needed data\n",
    "    # We only need the specific base vectors that are in the GT of the queries\n",
    "    # But transforming all might be easier if memory allows (1M * 128 * 4 bytes ~ 512MB)\n",
    "    q_pca = pca.transform(query_vectors)\n",
    "    db_pca = pca.transform(base_vectors)\n",
    "    \n",
    "    n_queries = len(query_vectors)\n",
    "    k_neighbors = gt_indices.shape[1] # Should be 100\n",
    "    n_dims = n_components\n",
    "    \n",
    "    # Array to store sum of absolute differences per dimension\n",
    "    total_abs_diff = np.zeros(n_dims)\n",
    "    count = 0\n",
    "    \n",
    "    print(\"Calculating differences...\")\n",
    "    for i in tqdm(range(n_queries), desc=\"Analyzing Queries\"):\n",
    "        q_vec = q_pca[i] # (128,)\n",
    "        neighbor_indices = gt_indices[i] # (100,)\n",
    "        \n",
    "        # Get neighbor vectors\n",
    "        neighbor_vecs = db_pca[neighbor_indices] # (100, 128)\n",
    "        \n",
    "        # Calculate absolute difference\n",
    "        # |q - n|\n",
    "        abs_diff = np.abs(neighbor_vecs - q_vec) # (100, 128)\n",
    "        \n",
    "        # Sum over neighbors\n",
    "        total_abs_diff += np.sum(abs_diff, axis=0)\n",
    "        count += k_neighbors\n",
    "        \n",
    "    # Mean absolute difference per dimension\n",
    "    mean_abs_diff = total_abs_diff / count\n",
    "    \n",
    "    return mean_abs_diff, pca.explained_variance_ratio_\n",
    "\n",
    "# Setup Data\n",
    "if 'base_vectors_full' not in locals():\n",
    "    base_vectors_full = base_vectors.astype(np.float32)\n",
    "    query_vectors_subset = query_vectors[:1000].astype(np.float32)\n",
    "\n",
    "# Ensure GT for Top-100 exists\n",
    "k_target = 100\n",
    "if 'gt_sweep' in locals() and gt_sweep.shape[1] >= k_target:\n",
    "    gt_100 = gt_sweep[:, :k_target]\n",
    "else:\n",
    "    print(\"Computing GT for Top-100...\")\n",
    "    gt_100 = calculate_top_k_ground_truth(query_vectors_subset, base_vectors_full, k=k_target)\n",
    "\n",
    "# Run Analysis\n",
    "mean_diffs, explained_var = analyze_pca_diff(base_vectors_full, query_vectors_subset, gt_100)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Mean Absolute Difference\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, 129), mean_diffs, color='skyblue')\n",
    "plt.xlabel('PCA Component (Dimension)')\n",
    "plt.ylabel('Mean Absolute Difference')\n",
    "plt.title('Mean Abs Diff between Query and Top-100 NN per PCA Dimension')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot 2: Explained Variance (for context)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 129), explained_var, marker='.', linestyle='-', color='orange')\n",
    "plt.xlabel('PCA Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Explained Variance Ratio')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save to CSV\n",
    "diff_df = pd.DataFrame({\n",
    "    'Dimension': range(1, 129),\n",
    "    'Mean_Abs_Diff': mean_diffs,\n",
    "    'Explained_Variance': explained_var\n",
    "})\n",
    "diff_df.to_csv('sift1m_pca_128_diff_analysis.csv', index=False)\n",
    "print(\"Analysis saved to sift1m_pca_128_diff_analysis.csv\")\n",
    "diff_df.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
