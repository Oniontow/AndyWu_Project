{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05eba1cf",
   "metadata": {},
   "source": [
    "## 2. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a51583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import umap\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "# 设置设备\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 参数设置\n",
    "SIFT_DIM = 128\n",
    "REDUCED_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d4f1a",
   "metadata": {},
   "source": [
    "## 3. 下载和解压 SIFT1M 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sift1m():\n",
    "    \"\"\"\n",
    "    下载SIFT1M数据集\n",
    "    \"\"\"\n",
    "    # 创建数据目录\n",
    "    os.makedirs('sift1m', exist_ok=True)\n",
    "    \n",
    "    files = [\n",
    "        \"sift_base.fvecs\",\n",
    "        \"sift_query.fvecs\",\n",
    "        \"sift_groundtruth.ivecs\"\n",
    "    ]\n",
    "    \n",
    "    # 检查文件是否已存在\n",
    "    all_exist = True\n",
    "    for filename in files:\n",
    "        if not os.path.exists(os.path.join('sift1m', filename)):\n",
    "            all_exist = False\n",
    "            break\n",
    "            \n",
    "    if all_exist:\n",
    "        print(\"所有文件已存在，跳过下载\")\n",
    "        return True\n",
    "        \n",
    "    print(\"正在下载 SIFT1M 数据集...\")\n",
    "    \n",
    "    # 尝试下载 tar.gz 文件\n",
    "    tar_url = \"ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz\"\n",
    "    tar_path = \"sift1m/sift.tar.gz\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"尝试从 {tar_url} 下载...\")\n",
    "        # 增加 timeout\n",
    "        import socket\n",
    "        socket.setdefaulttimeout(30)\n",
    "        urllib.request.urlretrieve(tar_url, tar_path)\n",
    "        print(\"下载完成，正在解压...\")\n",
    "        \n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=\"sift1m\")\n",
    "            \n",
    "        # 移动文件到 sift1m 根目录 (解压后会在 sift/ 目录下)\n",
    "        extracted_dir = os.path.join(\"sift1m\", \"sift\")\n",
    "        if os.path.exists(extracted_dir):\n",
    "            for filename in files:\n",
    "                src = os.path.join(extracted_dir, filename)\n",
    "                dst = os.path.join(\"sift1m\", filename)\n",
    "                if os.path.exists(src):\n",
    "                    if os.path.exists(dst):\n",
    "                        os.remove(dst)\n",
    "                    os.rename(src, dst)\n",
    "            # 清理\n",
    "            try:\n",
    "                import shutil\n",
    "                shutil.rmtree(extracted_dir)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # 删除 tar 文件\n",
    "        if os.path.exists(tar_path):\n",
    "            os.remove(tar_path)\n",
    "        print(\"数据集准备完成\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"FTP下载失败: {e}\")\n",
    "        print(\"尝试使用 wget 下载...\")\n",
    "        \n",
    "        try:\n",
    "            # 尝试使用 wget\n",
    "            res = os.system(f\"wget {tar_url} -O {tar_path}\")\n",
    "            if res == 0 and os.path.exists(tar_path):\n",
    "                print(\"wget 下载成功，正在解压...\")\n",
    "                with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "                    tar.extractall(path=\"sift1m\")\n",
    "                \n",
    "                # 移动文件\n",
    "                extracted_dir = os.path.join(\"sift1m\", \"sift\")\n",
    "                if os.path.exists(extracted_dir):\n",
    "                    for filename in files:\n",
    "                        src = os.path.join(extracted_dir, filename)\n",
    "                        dst = os.path.join(\"sift1m\", filename)\n",
    "                        if os.path.exists(src):\n",
    "                            if os.path.exists(dst):\n",
    "                                os.remove(dst)\n",
    "                            os.rename(src, dst)\n",
    "                    try:\n",
    "                        import shutil\n",
    "                        shutil.rmtree(extracted_dir)\n",
    "                    except:\n",
    "                        pass\n",
    "                if os.path.exists(tar_path):\n",
    "                    os.remove(tar_path)\n",
    "                return True\n",
    "        except Exception as e2:\n",
    "            print(f\"wget 失败: {e2}\")\n",
    "            \n",
    "        print(\"无法自动下载数据集。\")\n",
    "        print(\"请手动下载 sift.tar.gz 从 http://corpus-texmex.irisa.fr/\")\n",
    "        print(\"并解压到 sift1m/ 目录下。\")\n",
    "        return False\n",
    "\n",
    "# 下载数据集\n",
    "print(\"开始下载SIFT1M数据集...\")\n",
    "download_sift1m()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692adc9a",
   "metadata": {},
   "source": [
    "## 4. INT3 量化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_to_int3(data):\n",
    "    \"\"\"\n",
    "    將數據量化到INT3 (-4 到 3，共8個離散值)\n",
    "    改進：使用百分位數 (1% - 99%) 進行截斷，減少極端值對量化的影響\n",
    "    支援 PyTorch GPU 運算\n",
    "    \"\"\"\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        # PyTorch implementation\n",
    "        # Ensure data is on the correct device\n",
    "        if data.device != DEVICE:\n",
    "            data = data.to(DEVICE)\n",
    "            \n",
    "        arr = data.float() # Ensure float for quantile\n",
    "        \n",
    "        # Calculate percentiles on GPU\n",
    "        # Note: quantile requires float\n",
    "        # Optimization: For large tensors, estimate quantiles from a subset\n",
    "        # torch.quantile can fail on very large tensors (RuntimeError: quantile() input tensor is too large)\n",
    "        if arr.numel() > 100000:\n",
    "            # Sample ~100k elements uniformly\n",
    "            step = max(1, arr.numel() // 100000)\n",
    "            # Use view(-1) to flatten and slice\n",
    "            sample = arr.view(-1)[::step]\n",
    "            min_val = torch.quantile(sample, 0.01)\n",
    "            max_val = torch.quantile(sample, 0.99)\n",
    "        else:\n",
    "            min_val = torch.quantile(arr, 0.01)\n",
    "            max_val = torch.quantile(arr, 0.99)\n",
    "        \n",
    "        # Scale to [-4, 3]\n",
    "        scaled = (arr - min_val) / (max_val - min_val + 1e-8) * 7 - 4\n",
    "        \n",
    "        # Round and clip\n",
    "        quantized = torch.clamp(torch.round(scaled), -4, 3).to(torch.int8)\n",
    "        return quantized\n",
    "    else:\n",
    "        # Numpy implementation\n",
    "        arr = data\n",
    "        # Similar optimization for numpy to speed up\n",
    "        if arr.size > 100000:\n",
    "             step = max(1, arr.size // 100000)\n",
    "             sample = arr.ravel()[::step]\n",
    "             min_val = np.percentile(sample, 1)\n",
    "             max_val = np.percentile(sample, 99)\n",
    "        else:\n",
    "             min_val = np.percentile(arr, 1)\n",
    "             max_val = np.percentile(arr, 99)\n",
    "             \n",
    "        scaled = (arr - min_val) / (max_val - min_val + 1e-8) * 7 - 4\n",
    "        quantized = np.clip(np.round(scaled), -4, 3).astype(np.int8)\n",
    "        return quantized\n",
    "\n",
    "class PerDimensionQuantileQuantizer:\n",
    "    \"\"\"\n",
    "    Quantize to INT3 using per-dimension quantiles.\n",
    "    For each dimension, it calculates 7 thresholds (1/8, 2/8, ..., 7/8 quantiles)\n",
    "    from the training data, and maps values to bins -4 to 3.\n",
    "    This ensures that for each dimension, the distribution of quantized values\n",
    "    is approximately uniform across the 8 bins.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.thresholds = None\n",
    "        self.device = DEVICE\n",
    "\n",
    "    def fit(self, data):\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = torch.from_numpy(data)\n",
    "        if data.device != self.device:\n",
    "            data = data.to(self.device)\n",
    "        \n",
    "        data = data.float()\n",
    "        N, D = data.shape\n",
    "        \n",
    "        # Calculate quantiles for each dimension\n",
    "        # quantiles: (7, D)\n",
    "        q_vals = torch.tensor([0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875], device=self.device)\n",
    "        \n",
    "        # Use torch.quantile with dim=0 (requires reasonably new pytorch)\n",
    "        # If data is very large, we might want to sample, but for 1M x 128 it fits in GPU memory usually.\n",
    "        # 1M * 128 * 4 bytes = 512MB.\n",
    "        try:\n",
    "            self.thresholds = torch.quantile(data, q_vals, dim=0) # (7, D)\n",
    "        except RuntimeError:\n",
    "            # Fallback if memory issue or old pytorch\n",
    "            # Process per dimension or sample\n",
    "            if N > 100000:\n",
    "                step = N // 100000\n",
    "                sample = data[::step]\n",
    "                self.thresholds = torch.quantile(sample, q_vals, dim=0)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.thresholds is None:\n",
    "            raise ValueError(\"Quantizer not fitted\")\n",
    "            \n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = torch.from_numpy(data)\n",
    "        if data.device != self.device:\n",
    "            data = data.to(self.device)\n",
    "            \n",
    "        data = data.float()\n",
    "        \n",
    "        # Broadcast comparison\n",
    "        # data: (N, D) -> (N, 1, D)\n",
    "        # thresholds: (7, D) -> (1, 7, D)\n",
    "        # comparison: (N, 7, D)\n",
    "        \n",
    "        # We want to count how many thresholds are smaller than data\n",
    "        # If x < t1 (smallest), count is 0 -> map to -4\n",
    "        # If x >= t7 (largest), count is 7 -> map to 3\n",
    "        \n",
    "        comparison = data.unsqueeze(1) >= self.thresholds.unsqueeze(0)\n",
    "        rank = torch.sum(comparison, dim=1).to(torch.int8) # (N, D), values 0-7\n",
    "        \n",
    "        return rank - 4\n",
    "        \n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea22ff0",
   "metadata": {},
   "source": [
    "## 5. AutoEncoder 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=128, latent_dim=64):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder (加深並加入 BatchNorm)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 96),\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 96),\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "def train_autoencoder(data, input_dim=128, latent_dim=64, epochs=100, batch_size=256, patience=5, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    訓練自編碼器 (加入 Early Stopping)\n",
    "    \"\"\"\n",
    "    model = AutoEncoder(input_dim, latent_dim).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(data).to(DEVICE)\n",
    "    \n",
    "    # 創建 DataLoader 以便於 batch 處理\n",
    "    dataset = torch.utils.data.TensorDataset(data_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    pbar = tqdm(range(epochs), desc=\"训练AutoEncoder\")\n",
    "    for epoch in pbar:\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch_data = batch[0]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            _, reconstructed = model(batch_data)\n",
    "            loss = criterion(reconstructed, batch_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        pbar.set_postfix({'loss': f'{avg_loss:.6f}'})\n",
    "        \n",
    "        # Early Stopping Check\n",
    "        if avg_loss < best_loss - min_delta:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1} (Best Loss: {best_loss:.6f})\")\n",
    "            break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97cf329",
   "metadata": {},
   "source": [
    "## 6. 五种数据处理方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6cb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quantization(train_data, query_data, quantizer):\n",
    "    \"\"\"Helper to apply quantizer (function or class) to train and query data\"\"\"\n",
    "    # Check if quantizer is a class instance (has fit/transform)\n",
    "    if hasattr(quantizer, 'fit') and hasattr(quantizer, 'transform'):\n",
    "        # Stateful quantizer: Fit on Train, Transform Train & Query\n",
    "        # Note: We must fit on the specific data passed here (which might be reduced dim)\n",
    "        train_q = quantizer.fit_transform(train_data)\n",
    "        query_q = quantizer.transform(query_data) if query_data is not None else None\n",
    "    else:\n",
    "        # Functional quantizer: Stateless\n",
    "        train_q = quantizer(train_data)\n",
    "        query_q = quantizer(query_data) if query_data is not None else None\n",
    "    return train_q, query_q\n",
    "\n",
    "def method1_direct_int3(data, query_data=None, quantizer=quantize_to_int3):\n",
    "    \"\"\"方法1: 直接將128維向量量化為INT3\"\"\"\n",
    "    # Convert to Tensor if numpy\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = torch.from_numpy(data).to(DEVICE)\n",
    "    if query_data is not None and isinstance(query_data, np.ndarray):\n",
    "        query_data = torch.from_numpy(query_data).to(DEVICE)\n",
    "        \n",
    "    db_q, q_q = apply_quantization(data, query_data, quantizer)\n",
    "    \n",
    "    if query_data is not None:\n",
    "        return None, db_q, q_q\n",
    "    return db_q\n",
    "\n",
    "def method2_average_pooling_int3(data, query_data=None, quantizer=quantize_to_int3):\n",
    "    \"\"\"方法2: 將相鄰兩維做平均，從128維降到64維，然後量化為INT3\"\"\"\n",
    "    def process(arr):\n",
    "        if isinstance(arr, np.ndarray):\n",
    "            arr = torch.from_numpy(arr).to(DEVICE)\n",
    "        # Reshape (N, 64, 2)\n",
    "        reshaped = arr.reshape(arr.shape[0], 64, 2)\n",
    "        # PyTorch mean\n",
    "        return torch.mean(reshaped.float(), dim=2)\n",
    "\n",
    "    data_reduced = process(data)\n",
    "    query_reduced = process(query_data) if query_data is not None else None\n",
    "    \n",
    "    db_q, q_q = apply_quantization(data_reduced, query_reduced, quantizer)\n",
    "    \n",
    "    if query_data is not None:\n",
    "        return None, db_q, q_q\n",
    "    return db_q\n",
    "\n",
    "def method3_pca_int3(data, query_data=None, quantizer=quantize_to_int3):\n",
    "    \"\"\"方法3: 使用PCA降維到64維，然後量化為INT3\"\"\"\n",
    "    print(\"  - 训练PCA (CPU)...\")\n",
    "    \n",
    "    # Ensure data is numpy for sklearn fitting\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data_np = data.cpu().numpy()\n",
    "    else:\n",
    "        data_np = data\n",
    "        \n",
    "    # 使用 whiten=True 來標準化分量\n",
    "    pca = PCA(n_components=REDUCED_DIM, whiten=True)\n",
    "    pca.fit(data_np)\n",
    "    \n",
    "    # Prepare for GPU transform\n",
    "    mean = torch.from_numpy(pca.mean_).float().to(DEVICE)\n",
    "    components = torch.from_numpy(pca.components_).float().to(DEVICE)\n",
    "    explained_variance = torch.from_numpy(pca.explained_variance_).float().to(DEVICE)\n",
    "    \n",
    "    def transform_gpu(X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = torch.from_numpy(X).to(DEVICE)\n",
    "        X = X.float()\n",
    "        \n",
    "        # Center\n",
    "        X_centered = X - mean\n",
    "        # Project\n",
    "        X_transformed = torch.matmul(X_centered, components.T)\n",
    "        \n",
    "        # Whiten\n",
    "        if pca.whiten:\n",
    "            scale = torch.sqrt(explained_variance)\n",
    "            X_transformed = X_transformed / scale\n",
    "            \n",
    "        return X_transformed\n",
    "\n",
    "    data_reduced = transform_gpu(data_np)\n",
    "    query_reduced = transform_gpu(query_data) if query_data is not None else None\n",
    "    \n",
    "    db_q, q_q = apply_quantization(data_reduced, query_reduced, quantizer)\n",
    "    \n",
    "    if query_data is not None:\n",
    "        return pca, db_q, q_q\n",
    "    \n",
    "    return pca, db_q\n",
    "\n",
    "def method4_max_pooling_int3(data, query_data=None, quantizer=quantize_to_int3):\n",
    "    \"\"\"方法4: 將相鄰兩維做最大絕對值池化 (Max Magnitude Pooling)，從128維降到64維，然後量化為INT3\"\"\"\n",
    "    \n",
    "    def max_magnitude_pool_torch(arr):\n",
    "        if isinstance(arr, np.ndarray):\n",
    "            arr = torch.from_numpy(arr).to(DEVICE)\n",
    "            \n",
    "        # Reshape to (N, 64, 2)\n",
    "        reshaped = arr.reshape(arr.shape[0], 64, 2)\n",
    "        a = reshaped[:, :, 0]\n",
    "        b = reshaped[:, :, 1]\n",
    "        # 比較絕對值大小\n",
    "        mask = torch.abs(a) >= torch.abs(b)\n",
    "        # 選擇絕對值較大的那個原始值\n",
    "        return torch.where(mask, a, b)\n",
    "\n",
    "    data_reduced = max_magnitude_pool_torch(data)\n",
    "    query_reduced = max_magnitude_pool_torch(query_data) if query_data is not None else None\n",
    "    \n",
    "    db_q, q_q = apply_quantization(data_reduced, query_reduced, quantizer)\n",
    "    \n",
    "    if query_data is not None:\n",
    "        return None, db_q, q_q\n",
    "    \n",
    "    return None, db_q\n",
    "\n",
    "def method5_autoencoder_int3(data, query_data=None, epochs=100, quantizer=quantize_to_int3):\n",
    "    \"\"\"方法5: 使用AutoEncoder降維到64維，然後量化為INT3\"\"\"\n",
    "    print(\"  - 训练AutoEncoder...\")\n",
    "    # Ensure numpy for training (dataloader handles conversion)\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data_np = data.cpu().numpy()\n",
    "    else:\n",
    "        data_np = data\n",
    "        \n",
    "    # 這裡 epochs 預設為 100，配合 Early Stopping\n",
    "    ae_model = train_autoencoder(data_np, SIFT_DIM, REDUCED_DIM, epochs=epochs)\n",
    "    ae_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode on GPU\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data_tensor = torch.FloatTensor(data).to(DEVICE)\n",
    "        else:\n",
    "            data_tensor = data.float().to(DEVICE)\n",
    "            \n",
    "        data_reduced = ae_model.encode(data_tensor) # Returns tensor on GPU\n",
    "        \n",
    "        if query_data is not None:\n",
    "            if isinstance(query_data, np.ndarray):\n",
    "                query_tensor = torch.FloatTensor(query_data).to(DEVICE)\n",
    "            else:\n",
    "                query_tensor = query_data.float().to(DEVICE)\n",
    "            query_reduced = ae_model.encode(query_tensor)\n",
    "        else:\n",
    "            query_reduced = None\n",
    "            \n",
    "        db_q, q_q = apply_quantization(data_reduced, query_reduced, quantizer)\n",
    "        \n",
    "        if query_data is not None:\n",
    "            return ae_model, db_q, q_q\n",
    "    \n",
    "    return ae_model, db_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21951698",
   "metadata": {},
   "source": [
    "## 7. 距离计算和评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_l2_distances(queries, database):\n",
    "    \"\"\"計算L2距離\"\"\"\n",
    "    return pairwise_distances(queries, database, metric='euclidean')\n",
    "\n",
    "def evaluate_recall_at_k(distances, ground_truth, k_values=[1, 10, 100]):\n",
    "    \"\"\"計算Recall@K指標\"\"\"\n",
    "    sorted_indices = np.argsort(distances, axis=1)\n",
    "    \n",
    "    recalls = {}\n",
    "    for k in k_values:\n",
    "        top_k_predictions = sorted_indices[:, :k]\n",
    "        \n",
    "        query_recalls = []\n",
    "        for i in range(len(ground_truth)):\n",
    "            gt_i = ground_truth[i]\n",
    "            # Handle jagged arrays: check length\n",
    "            limit = min(k, len(gt_i))\n",
    "            \n",
    "            if limit == 0:\n",
    "                query_recalls.append(0.0)\n",
    "                continue\n",
    "                \n",
    "            true_neighbors = set(gt_i[:limit])\n",
    "            pred_neighbors = set(top_k_predictions[i])\n",
    "            intersection = true_neighbors & pred_neighbors\n",
    "            recall = len(intersection) / len(true_neighbors)\n",
    "            query_recalls.append(recall)\n",
    "        \n",
    "        recalls[f'recall@{k}'] = np.mean(query_recalls)\n",
    "    \n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a202e3",
   "metadata": {},
   "source": [
    "## 8. 加载 SIFT1M 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fvecs(filename):\n",
    "    \"\"\"讀取.fvecs格式文件\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        d = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "        f.seek(0)\n",
    "        # .fvecs 格式: 4 bytes int32 (dimension) + d * 4 bytes float32 (data)\n",
    "        # 读取为 float32，header 的 int32 会被读成一个 float32，reshape 后切片去掉即可\n",
    "        data = np.fromfile(f, dtype=np.float32)\n",
    "        data = data.reshape(-1, d + 1)\n",
    "        return data[:, 1:].copy()\n",
    "\n",
    "def read_ivecs(filename):\n",
    "    \"\"\"讀取.ivecs格式文件\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        d = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "        f.seek(0)\n",
    "        data = np.fromfile(f, dtype=np.int32)\n",
    "        data = data.reshape(-1, d + 1)\n",
    "        return data[:, 1:].copy()\n",
    "\n",
    "print(\"载入SIFT1M数据集...\")\n",
    "base_vectors = read_fvecs('sift1m/sift_base.fvecs')\n",
    "query_vectors = read_fvecs('sift1m/sift_query.fvecs')\n",
    "ground_truth = read_ivecs('sift1m/sift_groundtruth.ivecs')\n",
    "\n",
    "print(f\"Base vectors shape: {base_vectors.shape}\")\n",
    "print(f\"Query vectors shape: {query_vectors.shape}\")\n",
    "print(f\"Ground truth shape: {ground_truth.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70c0f9",
   "metadata": {},
   "source": [
    "## 9. 运行实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top_k_ground_truth(queries, database, k=100):\n",
    "    \"\"\"\n",
    "    計算每個查詢的前 K 個真實最近鄰居 (Ground Truth)\n",
    "    使用 GPU 加速計算，並分批處理 Query 以節省記憶體\n",
    "    \"\"\"\n",
    "    print(f\"正在計算 Top-{k} Ground Truth (Query: {len(queries)}, DB: {len(database)})...\")\n",
    "    \n",
    "    num_queries = len(queries)\n",
    "    query_batch_size = 100  # 每次處理 100 個 Query\n",
    "    \n",
    "    all_indices = []\n",
    "    \n",
    "    for i in tqdm(range(0, num_queries, query_batch_size), desc=\"Computing GT\"):\n",
    "        q_end = min(i + query_batch_size, num_queries)\n",
    "        q_batch = queries[i:q_end]\n",
    "        q_tensor = torch.FloatTensor(q_batch).to(DEVICE)\n",
    "        q_sq = torch.sum(q_tensor**2, dim=1, keepdim=True)\n",
    "        \n",
    "        dists_batch = []\n",
    "        \n",
    "        # 分批處理 DB\n",
    "        db_batch_size = 50000\n",
    "        for j in range(0, len(database), db_batch_size):\n",
    "            db_end = min(j + db_batch_size, len(database))\n",
    "            db_chunk = torch.FloatTensor(database[j:db_end]).to(DEVICE)\n",
    "            \n",
    "            db_sq_chunk = torch.sum(db_chunk**2, dim=1)\n",
    "            term2 = -2 * torch.matmul(q_tensor, db_chunk.t())\n",
    "            \n",
    "            dists_chunk = q_sq + db_sq_chunk + term2\n",
    "            dists_batch.append(dists_chunk.cpu())\n",
    "            \n",
    "            del db_chunk, db_sq_chunk, term2, dists_chunk\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        full_dists_batch = torch.cat(dists_batch, dim=1)\n",
    "        \n",
    "        # 取 Top K\n",
    "        _, indices = torch.topk(full_dists_batch, k=k, dim=1, largest=False)\n",
    "        all_indices.append(indices.numpy())\n",
    "        \n",
    "        del full_dists_batch, indices, q_tensor, q_sq\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return np.vstack(all_indices)\n",
    "\n",
    "def evaluate_recall_batched(query_vectors, db_vectors, gt_top_k, retrieval_depths, k_true_neighbors):\n",
    "    \"\"\"\n",
    "    分批計算距離並評估 Recall\n",
    "    使用 GPU 加速距離計算\n",
    "    Recall = (Retrieved & Top-K_GT) / K_GT\n",
    "    \"\"\"\n",
    "    max_depth = max(retrieval_depths)\n",
    "    num_queries = len(query_vectors)\n",
    "    query_batch_size = 100\n",
    "    \n",
    "    total_hits = {r: 0 for r in retrieval_depths}\n",
    "    \n",
    "    # Ensure inputs are tensors on GPU\n",
    "    if isinstance(query_vectors, np.ndarray):\n",
    "        query_vectors = torch.from_numpy(query_vectors).to(DEVICE)\n",
    "    elif query_vectors.device != DEVICE:\n",
    "        query_vectors = query_vectors.to(DEVICE)\n",
    "        \n",
    "    if isinstance(db_vectors, np.ndarray):\n",
    "        db_vectors = torch.from_numpy(db_vectors).to(DEVICE)\n",
    "    elif db_vectors.device != DEVICE:\n",
    "        db_vectors = db_vectors.to(DEVICE)\n",
    "    \n",
    "    # Ensure float for distance calc (INT3 needs to be float for calc)\n",
    "    if query_vectors.dtype != torch.float32:\n",
    "        query_vectors = query_vectors.float()\n",
    "    if db_vectors.dtype != torch.float32:\n",
    "        db_vectors = db_vectors.float()\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Pre-calculate DB squared norms if memory allows, or do it in chunks\n",
    "    # For 1M vectors, pre-calculating norms is fast and takes little memory (1M floats = 4MB)\n",
    "    db_sq = torch.sum(db_vectors**2, dim=1)\n",
    "    \n",
    "    for i in tqdm(range(0, num_queries, query_batch_size), desc=\"Evaluating Batches\", leave=False):\n",
    "        q_end = min(i + query_batch_size, num_queries)\n",
    "        q_batch = query_vectors[i:q_end] # (Batch, Dim)\n",
    "        gt_batch = gt_top_k[i:q_end]\n",
    "        \n",
    "        # Calculate L2 Distance on GPU: |x-y|^2 = |x|^2 + |y|^2 - 2xy\n",
    "        q_sq = torch.sum(q_batch**2, dim=1, keepdim=True) # (Batch, 1)\n",
    "        \n",
    "        # Matrix multiplication: (Batch, Dim) @ (Dim, DB_Size) -> (Batch, DB_Size)\n",
    "        # Note: db_vectors is (DB_Size, Dim), so we transpose it\n",
    "        # For very large DB, we might need to chunk this too, but 1M fits in GPU memory for matmul usually\n",
    "        # 100 * 1M * 4 bytes = 400MB result matrix. This is fine.\n",
    "        \n",
    "        term2 = -2 * torch.matmul(q_batch, db_vectors.t())\n",
    "        \n",
    "        # Broadcasting: (Batch, 1) + (DB_Size,) + (Batch, DB_Size)\n",
    "        dists = q_sq + db_sq + term2\n",
    "        \n",
    "        # Find Top-K on GPU\n",
    "        # We need the smallest distances\n",
    "        _, sorted_indices_tensor = torch.topk(dists, k=max_depth, dim=1, largest=False)\n",
    "        \n",
    "        # Move indices to CPU for set intersection (faster on CPU for small sets logic)\n",
    "        sorted_indices = sorted_indices_tensor.cpu().numpy()\n",
    "        \n",
    "        for r in retrieval_depths:\n",
    "            retrieved_indices = sorted_indices[:, :r]\n",
    "            for j in range(len(gt_batch)):\n",
    "                gt_set = set(gt_batch[j]) # 這是 Top K 真實鄰居\n",
    "                retrieved_set = set(retrieved_indices[j])\n",
    "                total_hits[r] += len(gt_set & retrieved_set)\n",
    "                \n",
    "        # Clean up intermediate tensors\n",
    "        del dists, term2, q_sq, sorted_indices_tensor\n",
    "        # torch.cuda.empty_cache() # Optional: can slow down loop if called too often\n",
    "    \n",
    "    end_time = time.time()\n",
    "    search_time = end_time - start_time\n",
    "                \n",
    "    recalls = {}\n",
    "    for r in retrieval_depths:\n",
    "        # Normalize by k_true_neighbors (e.g., 100)\n",
    "        recalls[f'recall@{r}'] = total_hits[r] / (num_queries * float(k_true_neighbors))\n",
    "        \n",
    "    recalls['search_time'] = search_time\n",
    "    recalls['qps'] = num_queries / search_time if search_time > 0 else 0\n",
    "        \n",
    "    return recalls\n",
    "\n",
    "def run_experiment_part(part_name, base_vectors, query_vectors, gt_top_k, retrieval_depths, k_true_neighbors, quantizer=quantize_to_int3, quantizer_name=\"Standard\"):\n",
    "    \"\"\"\n",
    "    執行單個部分的實驗\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"實驗部分: {part_name} (Quantizer: {quantizer_name})\")\n",
    "    print(f\"Database Size: {len(base_vectors)}\")\n",
    "    print(f\"Retrieval Depths: {retrieval_depths}\")\n",
    "    print(f\"Target GT Size (K): {k_true_neighbors}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Helper to get a fresh quantizer instance if it's a class\n",
    "    # This ensures we don't reuse fitted thresholds across different methods (e.g. PCA vs Raw)\n",
    "    def get_quantizer():\n",
    "        if isinstance(quantizer, type):\n",
    "            return quantizer()\n",
    "        elif hasattr(quantizer, 'fit'):\n",
    "            # If it's an instance, we should probably clone it or create new\n",
    "            # Assuming the user passed a class instance that can be re-fitted\n",
    "            # But to be safe, we should create a new one if possible.\n",
    "            # For now, we assume .fit() overwrites state.\n",
    "            return quantizer\n",
    "        else:\n",
    "            return quantizer\n",
    "\n",
    "    # 方法1\n",
    "    print(f\"\\n[方法1] 直接INT3量化 (128維) - {quantizer_name}...\")\n",
    "    t0 = time.time()\n",
    "    # Note: method1 now accepts query_data and returns (None, db, q)\n",
    "    _, db_m1, q_m1 = method1_direct_int3(base_vectors, query_vectors, quantizer=get_quantizer())\n",
    "    recalls_m1 = evaluate_recall_batched(q_m1, db_m1, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': f'Method 1: Direct INT3 ({quantizer_name})', 'time': time.time()-t0, **recalls_m1})\n",
    "    print(f\"Recall: {recalls_m1}, QPS: {recalls_m1.get('qps', 0):.2f}\")\n",
    "    \n",
    "    # 方法2\n",
    "    print(f\"\\n[方法2] 平均池化降維 + INT3 (64維) - {quantizer_name}...\")\n",
    "    t0 = time.time()\n",
    "    _, db_m2, q_m2 = method2_average_pooling_int3(base_vectors, query_vectors, quantizer=get_quantizer())\n",
    "    recalls_m2 = evaluate_recall_batched(q_m2, db_m2, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': f'Method 2: AvgPooling + INT3 ({quantizer_name})', 'time': time.time()-t0, **recalls_m2})\n",
    "    print(f\"Recall: {recalls_m2}, QPS: {recalls_m2.get('qps', 0):.2f}\")\n",
    "    \n",
    "    # 方法3\n",
    "    print(f\"\\n[方法3] PCA降維 + INT3 (64維) - {quantizer_name}...\")\n",
    "    t0 = time.time()\n",
    "    _, db_m3, q_m3 = method3_pca_int3(base_vectors, query_vectors, quantizer=get_quantizer())\n",
    "    recalls_m3 = evaluate_recall_batched(q_m3, db_m3, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': f'Method 3: PCA + INT3 ({quantizer_name})', 'time': time.time()-t0, **recalls_m3})\n",
    "    print(f\"Recall: {recalls_m3}, QPS: {recalls_m3.get('qps', 0):.2f}\")\n",
    "    \n",
    "    # 方法4\n",
    "    print(f\"\\n[方法4] Max Magnitude Pooling降維 + INT3 (64維) - {quantizer_name}...\")\n",
    "    t0 = time.time()\n",
    "    _, db_m4, q_m4 = method4_max_pooling_int3(base_vectors, query_vectors, quantizer=get_quantizer())\n",
    "    recalls_m4 = evaluate_recall_batched(q_m4, db_m4, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': f'Method 4: Max Mag Pooling + INT3 ({quantizer_name})', 'time': time.time()-t0, **recalls_m4})\n",
    "    print(f\"Recall: {recalls_m4}, QPS: {recalls_m4.get('qps', 0):.2f}\")\n",
    "    \n",
    "    # 方法5\n",
    "    print(f\"\\n[方法5] AutoEncoder降維 + INT3 (64維) - {quantizer_name}...\")\n",
    "    t0 = time.time()\n",
    "    # Epochs 設為 100，配合 Early Stopping\n",
    "    _, db_m5, q_m5 = method5_autoencoder_int3(base_vectors, query_vectors, epochs=100, quantizer=get_quantizer())\n",
    "    recalls_m5 = evaluate_recall_batched(q_m5, db_m5, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': f'Method 5: AutoEncoder + INT3 ({quantizer_name})', 'time': time.time()-t0, **recalls_m5})\n",
    "    print(f\"Recall: {recalls_m5}, QPS: {recalls_m5.get('qps', 0):.2f}\")\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行實驗\n",
    "# 實驗配置：\n",
    "# Part 1: 100K DB, Recall 100@1000, 100@5000, 100@10000\n",
    "# Part 2: 1M DB, Recall 100@1000, 100@5000, 100@10000\n",
    "# 比較兩種 Quantization 方法：Standard (Percentile) vs Quantile (Per-Dimension Rank-based)\n",
    "\n",
    "all_experiment_results = []  # 存儲結果 (包含 Recall, Time, QPS)\n",
    "\n",
    "# 定義要比較的 Quantizers\n",
    "# 注意：對於 Class 類型的 Quantizer，我們傳遞實例，run_experiment_part 會重複使用它 (每次 fit 會覆蓋)\n",
    "quantizers = [\n",
    "    (\"Standard\", quantize_to_int3),\n",
    "    (\"Quantile\", PerDimensionQuantileQuantizer())\n",
    "]\n",
    "\n",
    "# --- Part 1: 100K Database ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Part 1: First 100K Database\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "db_100k = base_vectors[:100000].astype(np.float32)\n",
    "query_subset = query_vectors[:1000].astype(np.float32)\n",
    "\n",
    "# 計算 Ground Truth for 100K DB\n",
    "print(\"Computing Ground Truth for 100K DB...\")\n",
    "gt_100k = calculate_top_k_ground_truth(query_subset, db_100k, k=100)\n",
    "\n",
    "# 執行實驗 Part 1 (兩種 Quantization)\n",
    "retrieval_depths_1 = [1000, 5000, 10000]\n",
    "k_true_neighbors = 100\n",
    "\n",
    "for q_name, q_obj in quantizers:\n",
    "    results_df = run_experiment_part(\n",
    "        f\"Part 1: 100K DB ({q_name})\",\n",
    "        db_100k,\n",
    "        query_subset,\n",
    "        gt_100k,\n",
    "        retrieval_depths_1,\n",
    "        k_true_neighbors,\n",
    "        quantizer=q_obj,\n",
    "        quantizer_name=q_name\n",
    "    )\n",
    "    all_experiment_results.append(results_df)\n",
    "    print(f\"\\nPart 1 ({q_name}) Results:\")\n",
    "    display(results_df)\n",
    "\n",
    "\n",
    "# --- Part 2: 1M Database ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Part 2: Full 1M Database\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "db_1m = base_vectors.astype(np.float32)\n",
    "\n",
    "# 計算 Ground Truth for 1M DB\n",
    "print(\"Computing Ground Truth for 1M DB...\")\n",
    "gt_1m = calculate_top_k_ground_truth(query_subset, db_1m, k=100)\n",
    "\n",
    "# 執行實驗 Part 2 (兩種 Quantization)\n",
    "retrieval_depths_2 = [1000, 5000, 10000]\n",
    "\n",
    "for q_name, q_obj in quantizers:\n",
    "    results_df = run_experiment_part(\n",
    "        f\"Part 2: 1M DB ({q_name})\",\n",
    "        db_1m,\n",
    "        query_subset,\n",
    "        gt_1m,\n",
    "        retrieval_depths_2,\n",
    "        k_true_neighbors,\n",
    "        quantizer=q_obj,\n",
    "        quantizer_name=q_name\n",
    "    )\n",
    "    all_experiment_results.append(results_df)\n",
    "    print(f\"\\nPart 2 ({q_name}) Results:\")\n",
    "    display(results_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"所有實驗完成！\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4ac29",
   "metadata": {},
   "source": [
    "## 10. 可视化结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a08e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化結果\n",
    "# 比較 Standard vs Quantile Quantization\n",
    "# 每個指標 (Recall@K, QPS) 獨立一張圖，每種方法顯示兩條 Bar (Standard, Quantile)\n",
    "\n",
    "# 確保變數名稱兼容性\n",
    "if 'all_experiment_results' not in locals() and 'all_results' in locals():\n",
    "    all_experiment_results = all_results\n",
    "\n",
    "num_results = len(all_experiment_results)\n",
    "print(f\"Detected {num_results} result dataframes.\")\n",
    "\n",
    "if num_results > 0 and num_results % 2 == 0:\n",
    "    # 配對邏輯: 假設每兩個結果是一組 (Standard, Quantile)\n",
    "    # 順序: Part1-Std, Part1-Qnt, Part2-Std, Part2-Qnt...\n",
    "    \n",
    "    num_parts = num_results // 2\n",
    "    \n",
    "    for i in range(num_parts):\n",
    "        df_std = all_experiment_results[2*i]\n",
    "        df_qnt = all_experiment_results[2*i+1]\n",
    "        \n",
    "        # 決定標題\n",
    "        part_title = f\"Experiment Part {i+1}\"\n",
    "        if num_parts == 2:\n",
    "            part_title = \"100K DB\" if i == 0 else \"1M DB\"\n",
    "        elif num_parts == 1:\n",
    "            part_title = \"Experiment Results\"\n",
    "            \n",
    "        print(f\"\\nVisualizing {part_title}...\")\n",
    "        \n",
    "        # 準備 X 軸標籤 (方法名稱)\n",
    "        raw_methods = df_std['method'].tolist()\n",
    "        methods = []\n",
    "        for m in raw_methods:\n",
    "            # 清理名稱\n",
    "            m = m.split('(')[0].strip() # 移除 (Standard)\n",
    "            m = m.replace('Method ', 'M').replace(': ', '\\n')\n",
    "            m = m.replace('Direct INT3', 'Direct').replace('AvgPooling + INT3', 'AvgPool')\n",
    "            m = m.replace('PCA + INT3', 'PCA').replace('Max Mag Pooling + INT3', 'MaxMag')\n",
    "            m = m.replace('AutoEncoder + INT3', 'AE')\n",
    "            methods.append(m)\n",
    "            \n",
    "        # 找出所有的 Recall 指標\n",
    "        recall_cols = [c for c in df_std.columns if 'recall@' in c]\n",
    "        try:\n",
    "            recall_cols.sort(key=lambda x: int(x.split('@')[1]))\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        # 加上 QPS\n",
    "        metrics = recall_cols + ['qps']\n",
    "        \n",
    "        # 建立圖表: 每個指標一張子圖\n",
    "        # 使用多行佈局\n",
    "        n_metrics = len(metrics)\n",
    "        ncols = min(n_metrics, 2) # 每行最多2張圖，避免太擠\n",
    "        nrows = (n_metrics + ncols - 1) // ncols\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(12, 6 * nrows))\n",
    "        if n_metrics == 1:\n",
    "            axes = np.array([axes])\n",
    "        axes = np.array(axes).reshape(-1) # Flatten\n",
    "        \n",
    "        fig.suptitle(f'{part_title}: Standard vs Quantile Quantization', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        x = np.arange(len(methods))\n",
    "        width = 0.35\n",
    "        \n",
    "        for j, metric in enumerate(metrics):\n",
    "            ax = axes[j]\n",
    "            \n",
    "            val_std = df_std[metric].values\n",
    "            val_qnt = df_qnt[metric].values\n",
    "            \n",
    "            # 繪製長條圖\n",
    "            rects1 = ax.bar(x - width/2, val_std, width, label='Standard', color='#1f77b4')\n",
    "            rects2 = ax.bar(x + width/2, val_qnt, width, label='Quantile', color='#ff7f0e')\n",
    "            \n",
    "            ax.set_title(metric, fontsize=14, fontweight='bold')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(methods, rotation=45, ha='right')\n",
    "            ax.legend()\n",
    "            ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # 設定 Y 軸範圍 (Recall 0-1, QPS 自動)\n",
    "            if 'recall' in metric:\n",
    "                ax.set_ylim([0, 1.1])\n",
    "                ax.set_ylabel('Recall')\n",
    "            else:\n",
    "                ax.set_ylabel('QPS')\n",
    "                \n",
    "            # 標註數值\n",
    "            def autolabel(rects, is_recall):\n",
    "                for rect in rects:\n",
    "                    height = rect.get_height()\n",
    "                    val_str = f'{height:.3f}' if is_recall else f'{int(height)}'\n",
    "                    # 調整標註位置，避免重疊\n",
    "                    ax.annotate(val_str,\n",
    "                                xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                                xytext=(0, 3),\n",
    "                                textcoords=\"offset points\",\n",
    "                                ha='center', va='bottom', fontsize=9, \n",
    "                                rotation=90 if is_recall else 0)\n",
    "            \n",
    "            autolabel(rects1, 'recall' in metric)\n",
    "            autolabel(rects2, 'recall' in metric)\n",
    "            \n",
    "        # 隱藏多餘的子圖\n",
    "        for k in range(j+1, len(axes)):\n",
    "            axes[k].axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.92, hspace=0.4)\n",
    "        \n",
    "        # --- Save Figure ---\n",
    "        safe_title = part_title.replace(\" \", \"_\").replace(\":\", \"\")\n",
    "        filename = f'sift1m_comparison_{safe_title}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot to {filename}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "else:\n",
    "    print(\"無法配對結果進行比較 (結果數量不是偶數或為0)。\")\n",
    "    print(f\"Results count: {num_results}\")\n",
    "    if num_results > 0:\n",
    "        print(\"顯示原始表格:\")\n",
    "        for i, df in enumerate(all_experiment_results):\n",
    "            print(f\"Result {i+1}\")\n",
    "            display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44c37c",
   "metadata": {},
   "source": [
    "## 11. 实验总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"實驗總結\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 確保變數名稱兼容性\n",
    "if 'all_experiment_results' not in locals() and 'all_results' in locals():\n",
    "    all_experiment_results = all_results\n",
    "\n",
    "if 'all_experiment_results' in locals():\n",
    "    for i, results_df in enumerate(all_experiment_results):\n",
    "        print(f\"\\nExperiment Part {i+1}\")\n",
    "        display(results_df)\n",
    "else:\n",
    "    print(\"No results found.\")\n",
    "\n",
    "print(\"\\n所有實驗完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Dimension Reduction Analysis (128 -> 16)\n",
    "# Analyzing Recall Rate vs Dimension for AvgPooling and PCA\n",
    "# Configuration: 1M Database, Recall of Top-1000 GT @ Depth 10000\n",
    "\n",
    "def adaptive_avg_pool_int3(data, target_dim):\n",
    "    \"\"\"\n",
    "    Adaptive Average Pooling to reduce to specific target dimension\n",
    "    \"\"\"\n",
    "    if isinstance(data, np.ndarray):\n",
    "        t = torch.from_numpy(data).float()\n",
    "    else:\n",
    "        t = data.float()\n",
    "    \n",
    "    t = t.to(DEVICE)\n",
    "    # Input: (N, 128) -> (N, 1, 128) for pooling\n",
    "    t = t.unsqueeze(1)\n",
    "    # Adaptive Pool to (N, 1, target_dim)\n",
    "    pooled = nn.functional.adaptive_avg_pool1d(t, target_dim)\n",
    "    # (N, target_dim)\n",
    "    pooled = pooled.squeeze(1)\n",
    "    \n",
    "    return quantize_to_int3(pooled)\n",
    "\n",
    "def run_dimension_sweep(base_vectors, query_vectors, gt_top_k, k_true_neighbors=1000, retrieval_depth=10000):\n",
    "    # Dimensions: 128, 112, 96, ..., 16\n",
    "    target_dims = list(range(128, 15, -16)) \n",
    "    results = []\n",
    "    \n",
    "    print(f\"Starting Dimension Sweep: {target_dims}\")\n",
    "    print(f\"Database Size: {len(base_vectors)}\")\n",
    "    print(f\"Target GT Size: {k_true_neighbors}\")\n",
    "    print(f\"Retrieval Depth: {retrieval_depth}\")\n",
    "    \n",
    "    for dim in target_dims:\n",
    "        print(f\"\\nTesting Target Dimension: {dim}\")\n",
    "        \n",
    "        # --- Method: Adaptive Avg Pooling ---\n",
    "        print(\"  Running Avg Pooling...\")\n",
    "        t0 = time.time()\n",
    "        db_avg = adaptive_avg_pool_int3(base_vectors, dim)\n",
    "        q_avg = adaptive_avg_pool_int3(query_vectors, dim)\n",
    "        \n",
    "        # Evaluate Recall\n",
    "        recalls_avg = evaluate_recall_batched(q_avg, db_avg, gt_top_k, [retrieval_depth], k_true_neighbors)\n",
    "        time_avg = time.time() - t0\n",
    "        \n",
    "        metric_key = f'recall@{retrieval_depth}'\n",
    "        results.append({\n",
    "            'Dimension': dim,\n",
    "            'Method': 'AvgPooling',\n",
    "            'Recall': recalls_avg[metric_key],\n",
    "            'Time': time_avg\n",
    "        })\n",
    "        print(f\"    AvgPool {metric_key}: {recalls_avg[metric_key]:.4f}\")\n",
    "        \n",
    "        # --- Method: PCA ---\n",
    "        print(\"  Running PCA...\")\n",
    "        t0 = time.time()\n",
    "        # Fit PCA on a subset for speed (max 50k)\n",
    "        fit_size = min(len(base_vectors), 50000)\n",
    "        pca = PCA(n_components=dim, whiten=True)\n",
    "        pca.fit(base_vectors[:fit_size])\n",
    "        \n",
    "        db_pca = pca.transform(base_vectors)\n",
    "        q_pca = pca.transform(query_vectors)\n",
    "        \n",
    "        db_pca_q = quantize_to_int3(db_pca)\n",
    "        q_pca_q = quantize_to_int3(q_pca)\n",
    "        \n",
    "        recalls_pca = evaluate_recall_batched(q_pca_q, db_pca_q, gt_top_k, [retrieval_depth], k_true_neighbors)\n",
    "        time_pca = time.time() - t0\n",
    "        \n",
    "        results.append({\n",
    "            'Dimension': dim,\n",
    "            'Method': 'PCA',\n",
    "            'Recall': recalls_pca[metric_key],\n",
    "            'Time': time_pca\n",
    "        })\n",
    "        print(f\"    PCA {metric_key}: {recalls_pca[metric_key]:.4f}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Setup Data for Sweep (Using 1M Full DB)\n",
    "# Ensure we have the data\n",
    "if 'base_vectors_full' not in locals():\n",
    "    base_vectors_full = base_vectors.astype(np.float32)\n",
    "    query_vectors_subset = query_vectors[:1000].astype(np.float32)\n",
    "\n",
    "# Use Full 1M Database\n",
    "db_subset_sweep = base_vectors_full \n",
    "k_gt = 1000\n",
    "depth = 10000\n",
    "\n",
    "print(f\"Computing GT for sweep (1M DB, Top-{k_gt})...\")\n",
    "# Always recalculate to be safe or check if existing gt matches requirements\n",
    "# Since previous cells might have calculated k=100, we likely need to recalc for k=1000\n",
    "gt_sweep = calculate_top_k_ground_truth(query_vectors_subset, db_subset_sweep, k=k_gt)\n",
    "\n",
    "# Run Sweep\n",
    "sweep_df = run_dimension_sweep(db_subset_sweep, query_vectors_subset, gt_sweep, k_true_neighbors=k_gt, retrieval_depth=depth)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "avg_df = sweep_df[sweep_df['Method'] == 'AvgPooling']\n",
    "pca_df = sweep_df[sweep_df['Method'] == 'PCA']\n",
    "\n",
    "plt.plot(avg_df['Dimension'], avg_df['Recall'], marker='o', linewidth=2, label='Avg Pooling + INT3')\n",
    "plt.plot(pca_df['Dimension'], pca_df['Recall'], marker='s', linewidth=2, label='PCA + INT3')\n",
    "\n",
    "plt.xlabel('Dimension', fontsize=12)\n",
    "plt.ylabel(f'Recall@{depth} (Top-{k_gt} GT)', fontsize=12)\n",
    "plt.title(f'Recall vs Dimension (1M DB, {k_gt}@{depth})', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.gca().invert_xaxis() # Display 128 on left, 16 on right\n",
    "plt.xticks(list(range(128, 15, -16)))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save Results\n",
    "csv_filename = 'sift1m_1M_1000at10000_sweep_results.csv'\n",
    "sweep_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Sweep results saved to {csv_filename}\")\n",
    "sweep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fe3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. PCA Dimension Difference Analysis\n",
    "# Analyze absolute difference between Query and Top-100 NN in PCA space per dimension\n",
    "\n",
    "def analyze_pca_diff(base_vectors, query_vectors, gt_indices, n_components=128):\n",
    "    print(\"Fitting PCA...\")\n",
    "    # Fit PCA on a subset for speed\n",
    "    fit_size = min(len(base_vectors), 50000)\n",
    "    pca = PCA(n_components=n_components, whiten=True)\n",
    "    pca.fit(base_vectors[:fit_size])\n",
    "    \n",
    "    print(\"Transforming data...\")\n",
    "    # Transform all needed data\n",
    "    # We only need the specific base vectors that are in the GT of the queries\n",
    "    # But transforming all might be easier if memory allows (1M * 128 * 4 bytes ~ 512MB)\n",
    "    q_pca = pca.transform(query_vectors)\n",
    "    db_pca = pca.transform(base_vectors)\n",
    "    \n",
    "    n_queries = len(query_vectors)\n",
    "    k_neighbors = gt_indices.shape[1] # Should be 100\n",
    "    n_dims = n_components\n",
    "    \n",
    "    # Array to store sum of absolute differences per dimension\n",
    "    total_abs_diff = np.zeros(n_dims)\n",
    "    count = 0\n",
    "    \n",
    "    print(\"Calculating differences...\")\n",
    "    for i in tqdm(range(n_queries), desc=\"Analyzing Queries\"):\n",
    "        q_vec = q_pca[i] # (128,)\n",
    "        neighbor_indices = gt_indices[i] # (100,)\n",
    "        \n",
    "        # Get neighbor vectors\n",
    "        neighbor_vecs = db_pca[neighbor_indices] # (100, 128)\n",
    "        \n",
    "        # Calculate absolute difference\n",
    "        # |q - n|\n",
    "        abs_diff = np.abs(neighbor_vecs - q_vec) # (100, 128)\n",
    "        \n",
    "        # Sum over neighbors\n",
    "        total_abs_diff += np.sum(abs_diff, axis=0)\n",
    "        count += k_neighbors\n",
    "        \n",
    "    # Mean absolute difference per dimension\n",
    "    mean_abs_diff = total_abs_diff / count\n",
    "    \n",
    "    return mean_abs_diff, pca.explained_variance_ratio_\n",
    "\n",
    "# Setup Data\n",
    "if 'base_vectors_full' not in locals():\n",
    "    base_vectors_full = base_vectors.astype(np.float32)\n",
    "    query_vectors_subset = query_vectors[:1000].astype(np.float32)\n",
    "\n",
    "# Ensure GT for Top-100 exists\n",
    "k_target = 100\n",
    "if 'gt_sweep' in locals() and gt_sweep.shape[1] >= k_target:\n",
    "    gt_100 = gt_sweep[:, :k_target]\n",
    "else:\n",
    "    print(\"Computing GT for Top-100...\")\n",
    "    gt_100 = calculate_top_k_ground_truth(query_vectors_subset, base_vectors_full, k=k_target)\n",
    "\n",
    "# Run Analysis\n",
    "mean_diffs, explained_var = analyze_pca_diff(base_vectors_full, query_vectors_subset, gt_100)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Mean Absolute Difference\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, 129), mean_diffs, color='skyblue')\n",
    "plt.xlabel('PCA Component (Dimension)')\n",
    "plt.ylabel('Mean Absolute Difference')\n",
    "plt.title('Mean Abs Diff between Query and Top-100 NN per PCA Dimension')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot 2: Explained Variance (for context)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 129), explained_var, marker='.', linestyle='-', color='orange')\n",
    "plt.xlabel('PCA Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Explained Variance Ratio')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save to CSV\n",
    "diff_df = pd.DataFrame({\n",
    "    'Dimension': range(1, 129),\n",
    "    'Mean_Abs_Diff': mean_diffs,\n",
    "    'Explained_Variance': explained_var\n",
    "})\n",
    "diff_df.to_csv('sift1m_pca_128_diff_analysis.csv', index=False)\n",
    "print(\"Analysis saved to sift1m_pca_128_diff_analysis.csv\")\n",
    "diff_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36d9c63",
   "metadata": {},
   "source": [
    "## 14. 降維後資料分佈分析 (尚未量化)\n",
    "分析各種降維方法處理後，但尚未進行 INT3 量化前的資料分佈情形。\n",
    "這有助於了解不同方法產生的數值範圍與分佈特性，進而評估量化策略的適用性。\n",
    "- **Overall Histogram**: 所有維度數值的整體分佈。\n",
    "- **Per-Dimension Heatmap**: 每一維度的數值分佈 (X軸為數值，Y軸為維度索引，顏色為頻率)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Data Distribution Analysis (Before Quantization)\n",
    "\n",
    "def analyze_distribution(data, method_name, save_prefix):\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes the distribution of data.\n",
    "    1. Overall histogram of all values.\n",
    "    2. Heatmap of distributions per dimension (Dimension vs Value).\n",
    "    \"\"\"\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data = data.cpu().numpy()\n",
    "        \n",
    "    N, D = data.shape\n",
    "    print(f\"Analyzing {method_name}: Shape {data.shape}\")\n",
    "    \n",
    "    # 1. Overall Histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # 使用 100 個 bins\n",
    "    plt.hist(data.flatten(), bins=100, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'{method_name} - Overall Value Distribution')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    filename_hist = f'{save_prefix}_overall_hist.png'\n",
    "    plt.savefig(filename_hist, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved {filename_hist}\")\n",
    "    \n",
    "    # 2. Per-Dimension Distribution (Heatmap)\n",
    "    # 計算每個維度的 Histogram 並堆疊成 Heatmap\n",
    "    \n",
    "    # Determine global min/max for binning\n",
    "    v_min, v_max = np.min(data), np.max(data)\n",
    "    bins = np.linspace(v_min, v_max, 101) # 100 bins\n",
    "    \n",
    "    hist_matrix = np.zeros((D, 100))\n",
    "    \n",
    "    for d in range(D):\n",
    "        hist, _ = np.histogram(data[:, d], bins=bins)\n",
    "        hist_matrix[d, :] = hist\n",
    "        \n",
    "    # Normalize for better visualization (顯示相對分佈形狀)\n",
    "    hist_matrix_norm = hist_matrix / (hist_matrix.max(axis=1, keepdims=True) + 1e-9)\n",
    "    \n",
    "    plt.figure(figsize=(12, max(6, D/4))) # 動態調整高度\n",
    "    # Extent: [left, right, bottom, top]\n",
    "    plt.imshow(hist_matrix_norm, aspect='auto', origin='lower', \n",
    "               extent=[v_min, v_max, 0, D], cmap='viridis')\n",
    "    plt.colorbar(label='Normalized Frequency')\n",
    "    plt.title(f'{method_name} - Per-Dimension Distribution Heatmap')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Dimension Index')\n",
    "    \n",
    "    filename_heatmap = f'{save_prefix}_dim_heatmap.png'\n",
    "    plt.savefig(filename_heatmap, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved {filename_heatmap}\")\n",
    "\n",
    "# 準備數據 (使用 10000 筆子集進行快速分析)\n",
    "subset_size = 10000\n",
    "# 確保 base_vectors 存在\n",
    "if 'base_vectors' not in locals():\n",
    "    print(\"Error: base_vectors not found. Please run previous cells to load data.\")\n",
    "else:\n",
    "    data_subset = base_vectors[:subset_size].astype(np.float32)\n",
    "    data_tensor = torch.from_numpy(data_subset).to(DEVICE)\n",
    "\n",
    "    print(f\"Using subset of {subset_size} vectors for distribution analysis.\")\n",
    "\n",
    "    # --- Method 1: Original (128 dims) ---\n",
    "    print(\"\\nAnalyzing Method 1 (Original)...\")\n",
    "    analyze_distribution(data_subset, \"Method 1 (Original 128D)\", \"dist_m1\")\n",
    "\n",
    "    # --- Method 2: Avg Pooling (64 dims) ---\n",
    "    print(\"\\nAnalyzing Method 2 (Avg Pooling)...\")\n",
    "    reshaped = data_tensor.reshape(data_tensor.shape[0], 64, 2)\n",
    "    m2_data = torch.mean(reshaped, dim=2)\n",
    "    analyze_distribution(m2_data, \"Method 2 (Avg Pooling 64D)\", \"dist_m2\")\n",
    "\n",
    "    # --- Method 3: PCA (64 dims) ---\n",
    "    print(\"\\nAnalyzing Method 3 (PCA)...\")\n",
    "    pca = PCA(n_components=64, whiten=True)\n",
    "    m3_data = pca.fit_transform(data_subset)\n",
    "    analyze_distribution(m3_data, \"Method 3 (PCA 64D)\", \"dist_m3\")\n",
    "\n",
    "    # --- Method 4: Max Mag Pooling (64 dims) ---\n",
    "    print(\"\\nAnalyzing Method 4 (Max Mag Pooling)...\")\n",
    "    reshaped = data_tensor.reshape(data_tensor.shape[0], 64, 2)\n",
    "    a = reshaped[:, :, 0]\n",
    "    b = reshaped[:, :, 1]\n",
    "    mask = torch.abs(a) >= torch.abs(b)\n",
    "    m4_data = torch.where(mask, a, b)\n",
    "    analyze_distribution(m4_data, \"Method 4 (Max Mag Pooling 64D)\", \"dist_m4\")\n",
    "\n",
    "    # --- Method 5: AutoEncoder (64 dims) ---\n",
    "    print(\"\\nAnalyzing Method 5 (AutoEncoder)...\")\n",
    "    # 訓練一個臨時的 AE 用於分析\n",
    "    ae_model = train_autoencoder(data_subset, 128, 64, epochs=50) \n",
    "    ae_model.eval()\n",
    "    with torch.no_grad():\n",
    "        m5_data = ae_model.encode(data_tensor)\n",
    "    analyze_distribution(m5_data, \"Method 5 (AutoEncoder 64D)\", \"dist_m5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
