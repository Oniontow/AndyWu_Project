{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae8866e",
   "metadata": {},
   "source": [
    "## 1. 安装依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eba1cf",
   "metadata": {},
   "source": [
    "## 2. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a51583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import umap\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "# 设置设备\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 参数设置\n",
    "SIFT_DIM = 128\n",
    "REDUCED_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d4f1a",
   "metadata": {},
   "source": [
    "## 3. 下载和解压 SIFT1M 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sift1m():\n",
    "    \"\"\"\n",
    "    下载SIFT1M数据集\n",
    "    \"\"\"\n",
    "    # 创建数据目录\n",
    "    os.makedirs('sift1m', exist_ok=True)\n",
    "    \n",
    "    files = [\n",
    "        \"sift_base.fvecs\",\n",
    "        \"sift_query.fvecs\",\n",
    "        \"sift_groundtruth.ivecs\"\n",
    "    ]\n",
    "    \n",
    "    # 检查文件是否已存在\n",
    "    all_exist = True\n",
    "    for filename in files:\n",
    "        if not os.path.exists(os.path.join('sift1m', filename)):\n",
    "            all_exist = False\n",
    "            break\n",
    "            \n",
    "    if all_exist:\n",
    "        print(\"所有文件已存在，跳过下载\")\n",
    "        return True\n",
    "        \n",
    "    print(\"正在下载 SIFT1M 数据集...\")\n",
    "    \n",
    "    # 尝试下载 tar.gz 文件\n",
    "    tar_url = \"ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz\"\n",
    "    tar_path = \"sift1m/sift.tar.gz\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"尝试从 {tar_url} 下载...\")\n",
    "        # 增加 timeout\n",
    "        import socket\n",
    "        socket.setdefaulttimeout(30)\n",
    "        urllib.request.urlretrieve(tar_url, tar_path)\n",
    "        print(\"下载完成，正在解压...\")\n",
    "        \n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=\"sift1m\")\n",
    "            \n",
    "        # 移动文件到 sift1m 根目录 (解压后会在 sift/ 目录下)\n",
    "        extracted_dir = os.path.join(\"sift1m\", \"sift\")\n",
    "        if os.path.exists(extracted_dir):\n",
    "            for filename in files:\n",
    "                src = os.path.join(extracted_dir, filename)\n",
    "                dst = os.path.join(\"sift1m\", filename)\n",
    "                if os.path.exists(src):\n",
    "                    if os.path.exists(dst):\n",
    "                        os.remove(dst)\n",
    "                    os.rename(src, dst)\n",
    "            # 清理\n",
    "            try:\n",
    "                import shutil\n",
    "                shutil.rmtree(extracted_dir)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # 删除 tar 文件\n",
    "        if os.path.exists(tar_path):\n",
    "            os.remove(tar_path)\n",
    "        print(\"数据集准备完成\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"FTP下载失败: {e}\")\n",
    "        print(\"尝试使用 wget 下载...\")\n",
    "        \n",
    "        try:\n",
    "            # 尝试使用 wget\n",
    "            res = os.system(f\"wget {tar_url} -O {tar_path}\")\n",
    "            if res == 0 and os.path.exists(tar_path):\n",
    "                print(\"wget 下载成功，正在解压...\")\n",
    "                with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "                    tar.extractall(path=\"sift1m\")\n",
    "                \n",
    "                # 移动文件\n",
    "                extracted_dir = os.path.join(\"sift1m\", \"sift\")\n",
    "                if os.path.exists(extracted_dir):\n",
    "                    for filename in files:\n",
    "                        src = os.path.join(extracted_dir, filename)\n",
    "                        dst = os.path.join(\"sift1m\", filename)\n",
    "                        if os.path.exists(src):\n",
    "                            if os.path.exists(dst):\n",
    "                                os.remove(dst)\n",
    "                            os.rename(src, dst)\n",
    "                    try:\n",
    "                        import shutil\n",
    "                        shutil.rmtree(extracted_dir)\n",
    "                    except:\n",
    "                        pass\n",
    "                if os.path.exists(tar_path):\n",
    "                    os.remove(tar_path)\n",
    "                return True\n",
    "        except Exception as e2:\n",
    "            print(f\"wget 失败: {e2}\")\n",
    "            \n",
    "        print(\"无法自动下载数据集。\")\n",
    "        print(\"请手动下载 sift.tar.gz 从 http://corpus-texmex.irisa.fr/\")\n",
    "        print(\"并解压到 sift1m/ 目录下。\")\n",
    "        return False\n",
    "\n",
    "# 下载数据集\n",
    "print(\"开始下载SIFT1M数据集...\")\n",
    "download_sift1m()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692adc9a",
   "metadata": {},
   "source": [
    "## 4. INT3 量化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_to_int3(data):\n",
    "    \"\"\"\n",
    "    將數據量化到INT3 (-4 到 3，共8個離散值)\n",
    "    改進：使用百分位數 (1% - 99%) 進行截斷，減少極端值對量化的影響\n",
    "    \"\"\"\n",
    "    is_torch = isinstance(data, torch.Tensor)\n",
    "    \n",
    "    if is_torch:\n",
    "        arr = data.cpu().numpy()\n",
    "    else:\n",
    "        arr = data\n",
    "    \n",
    "    # 使用百分位數來決定範圍，避免 outlier 影響\n",
    "    # SIFT 特徵通常會有少數極大值\n",
    "    min_val = np.percentile(arr, 1)\n",
    "    max_val = np.percentile(arr, 99)\n",
    "    \n",
    "    # 將數據縮放到 [-4, 3] 範圍\n",
    "    scaled = (arr - min_val) / (max_val - min_val + 1e-8) * 7 - 4\n",
    "    \n",
    "    # 四捨五入並裁剪到 INT3 範圍\n",
    "    quantized = np.clip(np.round(scaled), -4, 3).astype(np.int8)\n",
    "    \n",
    "    if is_torch:\n",
    "        return torch.from_numpy(quantized).to(data.device)\n",
    "    else:\n",
    "        return quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea22ff0",
   "metadata": {},
   "source": [
    "## 5. AutoEncoder 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=128, latent_dim=64):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder (加深並加入 BatchNorm)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 96),\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 96),\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "def train_autoencoder(data, input_dim=128, latent_dim=64, epochs=100, batch_size=256, patience=5, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    訓練自編碼器 (加入 Early Stopping)\n",
    "    \"\"\"\n",
    "    model = AutoEncoder(input_dim, latent_dim).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(data).to(DEVICE)\n",
    "    \n",
    "    # 創建 DataLoader 以便於 batch 處理\n",
    "    dataset = torch.utils.data.TensorDataset(data_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    pbar = tqdm(range(epochs), desc=\"训练AutoEncoder\")\n",
    "    for epoch in pbar:\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch_data = batch[0]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            _, reconstructed = model(batch_data)\n",
    "            loss = criterion(reconstructed, batch_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        pbar.set_postfix({'loss': f'{avg_loss:.6f}'})\n",
    "        \n",
    "        # Early Stopping Check\n",
    "        if avg_loss < best_loss - min_delta:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1} (Best Loss: {best_loss:.6f})\")\n",
    "            break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97cf329",
   "metadata": {},
   "source": [
    "## 6. 五种数据处理方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6cb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method1_direct_int3(data):\n",
    "    \"\"\"方法1: 直接將128維向量量化為INT3\"\"\"\n",
    "    return quantize_to_int3(data)\n",
    "\n",
    "def method2_average_pooling_int3(data):\n",
    "    \"\"\"方法2: 將相鄰兩維做平均，從128維降到64維，然後量化為INT3\"\"\"\n",
    "    reshaped = data.reshape(data.shape[0], 64, 2)\n",
    "    averaged = np.mean(reshaped, axis=2)\n",
    "    return quantize_to_int3(averaged)\n",
    "\n",
    "def method3_pca_int3(data, query_data=None):\n",
    "    \"\"\"方法3: 使用PCA降維到64維，然後量化為INT3\"\"\"\n",
    "    print(\"  - 训练PCA...\")\n",
    "    # 使用 whiten=True 來標準化分量，通常能改善後續量化效果\n",
    "    pca = PCA(n_components=REDUCED_DIM, whiten=True)\n",
    "    pca.fit(data)\n",
    "    \n",
    "    data_reduced = pca.transform(data)\n",
    "    data_quantized = quantize_to_int3(data_reduced)\n",
    "    \n",
    "    if query_data is not None:\n",
    "        query_reduced = pca.transform(query_data)\n",
    "        query_quantized = quantize_to_int3(query_reduced)\n",
    "        return pca, data_quantized, query_quantized\n",
    "    \n",
    "    return pca, data_quantized\n",
    "\n",
    "def method4_max_pooling_int3(data, query_data=None):\n",
    "    \"\"\"方法4: 將相鄰兩維做最大絕對值池化 (Max Magnitude Pooling)，從128維降到64維，然後量化為INT3\"\"\"\n",
    "    \n",
    "    def max_magnitude_pool(arr):\n",
    "        # Reshape to (N, 64, 2)\n",
    "        reshaped = arr.reshape(arr.shape[0], 64, 2)\n",
    "        a = reshaped[:, :, 0]\n",
    "        b = reshaped[:, :, 1]\n",
    "        # 比較絕對值大小\n",
    "        mask = np.abs(a) >= np.abs(b)\n",
    "        # 選擇絕對值較大的那個原始值\n",
    "        return np.where(mask, a, b)\n",
    "\n",
    "    max_pooled = max_magnitude_pool(data)\n",
    "    data_quantized = quantize_to_int3(max_pooled)\n",
    "    \n",
    "    if query_data is not None:\n",
    "        q_max_pooled = max_magnitude_pool(query_data)\n",
    "        query_quantized = quantize_to_int3(q_max_pooled)\n",
    "        return None, data_quantized, query_quantized\n",
    "    \n",
    "    return None, data_quantized\n",
    "\n",
    "def method5_autoencoder_int3(data, query_data=None, epochs=100):\n",
    "    \"\"\"方法5: 使用AutoEncoder降維到64維，然後量化為INT3\"\"\"\n",
    "    print(\"  - 训练AutoEncoder...\")\n",
    "    # 這裡 epochs 預設為 100，配合 Early Stopping\n",
    "    ae_model = train_autoencoder(data, SIFT_DIM, REDUCED_DIM, epochs=epochs)\n",
    "    ae_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        data_tensor = torch.FloatTensor(data).to(DEVICE)\n",
    "        data_reduced = ae_model.encode(data_tensor).cpu().numpy()\n",
    "        data_quantized = quantize_to_int3(data_reduced)\n",
    "        \n",
    "        if query_data is not None:\n",
    "            query_tensor = torch.FloatTensor(query_data).to(DEVICE)\n",
    "            query_reduced = ae_model.encode(query_tensor).cpu().numpy()\n",
    "            query_quantized = quantize_to_int3(query_reduced)\n",
    "            return ae_model, data_quantized, query_quantized\n",
    "    \n",
    "    return ae_model, data_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21951698",
   "metadata": {},
   "source": [
    "## 7. 距离计算和评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_l2_distances(queries, database):\n",
    "    \"\"\"計算L2距離\"\"\"\n",
    "    return pairwise_distances(queries, database, metric='euclidean')\n",
    "\n",
    "def evaluate_recall_at_k(distances, ground_truth, k_values=[1, 10, 100]):\n",
    "    \"\"\"計算Recall@K指標\"\"\"\n",
    "    sorted_indices = np.argsort(distances, axis=1)\n",
    "    \n",
    "    recalls = {}\n",
    "    for k in k_values:\n",
    "        top_k_predictions = sorted_indices[:, :k]\n",
    "        \n",
    "        query_recalls = []\n",
    "        for i in range(len(ground_truth)):\n",
    "            gt_i = ground_truth[i]\n",
    "            # Handle jagged arrays: check length\n",
    "            limit = min(k, len(gt_i))\n",
    "            \n",
    "            if limit == 0:\n",
    "                query_recalls.append(0.0)\n",
    "                continue\n",
    "                \n",
    "            true_neighbors = set(gt_i[:limit])\n",
    "            pred_neighbors = set(top_k_predictions[i])\n",
    "            intersection = true_neighbors & pred_neighbors\n",
    "            recall = len(intersection) / len(true_neighbors)\n",
    "            query_recalls.append(recall)\n",
    "        \n",
    "        recalls[f'recall@{k}'] = np.mean(query_recalls)\n",
    "    \n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a202e3",
   "metadata": {},
   "source": [
    "## 8. 加载 SIFT1M 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fvecs(filename):\n",
    "    \"\"\"讀取.fvecs格式文件\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        d = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "        f.seek(0)\n",
    "        # .fvecs 格式: 4 bytes int32 (dimension) + d * 4 bytes float32 (data)\n",
    "        # 读取为 float32，header 的 int32 会被读成一个 float32，reshape 后切片去掉即可\n",
    "        data = np.fromfile(f, dtype=np.float32)\n",
    "        data = data.reshape(-1, d + 1)\n",
    "        return data[:, 1:].copy()\n",
    "\n",
    "def read_ivecs(filename):\n",
    "    \"\"\"讀取.ivecs格式文件\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        d = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "        f.seek(0)\n",
    "        data = np.fromfile(f, dtype=np.int32)\n",
    "        data = data.reshape(-1, d + 1)\n",
    "        return data[:, 1:].copy()\n",
    "\n",
    "print(\"载入SIFT1M数据集...\")\n",
    "base_vectors = read_fvecs('sift1m/sift_base.fvecs')\n",
    "query_vectors = read_fvecs('sift1m/sift_query.fvecs')\n",
    "ground_truth = read_ivecs('sift1m/sift_groundtruth.ivecs')\n",
    "\n",
    "print(f\"Base vectors shape: {base_vectors.shape}\")\n",
    "print(f\"Query vectors shape: {query_vectors.shape}\")\n",
    "print(f\"Ground truth shape: {ground_truth.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70c0f9",
   "metadata": {},
   "source": [
    "## 9. 运行实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top_k_ground_truth(queries, database, k=100):\n",
    "    \"\"\"\n",
    "    計算每個查詢的前 K 個真實最近鄰居 (Ground Truth)\n",
    "    使用 GPU 加速計算，並分批處理 Query 以節省記憶體\n",
    "    \"\"\"\n",
    "    print(f\"正在計算 Top-{k} Ground Truth (Query: {len(queries)}, DB: {len(database)})...\")\n",
    "    \n",
    "    num_queries = len(queries)\n",
    "    query_batch_size = 100  # 每次處理 100 個 Query\n",
    "    \n",
    "    all_indices = []\n",
    "    \n",
    "    for i in tqdm(range(0, num_queries, query_batch_size), desc=\"Computing GT\"):\n",
    "        q_end = min(i + query_batch_size, num_queries)\n",
    "        q_batch = queries[i:q_end]\n",
    "        q_tensor = torch.FloatTensor(q_batch).to(DEVICE)\n",
    "        q_sq = torch.sum(q_tensor**2, dim=1, keepdim=True)\n",
    "        \n",
    "        dists_batch = []\n",
    "        \n",
    "        # 分批處理 DB\n",
    "        db_batch_size = 50000\n",
    "        for j in range(0, len(database), db_batch_size):\n",
    "            db_end = min(j + db_batch_size, len(database))\n",
    "            db_chunk = torch.FloatTensor(database[j:db_end]).to(DEVICE)\n",
    "            \n",
    "            db_sq_chunk = torch.sum(db_chunk**2, dim=1)\n",
    "            term2 = -2 * torch.matmul(q_tensor, db_chunk.t())\n",
    "            \n",
    "            dists_chunk = q_sq + db_sq_chunk + term2\n",
    "            dists_batch.append(dists_chunk.cpu())\n",
    "            \n",
    "            del db_chunk, db_sq_chunk, term2, dists_chunk\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        full_dists_batch = torch.cat(dists_batch, dim=1)\n",
    "        \n",
    "        # 取 Top K\n",
    "        _, indices = torch.topk(full_dists_batch, k=k, dim=1, largest=False)\n",
    "        all_indices.append(indices.numpy())\n",
    "        \n",
    "        del full_dists_batch, indices, q_tensor, q_sq\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return np.vstack(all_indices)\n",
    "\n",
    "def evaluate_recall_batched(query_vectors, db_vectors, gt_top_k, retrieval_depths, k_true_neighbors):\n",
    "    \"\"\"\n",
    "    分批計算距離並評估 Recall\n",
    "    Recall = (Retrieved & Top-K_GT) / K_GT\n",
    "    \"\"\"\n",
    "    max_depth = max(retrieval_depths)\n",
    "    num_queries = len(query_vectors)\n",
    "    query_batch_size = 100\n",
    "    \n",
    "    total_hits = {r: 0 for r in retrieval_depths}\n",
    "    \n",
    "    if query_vectors.dtype != np.float32:\n",
    "        query_vectors = query_vectors.astype(np.float32)\n",
    "    if db_vectors.dtype != np.float32:\n",
    "        db_vectors = db_vectors.astype(np.float32)\n",
    "        \n",
    "    for i in tqdm(range(0, num_queries, query_batch_size), desc=\"Evaluating Batches\", leave=False):\n",
    "        q_end = min(i + query_batch_size, num_queries)\n",
    "        q_batch = query_vectors[i:q_end]\n",
    "        gt_batch = gt_top_k[i:q_end]\n",
    "        \n",
    "        dists = pairwise_distances(q_batch, db_vectors, metric='euclidean')\n",
    "        sorted_indices = np.argsort(dists, axis=1)[:, :max_depth]\n",
    "        \n",
    "        for r in retrieval_depths:\n",
    "            retrieved_indices = sorted_indices[:, :r]\n",
    "            for j in range(len(gt_batch)):\n",
    "                gt_set = set(gt_batch[j]) # 這是 Top K 真實鄰居\n",
    "                retrieved_set = set(retrieved_indices[j])\n",
    "                total_hits[r] += len(gt_set & retrieved_set)\n",
    "                \n",
    "    recalls = {}\n",
    "    for r in retrieval_depths:\n",
    "        # Normalize by k_true_neighbors (e.g., 100)\n",
    "        recalls[f'recall@{r}'] = total_hits[r] / (num_queries * float(k_true_neighbors))\n",
    "        \n",
    "    return recalls\n",
    "\n",
    "def run_experiment_part(part_name, base_vectors, query_vectors, gt_top_k, retrieval_depths, k_true_neighbors):\n",
    "    \"\"\"\n",
    "    執行單個部分的實驗\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"實驗部分: {part_name}\")\n",
    "    print(f\"Database Size: {len(base_vectors)}\")\n",
    "    print(f\"Retrieval Depths: {retrieval_depths}\")\n",
    "    print(f\"Target GT Size (K): {k_true_neighbors}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    # 方法1\n",
    "    print(\"\\n[方法1] 直接INT3量化 (128維)...\")\n",
    "    t0 = time.time()\n",
    "    db_m1 = method1_direct_int3(base_vectors)\n",
    "    q_m1 = method1_direct_int3(query_vectors)\n",
    "    recalls_m1 = evaluate_recall_batched(q_m1, db_m1, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 1: Direct INT3', 'time': time.time()-t0, **recalls_m1})\n",
    "    print(recalls_m1)\n",
    "    \n",
    "    # 方法2\n",
    "    print(\"\\n[方法2] 平均池化降維 + INT3 (64維)...\")\n",
    "    t0 = time.time()\n",
    "    db_m2 = method2_average_pooling_int3(base_vectors)\n",
    "    q_m2 = method2_average_pooling_int3(query_vectors)\n",
    "    recalls_m2 = evaluate_recall_batched(q_m2, db_m2, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 2: AvgPooling + INT3', 'time': time.time()-t0, **recalls_m2})\n",
    "    print(recalls_m2)\n",
    "    \n",
    "    # 方法3\n",
    "    print(\"\\n[方法3] PCA降維 + INT3 (64維)...\")\n",
    "    t0 = time.time()\n",
    "    _, db_m3, q_m3 = method3_pca_int3(base_vectors, query_vectors)\n",
    "    recalls_m3 = evaluate_recall_batched(q_m3, db_m3, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 3: PCA + INT3', 'time': time.time()-t0, **recalls_m3})\n",
    "    print(recalls_m3)\n",
    "    \n",
    "    # 方法4\n",
    "    print(f\"\\n[方法4] Max Magnitude Pooling降維 + INT3 (64維)...\")\n",
    "    t0 = time.time()\n",
    "    _, db_m4, q_m4 = method4_max_pooling_int3(base_vectors, query_vectors)\n",
    "    recalls_m4 = evaluate_recall_batched(q_m4, db_m4, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 4: Max Mag Pooling + INT3', 'time': time.time()-t0, **recalls_m4})\n",
    "    print(recalls_m4)\n",
    "    \n",
    "    # 方法5\n",
    "    print(\"\\n[方法5] AutoEncoder降維 + INT3 (64維)...\")\n",
    "    t0 = time.time()\n",
    "    # Epochs 設為 100，配合 Early Stopping\n",
    "    _, db_m5, q_m5 = method5_autoencoder_int3(base_vectors, query_vectors, epochs=100)\n",
    "    recalls_m5 = evaluate_recall_batched(q_m5, db_m5, gt_top_k, retrieval_depths, k_true_neighbors)\n",
    "    results.append({'method': 'Method 5: AutoEncoder + INT3', 'time': time.time()-t0, **recalls_m5})\n",
    "    print(recalls_m5)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# --- 主執行流程 ---\n",
    "\n",
    "# 準備數據\n",
    "NUM_QUERIES = 1000\n",
    "base_vectors_full = base_vectors.astype(np.float32)\n",
    "query_vectors_subset = query_vectors[:NUM_QUERIES].astype(np.float32)\n",
    "\n",
    "all_experiment_results = []\n",
    "\n",
    "# --- 第一部分: 10k 資料 ---\n",
    "db_size_1 = 10000\n",
    "db_subset_1 = base_vectors_full[:db_size_1]\n",
    "gt_k_1 = 100\n",
    "print(f\"\\n正在計算 Part 1 Ground Truth (Top-{gt_k_1})...\")\n",
    "gt_1 = calculate_top_k_ground_truth(query_vectors_subset, db_subset_1, k=gt_k_1)\n",
    "\n",
    "df_1 = run_experiment_part(\n",
    "    part_name=\"Part 1 (10k DB)\",\n",
    "    base_vectors=db_subset_1,\n",
    "    query_vectors=query_vectors_subset,\n",
    "    gt_top_k=gt_1,\n",
    "    retrieval_depths=[1000],\n",
    "    k_true_neighbors=gt_k_1\n",
    ")\n",
    "df_1['Experiment'] = '10k DB'\n",
    "all_experiment_results.append(df_1)\n",
    "display(df_1)\n",
    "\n",
    "# --- 第二部分: 100k 資料 ---\n",
    "db_size_2 = 100000\n",
    "db_subset_2 = base_vectors_full[:db_size_2]\n",
    "gt_k_2 = 100\n",
    "print(f\"\\n正在計算 Part 2 Ground Truth (Top-{gt_k_2})...\")\n",
    "gt_2 = calculate_top_k_ground_truth(query_vectors_subset, db_subset_2, k=gt_k_2)\n",
    "\n",
    "df_2 = run_experiment_part(\n",
    "    part_name=\"Part 2 (100k DB)\",\n",
    "    base_vectors=db_subset_2,\n",
    "    query_vectors=query_vectors_subset,\n",
    "    gt_top_k=gt_2,\n",
    "    retrieval_depths=[1000, 10000],\n",
    "    k_true_neighbors=gt_k_2\n",
    ")\n",
    "df_2['Experiment'] = '100k DB'\n",
    "all_experiment_results.append(df_2)\n",
    "display(df_2)\n",
    "\n",
    "# --- 第三部分: 1M 資料 ---\n",
    "db_size_3 = 1000000\n",
    "db_subset_3 = base_vectors_full # Full\n",
    "gt_k_3 = 100\n",
    "print(f\"\\n正在計算 Part 3 Ground Truth (Top-{gt_k_3})...\")\n",
    "gt_3 = calculate_top_k_ground_truth(query_vectors_subset, db_subset_3, k=gt_k_3)\n",
    "\n",
    "df_3 = run_experiment_part(\n",
    "    part_name=\"Part 3 (1M DB)\",\n",
    "    base_vectors=db_subset_3,\n",
    "    query_vectors=query_vectors_subset,\n",
    "    gt_top_k=gt_3,\n",
    "    retrieval_depths=[1000, 10000],\n",
    "    k_true_neighbors=gt_k_3\n",
    ")\n",
    "df_3['Experiment'] = '1M DB'\n",
    "all_experiment_results.append(df_3)\n",
    "display(df_3)\n",
    "\n",
    "# 保存所有結果\n",
    "final_df = pd.concat(all_experiment_results, ignore_index=True)\n",
    "final_df.to_csv(\"sift1m_3parts_experiment_results.csv\", index=False)\n",
    "print(\"\\n所有實驗完成，結果已保存。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4ac29",
   "metadata": {},
   "source": [
    "## 10. 可视化结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a08e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化結果\n",
    "# 我們需要畫 3 張圖 (或 3 組圖)，分別對應 Part 1, Part 2, Part 3\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('Recall of Top-100 True Neighbors @ Different Retrieval Depths', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Part 1: 10k DB, Recall@1000\n",
    "ax1 = axes[0]\n",
    "df1 = all_experiment_results[0]\n",
    "methods = df1['method'].str.replace('Method ', 'M').str.replace(': ', '\\n', 1)\n",
    "bars1 = ax1.bar(range(len(df1)), df1['recall@1000'], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "ax1.set_xticks(range(len(df1)))\n",
    "ax1.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
    "ax1.set_ylabel('Recall@1000', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Part 1: 10k DB (100@1000)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim([0, 1.05])\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Part 2: 100k DB, Recall@1000 & Recall@10000\n",
    "# 這裡我們畫 Recall@10000 作為代表，或者畫兩個柱狀圖\n",
    "ax2 = axes[1]\n",
    "df2 = all_experiment_results[1]\n",
    "# 為了同時顯示 @1000 和 @10000，我們使用分組柱狀圖\n",
    "x = np.arange(len(df2))\n",
    "width = 0.35\n",
    "bars2_1 = ax2.bar(x - width/2, df2['recall@1000'], width, label='@1000', color='#1f77b4')\n",
    "bars2_2 = ax2.bar(x + width/2, df2['recall@10000'], width, label='@10000', color='#ff7f0e')\n",
    "\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
    "ax2.set_ylabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Part 2: 100k DB', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim([0, 1.05])\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Part 3: 1M DB, Recall@1000 & Recall@10000\n",
    "ax3 = axes[2]\n",
    "df3 = all_experiment_results[2]\n",
    "x = np.arange(len(df3))\n",
    "bars3_1 = ax3.bar(x - width/2, df3['recall@1000'], width, label='@1000', color='#1f77b4')\n",
    "bars3_2 = ax3.bar(x + width/2, df3['recall@10000'], width, label='@10000', color='#ff7f0e')\n",
    "\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
    "ax3.set_ylabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Part 3: 1M DB', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylim([0, 1.05])\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sift1m_3parts_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44c37c",
   "metadata": {},
   "source": [
    "## 11. 实验总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"實驗總結\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config, results_df in zip(EXPERIMENTS, all_results):\n",
    "    print(f\"\\n配置: {config['query_size']}@{config['database_size']}\")\n",
    "    display(results_df)\n",
    "\n",
    "print(\"\\n所有實驗完成！\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
